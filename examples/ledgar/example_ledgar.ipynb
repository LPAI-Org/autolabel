{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fe6e643-9453-4381-9445-bd471685fb96",
   "metadata": {},
   "source": [
    "## Exploring the LEDGAR dataset using Autolabel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80110a5b-2b3e-45e2-a2da-f6fa00200dff",
   "metadata": {},
   "source": [
    "#### Setup the API Keys for providers that you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92993c83-4473-4e05-9510-f543b070c7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# provide your own OpenAI API key here\n",
    "os.environ['OPENAI_API_KEY'] = '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'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c246f85",
   "metadata": {},
   "source": [
    "#### Install the autolabel library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc181e31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: refuel-autolabel[openai] in /opt/homebrew/lib/python3.10/site-packages (0.0.1)\n",
      "Requirement already satisfied: loguru>=0.5.0 in /opt/homebrew/lib/python3.10/site-packages (from refuel-autolabel[openai]) (0.5.3)\n",
      "Requirement already satisfied: numpy>=1.23.0 in /opt/homebrew/lib/python3.10/site-packages (from refuel-autolabel[openai]) (1.23.3)\n",
      "Requirement already satisfied: requests>=2.27.0 in /opt/homebrew/lib/python3.10/site-packages (from refuel-autolabel[openai]) (2.28.1)\n",
      "Requirement already satisfied: datasets>=2.7.0 in /opt/homebrew/lib/python3.10/site-packages (from refuel-autolabel[openai]) (2.11.0)\n",
      "Requirement already satisfied: langchain>=0.0.190 in /opt/homebrew/lib/python3.10/site-packages (from refuel-autolabel[openai]) (0.0.191)\n",
      "Requirement already satisfied: nervaluate>=0.1.8 in /opt/homebrew/lib/python3.10/site-packages (from refuel-autolabel[openai]) (0.1.8)\n",
      "Requirement already satisfied: pandas>=1.3.0 in /opt/homebrew/lib/python3.10/site-packages (from refuel-autolabel[openai]) (1.5.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /opt/homebrew/lib/python3.10/site-packages (from refuel-autolabel[openai]) (1.1.2)\n",
      "Requirement already satisfied: tenacity>=8.2.2 in /opt/homebrew/lib/python3.10/site-packages (from refuel-autolabel[openai]) (8.2.2)\n",
      "Requirement already satisfied: SQLAlchemy==1.4.47 in /opt/homebrew/lib/python3.10/site-packages (from refuel-autolabel[openai]) (1.4.47)\n",
      "Requirement already satisfied: regex>=2023.6.3 in /opt/homebrew/lib/python3.10/site-packages (from refuel-autolabel[openai]) (2023.6.3)\n",
      "Requirement already satisfied: rich>=13.3.5 in /opt/homebrew/lib/python3.10/site-packages (from refuel-autolabel[openai]) (13.4.1)\n",
      "Requirement already satisfied: scipy>=1.10.1 in /opt/homebrew/lib/python3.10/site-packages (from refuel-autolabel[openai]) (1.10.1)\n",
      "Requirement already satisfied: pydantic>=1.10.9 in /opt/homebrew/lib/python3.10/site-packages (from refuel-autolabel[openai]) (1.10.9)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/homebrew/lib/python3.10/site-packages (from refuel-autolabel[openai]) (1.13.0)\n",
      "Requirement already satisfied: matplotlib>=3.5.0 in /opt/homebrew/lib/python3.10/site-packages (from refuel-autolabel[openai]) (3.6.0)\n",
      "Requirement already satisfied: wget>=3.2 in /opt/homebrew/lib/python3.10/site-packages (from refuel-autolabel[openai]) (3.2)\n",
      "Requirement already satisfied: openai>=0.27.4 in /opt/homebrew/lib/python3.10/site-packages (from refuel-autolabel[openai]) (0.27.6)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /opt/homebrew/lib/python3.10/site-packages (from refuel-autolabel[openai]) (0.3.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/homebrew/lib/python3.10/site-packages (from datasets>=2.7.0->refuel-autolabel[openai]) (9.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/homebrew/lib/python3.10/site-packages (from datasets>=2.7.0->refuel-autolabel[openai]) (0.3.5.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/homebrew/lib/python3.10/site-packages (from datasets>=2.7.0->refuel-autolabel[openai]) (4.64.1)\n",
      "Requirement already satisfied: xxhash in /opt/homebrew/lib/python3.10/site-packages (from datasets>=2.7.0->refuel-autolabel[openai]) (3.1.0)\n",
      "Requirement already satisfied: multiprocess in /opt/homebrew/lib/python3.10/site-packages (from datasets>=2.7.0->refuel-autolabel[openai]) (0.70.13)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/homebrew/lib/python3.10/site-packages (from datasets>=2.7.0->refuel-autolabel[openai]) (2022.10.0)\n",
      "Requirement already satisfied: aiohttp in /opt/homebrew/lib/python3.10/site-packages (from datasets>=2.7.0->refuel-autolabel[openai]) (3.8.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /opt/homebrew/lib/python3.10/site-packages (from datasets>=2.7.0->refuel-autolabel[openai]) (0.14.1)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/lib/python3.10/site-packages (from datasets>=2.7.0->refuel-autolabel[openai]) (21.3)\n",
      "Requirement already satisfied: responses<0.19 in /opt/homebrew/lib/python3.10/site-packages (from datasets>=2.7.0->refuel-autolabel[openai]) (0.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.10/site-packages (from datasets>=2.7.0->refuel-autolabel[openai]) (6.0)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/homebrew/lib/python3.10/site-packages (from langchain>=0.0.190->refuel-autolabel[openai]) (4.0.2)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /opt/homebrew/lib/python3.10/site-packages (from langchain>=0.0.190->refuel-autolabel[openai]) (0.5.7)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /opt/homebrew/lib/python3.10/site-packages (from langchain>=0.0.190->refuel-autolabel[openai]) (2.8.4)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /opt/homebrew/lib/python3.10/site-packages (from langchain>=0.0.190->refuel-autolabel[openai]) (1.2.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/lib/python3.10/site-packages (from matplotlib>=3.5.0->refuel-autolabel[openai]) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/lib/python3.10/site-packages (from matplotlib>=3.5.0->refuel-autolabel[openai]) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/lib/python3.10/site-packages (from matplotlib>=3.5.0->refuel-autolabel[openai]) (4.37.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/homebrew/lib/python3.10/site-packages (from matplotlib>=3.5.0->refuel-autolabel[openai]) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/homebrew/lib/python3.10/site-packages (from matplotlib>=3.5.0->refuel-autolabel[openai]) (9.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/homebrew/lib/python3.10/site-packages (from matplotlib>=3.5.0->refuel-autolabel[openai]) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/homebrew/lib/python3.10/site-packages (from matplotlib>=3.5.0->refuel-autolabel[openai]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.10/site-packages (from pandas>=1.3.0->refuel-autolabel[openai]) (2022.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/homebrew/lib/python3.10/site-packages (from pydantic>=1.10.9->refuel-autolabel[openai]) (4.3.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/homebrew/lib/python3.10/site-packages (from requests>=2.27.0->refuel-autolabel[openai]) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.10/site-packages (from requests>=2.27.0->refuel-autolabel[openai]) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/lib/python3.10/site-packages (from requests>=2.27.0->refuel-autolabel[openai]) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.10/site-packages (from requests>=2.27.0->refuel-autolabel[openai]) (2022.9.14)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/homebrew/lib/python3.10/site-packages (from rich>=13.3.5->refuel-autolabel[openai]) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/homebrew/lib/python3.10/site-packages (from rich>=13.3.5->refuel-autolabel[openai]) (2.15.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /opt/homebrew/lib/python3.10/site-packages (from scikit-learn>=1.0.0->refuel-autolabel[openai]) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/lib/python3.10/site-packages (from scikit-learn>=1.0.0->refuel-autolabel[openai]) (3.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.0->refuel-autolabel[openai]) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.0->refuel-autolabel[openai]) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.0->refuel-autolabel[openai]) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.0->refuel-autolabel[openai]) (1.3.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.0->refuel-autolabel[openai]) (1.2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /opt/homebrew/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.190->refuel-autolabel[openai]) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /opt/homebrew/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.190->refuel-autolabel[openai]) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /opt/homebrew/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.190->refuel-autolabel[openai]) (0.8.0)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets>=2.7.0->refuel-autolabel[openai]) (3.8.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/homebrew/lib/python3.10/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=13.3.5->refuel-autolabel[openai]) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.5.0->refuel-autolabel[openai]) (1.12.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/homebrew/lib/python3.10/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.190->refuel-autolabel[openai]) (0.4.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install 'refuel-autolabel[openai]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d35966",
   "metadata": {},
   "source": [
    "#### Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4b07ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading example dataset from https://autolabel-benchmarking.s3.us-west-2.amazonaws.com/ledgar/seed.csv to seed.csv...\n",
      "Downloading example dataset from https://autolabel-benchmarking.s3.us-west-2.amazonaws.com/ledgar/test.csv to test.csv...\n",
      "100% [........................................] [1376364/1376364] bytes\r"
     ]
    }
   ],
   "source": [
    "from autolabel import get_data\n",
    "\n",
    "get_data('ledgar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13e2536",
   "metadata": {},
   "source": [
    "This downloads two datasets:\n",
    "* `test.csv`: This is the larger dataset we are trying to label using LLMs\n",
    "* `seed.csv`: This is a small dataset where we already have human-provided labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a676b6",
   "metadata": {},
   "source": [
    "## Start the labeling process!\n",
    "\n",
    "Labeling with Autolabel is a 3-step process:\n",
    "* First, we specify a labeling configuration (see `config.json` below)\n",
    "* Next, we do a dry-run on our dataset using the LLM specified in `config.json` by running `agent.plan`\n",
    "* Finally, we run the labeling with `agent.run`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68190738",
   "metadata": {},
   "source": [
    "### First labeling run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c093fe91-3508-4140-8bd6-217034e3cce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from autolabel import LabelingAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c93fae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the config\n",
    "with open('config_ledgar_mixtral.json', 'r') as f:\n",
    "     config = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e0a386",
   "metadata": {},
   "source": [
    "Let's review the configuration file below. You'll notice the following useful keys:\n",
    "* `task_type`: `classification` (since it's a classification task)\n",
    "* `model`: `{'provider': 'openai', 'name': 'gpt-3.5-turbo'}` (use a specific OpenAI model)\n",
    "* `prompt.task_guidelines`: `'You are an expert at understanding legal contracts...` (how we describe the task to the LLM)\n",
    "* `prompt.labels`: `['Agreements',\n",
    "   'Amendments',\n",
    "   'Anti-Corruption Laws',\n",
    "   'Applicable Laws',\n",
    "   'Approvals',\n",
    "   'Arbitration',\n",
    "   'Assignments',\n",
    "   'Assigns', ...]` (the full list of labels to choose from)\n",
    "* `prompt.few_shot_num`: 4 (how many labeled examples to provide to the LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8decb1f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task_name': 'LegalProvisionsClassification',\n",
       " 'task_type': 'classification',\n",
       " 'dataset': {'label_column': 'label', 'delimiter': ','},\n",
       " 'model': {'provider': 'huggingface_pipeline',\n",
       "  'name': 'Open-Orca/Mixtral-SlimOrca-8x7B'},\n",
       " 'prompt': {'task_guidelines': '<|im_start|>system\\nYou are an expert at understanding legal contracts. Your job is to correctly classify legal provisions in contracts into one of the following categories.\\nCategories:{labels}\\n',\n",
       "  'labels': ['Agreements',\n",
       "   'Amendments',\n",
       "   'Adjustments',\n",
       "   'Anti-Corruption Laws',\n",
       "   'Applicable Laws',\n",
       "   'Approvals',\n",
       "   'Arbitration',\n",
       "   'Assignments',\n",
       "   'Assigns',\n",
       "   'Authority',\n",
       "   'Authorizations',\n",
       "   'Base Salary',\n",
       "   'Benefits',\n",
       "   'Binding Effects',\n",
       "   'Books',\n",
       "   'Brokers',\n",
       "   'Capitalization',\n",
       "   'Change In Control',\n",
       "   'Closings',\n",
       "   'Compliance With Laws',\n",
       "   'Confidentiality',\n",
       "   'Consent To Jurisdiction',\n",
       "   'Consents',\n",
       "   'Construction',\n",
       "   'Cooperation',\n",
       "   'Costs',\n",
       "   'Counterparts',\n",
       "   'Death',\n",
       "   'Defined Terms',\n",
       "   'Definitions',\n",
       "   'Disability',\n",
       "   'Disclosures',\n",
       "   'Duties',\n",
       "   'Effective Dates',\n",
       "   'Effectiveness',\n",
       "   'Employment',\n",
       "   'Enforceability',\n",
       "   'Enforcements',\n",
       "   'Entire Agreements',\n",
       "   'Erisa',\n",
       "   'Existence',\n",
       "   'Expenses',\n",
       "   'Fees',\n",
       "   'Financial Statements',\n",
       "   'Forfeitures',\n",
       "   'Further Assurances',\n",
       "   'General',\n",
       "   'Governing Laws',\n",
       "   'Headings',\n",
       "   'Indemnifications',\n",
       "   'Indemnity',\n",
       "   'Insurances',\n",
       "   'Integration',\n",
       "   'Intellectual Property',\n",
       "   'Interests',\n",
       "   'Interpretations',\n",
       "   'Jurisdictions',\n",
       "   'Liens',\n",
       "   'Litigations',\n",
       "   'Miscellaneous',\n",
       "   'Modifications',\n",
       "   'No Conflicts',\n",
       "   'No Defaults',\n",
       "   'No Waivers',\n",
       "   'Non-Disparagement',\n",
       "   'Notices',\n",
       "   'Organizations',\n",
       "   'Participations',\n",
       "   'Payments',\n",
       "   'Positions',\n",
       "   'Powers',\n",
       "   'Publicity',\n",
       "   'Qualifications',\n",
       "   'Records',\n",
       "   'Releases',\n",
       "   'Remedies',\n",
       "   'Representations',\n",
       "   'Sales',\n",
       "   'Sanctions',\n",
       "   'Severability',\n",
       "   'Solvency',\n",
       "   'Specific Performance',\n",
       "   'Submission To Jurisdiction',\n",
       "   'Subsidiaries',\n",
       "   'Successors',\n",
       "   'Survival',\n",
       "   'Tax Withholdings',\n",
       "   'Taxes',\n",
       "   'Terminations',\n",
       "   'Terms',\n",
       "   'Titles',\n",
       "   'Transactions With Affiliates',\n",
       "   'Use Of Proceeds',\n",
       "   'Vacations',\n",
       "   'Venues',\n",
       "   'Vesting',\n",
       "   'Waiver Of Jury Trials',\n",
       "   'Waivers',\n",
       "   'Warranties',\n",
       "   'Withholdings'],\n",
       "  'example_template': 'Example: {example}\\nOutput: {label}',\n",
       "  'few_shot_examples': 'seed.csv',\n",
       "  'few_shot_selection': 'semantic_similarity',\n",
       "  'few_shot_num': 4}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa3884b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9db58f0a28eb493193fb9ac91a0fa92e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-29 16:35:17 root WARNING: Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "The model 'MixtralForCausalLM' is not supported for text2text-generation. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n"
     ]
    }
   ],
   "source": [
    "# create an agent for labeling\n",
    "agent = LabelingAgent(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92667a39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-29 16:35:18 sentence_transformers.SentenceTransformer INFO: Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "2023-12-29 16:35:26 sentence_transformers.SentenceTransformer INFO: Use pytorch device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfb6a62432cd4a389cffa6669edf70fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cadf804b8a614ba49cf9f811538fdda5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f4e1c2840c418cb1cad3f610f9402d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "327f16bda94a42419f7a27cb9e17c0c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb1ca128847143319620cb83bc650227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c8e0b54746e4f4baf21357a2c326913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af644a9d4c3541f59d757aa1959ca0e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98c4f8fee52c4b6582d22333495abf3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d04229a73c134b4ab4ba3aa734a85ef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c675ef10ef61497a8306819f5201584e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c08ca55e2851468bba41dccaaec094dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb3183d69b8048f381153a5ec3d95aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "379f8c03990241a58d032d1ac3e4536e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31ae84b1c3c14958aae4f555a27719cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "848edc0d75a54f86afc3f649e57c5c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "103175689e6d4342a7ac3e6fb40b1660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b09b47179c43c29d8350009583f907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68b1fb01fa024266b1b02fb642db49f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3825113b57e94dc783aab3cc52e53753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c06ba2d06036498991077a02c06d6460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c470ca6b1be04f55b9280b7170a39f36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2ba21b30c564b04a644dfa3b7e03079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a0464f014447dc80f67968ba3bccf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7bc6a4cfbad4bac8f83ff28e60d9162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91e76974b464489989ced7886799930c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c402af40b4d4bf4a6207548cb8d1242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c1076652c9475eb7f7c7fb181880a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c02ecc237534bd3a898dcbac2d44010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e4339cac4f47b2b0cae8121618d959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a11f86ecc141467a9231f34498cafaf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a81576b7621545cd839e532989e775c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "669fc39a92c44215ad7a7d1f16155dbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1f105cb3b00407b856d79ad99ee70e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20cc6ca9b32744318a9ace98723565dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9aaa8e94c794e18a45b718a85d37080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3ba2caa8ca34550aafd1281eadd3349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deaba805d84d48109757bfae15d10e90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1db2fa7dfb547bbb435ef32830bbd90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9277a0b5134a4a8393dda261673aca81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d33c38964ad345019b1f4a04634df425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12e46cd362f7488da829fecb7147bf05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "885170fbe4024ed5a535c6d04edb7a04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "712c6e2e93fd4a5bb5766e98044f5b38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "524d777ee8cf4661b5ae7263cc975eb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb9ede525d9f4b5ebc55ef5df1a455ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b1f934c482746d097deb1e4970a63fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "364ceee6508f4ca0bf1ae5161c5b1e33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec474ba6448b4a0396ac539865f79799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1c4581f9b1246308c26e30b56d2b473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62dee68e56b44d31999efdf21c078ccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d9c3a377264eaa879f651441d1e290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "532477e7faf14ccb88979cffd5f449d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bc8cab403b4474e8d388f391dcd0075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb6a9935b8e47b88b9f3b465e2f6856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "526929cad57c4ca89562a9991004c157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d650d5df0ffd43148a927a5f8b94022e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "666cac574ca245a0b0ce42e4a1db9d3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a29d61c9473406e83b2499c2ec52fce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c428bbb0e94e73b3868b356b24b814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f72c3fa58670455db19fdcad6478af3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "872bf10324e1428f90f4d85be39b9e36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe3fec2b4eb477482c620294abefecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d97117adc0dd46298b01064d12ddbdd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a918a72ff0a24a1c8fd4eb6d8f4305b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1caa931baf5a4295bee9826e545cee14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2d6bd102f284a11b9f3657529fae980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33575b832b9a4a2ea8dc58183da83504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce0e3ee8b32f4dd4abe9ec3072d7794f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "492786a122284bd8b282c4b6c0f5cfc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d5e661e4e246d6a808dee2af58379f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ff72c78be56418aa703c283b8f3cb96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9f54c28731c404688e86b357cde61a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "882caf5d28d94569a309a2953b37a7a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b41e42b060c4292af7b02c06ce732ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1346f96940984abb81efce9801ede6be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da26ecc3c774f8bbad53c5afbb9fff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d13f5a35be84bc3b88836a857d3b51d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a06267239f0b42d6a9494a12359d41fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99c4e991f02049e0aa8c0e946cc2ecb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9a6519718bf44fbaaa9772e079d35c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce75c47629a44f3ebfc71dbdd4d3646a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ee084ec740b4a6f86931be2bd72ee0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a9191ad149d42e89589cf27cdb877b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c7cf33507a46b6a255efbfeeb9dc2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b00d5f129b406080824f8ca1c0c4c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a5d934f13b04368b9501f93ee3c2b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd69f79b596e400ea40ae9ec8a5d96e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a61ea3da3674b06ae6ad65caf4722be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "164c19509b7340d3b607658256a72d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dd27a14b5fc48f28531181da4f03bc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e4930dccfb42608134226ae65dd3b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1516c1a90a64aceaf882b62b4b90843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "202f7bd5c52b43829014b60f7ec11d86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69920c7e0a52472995660f05d48a84b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50c88c0fed6640b18b56ee28af029480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "493fe0ff81b64bb79ee9431fae692c8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d27b71805022410297fcee48fc9b0316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd30aabcf894d9cb728cf0d6ecf2421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16e970ca7658479bb386a9effd6fe952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f5bd129ca9e470d81e29d69bbfd01e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf28d688482f402eac21491343f6fbc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┌──────────────────────────┬──────┐\n",
       "│<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Total Estimated Cost     </span>│<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> $0.0 </span>│\n",
       "│<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Number of Examples       </span>│<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> 2000 </span>│\n",
       "│<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Average cost per example </span>│<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> $0.0 </span>│\n",
       "└──────────────────────────┴──────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┌──────────────────────────┬──────┐\n",
       "│\u001b[1;35m \u001b[0m\u001b[1;35mTotal Estimated Cost    \u001b[0m\u001b[1;35m \u001b[0m│\u001b[1;32m \u001b[0m\u001b[1;32m$0.0\u001b[0m\u001b[1;32m \u001b[0m│\n",
       "│\u001b[1;35m \u001b[0m\u001b[1;35mNumber of Examples      \u001b[0m\u001b[1;35m \u001b[0m│\u001b[1;32m \u001b[0m\u001b[1;32m2000\u001b[0m\u001b[1;32m \u001b[0m│\n",
       "│\u001b[1;35m \u001b[0m\u001b[1;35mAverage cost per example\u001b[0m\u001b[1;35m \u001b[0m│\u001b[1;32m \u001b[0m\u001b[1;32m$0.0\u001b[0m\u001b[1;32m \u001b[0m│\n",
       "└──────────────────────────┴──────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">───────────────────────────────────────────────── </span>Prompt Example<span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ──────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m───────────────────────────────────────────────── \u001b[0mPrompt Example\u001b[92m ──────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">|im_start|</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;system</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">You are an expert at understanding legal contracts. Your job is to correctly classify legal provisions in contracts</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">into one of the following categories.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Categories:Agreements</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Amendments</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Adjustments</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Anti-Corruption Laws</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Applicable Laws</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Approvals</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Arbitration</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Assignments</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Assigns</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Authority</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Authorizations</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Base Salary</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Benefits</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Binding Effects</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Books</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Brokers</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Capitalization</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Change In Control</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Closings</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Compliance With Laws</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Confidentiality</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Consent To Jurisdiction</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Consents</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Construction</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Cooperation</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Costs</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Counterparts</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Death</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Defined Terms</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Definitions</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Disability</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Disclosures</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Duties</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Effective Dates</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Effectiveness</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Employment</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Enforceability</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Enforcements</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Entire Agreements</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Erisa</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Existence</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Expenses</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Fees</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Financial Statements</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Forfeitures</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Further Assurances</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">General</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Governing Laws</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Headings</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Indemnifications</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Indemnity</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Insurances</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Integration</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Intellectual Property</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Interests</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Interpretations</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Jurisdictions</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Liens</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Litigations</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Miscellaneous</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Modifications</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">No Conflicts</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">No Defaults</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">No Waivers</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Non-Disparagement</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Notices</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Organizations</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Participations</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Payments</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Positions</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Powers</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Publicity</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Qualifications</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Records</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Releases</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Remedies</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Representations</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Sales</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Sanctions</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Severability</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Solvency</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Specific Performance</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Submission To Jurisdiction</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Subsidiaries</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Successors</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Survival</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Tax Withholdings</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Taxes</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Terminations</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Terms</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Titles</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Transactions With Affiliates</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Use Of Proceeds</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Vacations</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Venues</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Vesting</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Waiver Of Jury Trials</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Waivers</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Warranties</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Withholdings</span>\n",
       "\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">You will return the answer with just one element: </span><span style=\"color: #008000; text-decoration-color: #008000\">\"the correct label\"</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Some examples with their output answers are provided below:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Example: The Company hereby employs Executive and Executive accepts employment under the terms and conditions of </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">this Agreement.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Output: Employment</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Example: The Executive further agrees that he will cooperate fully with the Company and its counsel with respect to</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">any matter </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">including litigation, investigations, or governmental proceedings</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> in which the Executive was in any </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">way involved during his employment with the Company. The Executive shall render such cooperation in a timely manner</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">on reasonable notice from the Company.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Output: Cooperation</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Example: Executive shall provide Executive’s reasonable cooperation in connection with any action or proceeding </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">or</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">any appeal from any action or proceeding</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> which relates to events occurring during Executive’s employment </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">hereunder. This provision shall survive any termination of this Agreement.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Output: Cooperation</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Example: This Agreement shall be binding upon and inure to the benefit of Executive and the Company, and their </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">officers, directors, executives, agents, legal counsel, heirs, successors and assigns.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Output: Binding Effects</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Now I want you to label the following example:</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;|im_end|&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;|im_start|&gt;user</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Example: Executive agrees to be employed with the Company, and the Company agrees to employ Executive, during the </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Term and on the terms and conditions set forth in this Agreement. Executive agrees during the term of this </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Agreement to devote substantially all of Executive’s business time, efforts, skills and abilities to the </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">performance of Executive’s duties to the Company and to the furtherance of the Company's business.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Output: &lt;|im_end|&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;|im_start|</span><span style=\"font-weight: bold\">&gt;</span>assistant\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95m|im_start|\u001b[0m\u001b[39m>system\u001b[0m\n",
       "\u001b[39mYou are an expert at understanding legal contracts. Your job is to correctly classify legal provisions in contracts\u001b[0m\n",
       "\u001b[39minto one of the following categories.\u001b[0m\n",
       "\u001b[39mCategories:Agreements\u001b[0m\n",
       "\u001b[39mAmendments\u001b[0m\n",
       "\u001b[39mAdjustments\u001b[0m\n",
       "\u001b[39mAnti-Corruption Laws\u001b[0m\n",
       "\u001b[39mApplicable Laws\u001b[0m\n",
       "\u001b[39mApprovals\u001b[0m\n",
       "\u001b[39mArbitration\u001b[0m\n",
       "\u001b[39mAssignments\u001b[0m\n",
       "\u001b[39mAssigns\u001b[0m\n",
       "\u001b[39mAuthority\u001b[0m\n",
       "\u001b[39mAuthorizations\u001b[0m\n",
       "\u001b[39mBase Salary\u001b[0m\n",
       "\u001b[39mBenefits\u001b[0m\n",
       "\u001b[39mBinding Effects\u001b[0m\n",
       "\u001b[39mBooks\u001b[0m\n",
       "\u001b[39mBrokers\u001b[0m\n",
       "\u001b[39mCapitalization\u001b[0m\n",
       "\u001b[39mChange In Control\u001b[0m\n",
       "\u001b[39mClosings\u001b[0m\n",
       "\u001b[39mCompliance With Laws\u001b[0m\n",
       "\u001b[39mConfidentiality\u001b[0m\n",
       "\u001b[39mConsent To Jurisdiction\u001b[0m\n",
       "\u001b[39mConsents\u001b[0m\n",
       "\u001b[39mConstruction\u001b[0m\n",
       "\u001b[39mCooperation\u001b[0m\n",
       "\u001b[39mCosts\u001b[0m\n",
       "\u001b[39mCounterparts\u001b[0m\n",
       "\u001b[39mDeath\u001b[0m\n",
       "\u001b[39mDefined Terms\u001b[0m\n",
       "\u001b[39mDefinitions\u001b[0m\n",
       "\u001b[39mDisability\u001b[0m\n",
       "\u001b[39mDisclosures\u001b[0m\n",
       "\u001b[39mDuties\u001b[0m\n",
       "\u001b[39mEffective Dates\u001b[0m\n",
       "\u001b[39mEffectiveness\u001b[0m\n",
       "\u001b[39mEmployment\u001b[0m\n",
       "\u001b[39mEnforceability\u001b[0m\n",
       "\u001b[39mEnforcements\u001b[0m\n",
       "\u001b[39mEntire Agreements\u001b[0m\n",
       "\u001b[39mErisa\u001b[0m\n",
       "\u001b[39mExistence\u001b[0m\n",
       "\u001b[39mExpenses\u001b[0m\n",
       "\u001b[39mFees\u001b[0m\n",
       "\u001b[39mFinancial Statements\u001b[0m\n",
       "\u001b[39mForfeitures\u001b[0m\n",
       "\u001b[39mFurther Assurances\u001b[0m\n",
       "\u001b[39mGeneral\u001b[0m\n",
       "\u001b[39mGoverning Laws\u001b[0m\n",
       "\u001b[39mHeadings\u001b[0m\n",
       "\u001b[39mIndemnifications\u001b[0m\n",
       "\u001b[39mIndemnity\u001b[0m\n",
       "\u001b[39mInsurances\u001b[0m\n",
       "\u001b[39mIntegration\u001b[0m\n",
       "\u001b[39mIntellectual Property\u001b[0m\n",
       "\u001b[39mInterests\u001b[0m\n",
       "\u001b[39mInterpretations\u001b[0m\n",
       "\u001b[39mJurisdictions\u001b[0m\n",
       "\u001b[39mLiens\u001b[0m\n",
       "\u001b[39mLitigations\u001b[0m\n",
       "\u001b[39mMiscellaneous\u001b[0m\n",
       "\u001b[39mModifications\u001b[0m\n",
       "\u001b[39mNo Conflicts\u001b[0m\n",
       "\u001b[39mNo Defaults\u001b[0m\n",
       "\u001b[39mNo Waivers\u001b[0m\n",
       "\u001b[39mNon-Disparagement\u001b[0m\n",
       "\u001b[39mNotices\u001b[0m\n",
       "\u001b[39mOrganizations\u001b[0m\n",
       "\u001b[39mParticipations\u001b[0m\n",
       "\u001b[39mPayments\u001b[0m\n",
       "\u001b[39mPositions\u001b[0m\n",
       "\u001b[39mPowers\u001b[0m\n",
       "\u001b[39mPublicity\u001b[0m\n",
       "\u001b[39mQualifications\u001b[0m\n",
       "\u001b[39mRecords\u001b[0m\n",
       "\u001b[39mReleases\u001b[0m\n",
       "\u001b[39mRemedies\u001b[0m\n",
       "\u001b[39mRepresentations\u001b[0m\n",
       "\u001b[39mSales\u001b[0m\n",
       "\u001b[39mSanctions\u001b[0m\n",
       "\u001b[39mSeverability\u001b[0m\n",
       "\u001b[39mSolvency\u001b[0m\n",
       "\u001b[39mSpecific Performance\u001b[0m\n",
       "\u001b[39mSubmission To Jurisdiction\u001b[0m\n",
       "\u001b[39mSubsidiaries\u001b[0m\n",
       "\u001b[39mSuccessors\u001b[0m\n",
       "\u001b[39mSurvival\u001b[0m\n",
       "\u001b[39mTax Withholdings\u001b[0m\n",
       "\u001b[39mTaxes\u001b[0m\n",
       "\u001b[39mTerminations\u001b[0m\n",
       "\u001b[39mTerms\u001b[0m\n",
       "\u001b[39mTitles\u001b[0m\n",
       "\u001b[39mTransactions With Affiliates\u001b[0m\n",
       "\u001b[39mUse Of Proceeds\u001b[0m\n",
       "\u001b[39mVacations\u001b[0m\n",
       "\u001b[39mVenues\u001b[0m\n",
       "\u001b[39mVesting\u001b[0m\n",
       "\u001b[39mWaiver Of Jury Trials\u001b[0m\n",
       "\u001b[39mWaivers\u001b[0m\n",
       "\u001b[39mWarranties\u001b[0m\n",
       "\u001b[39mWithholdings\u001b[0m\n",
       "\n",
       "\n",
       "\u001b[39mYou will return the answer with just one element: \u001b[0m\u001b[32m\"the correct label\"\u001b[0m\n",
       "\n",
       "\u001b[39mSome examples with their output answers are provided below:\u001b[0m\n",
       "\n",
       "\u001b[39mExample: The Company hereby employs Executive and Executive accepts employment under the terms and conditions of \u001b[0m\n",
       "\u001b[39mthis Agreement.\u001b[0m\n",
       "\u001b[39mOutput: Employment\u001b[0m\n",
       "\u001b[39mExample: The Executive further agrees that he will cooperate fully with the Company and its counsel with respect to\u001b[0m\n",
       "\u001b[39many matter \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mincluding litigation, investigations, or governmental proceedings\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m in which the Executive was in any \u001b[0m\n",
       "\u001b[39mway involved during his employment with the Company. The Executive shall render such cooperation in a timely manner\u001b[0m\n",
       "\u001b[39mon reasonable notice from the Company.\u001b[0m\n",
       "\u001b[39mOutput: Cooperation\u001b[0m\n",
       "\u001b[39mExample: Executive shall provide Executive’s reasonable cooperation in connection with any action or proceeding \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mor\u001b[0m\n",
       "\u001b[39many appeal from any action or proceeding\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m which relates to events occurring during Executive’s employment \u001b[0m\n",
       "\u001b[39mhereunder. This provision shall survive any termination of this Agreement.\u001b[0m\n",
       "\u001b[39mOutput: Cooperation\u001b[0m\n",
       "\u001b[39mExample: This Agreement shall be binding upon and inure to the benefit of Executive and the Company, and their \u001b[0m\n",
       "\u001b[39mofficers, directors, executives, agents, legal counsel, heirs, successors and assigns.\u001b[0m\n",
       "\u001b[39mOutput: Binding Effects\u001b[0m\n",
       "\n",
       "\u001b[39mNow I want you to label the following example:\u001b[0m\n",
       "\u001b[39m<|im_end|>\u001b[0m\n",
       "\u001b[39m<|im_start|>user\u001b[0m\n",
       "\u001b[39mExample: Executive agrees to be employed with the Company, and the Company agrees to employ Executive, during the \u001b[0m\n",
       "\u001b[39mTerm and on the terms and conditions set forth in this Agreement. Executive agrees during the term of this \u001b[0m\n",
       "\u001b[39mAgreement to devote substantially all of Executive’s business time, efforts, skills and abilities to the \u001b[0m\n",
       "\u001b[39mperformance of Executive’s duties to the Company and to the furtherance of the Company's business.\u001b[0m\n",
       "\u001b[39mOutput: <|im_end|>\u001b[0m\n",
       "\u001b[39m<|im_start|\u001b[0m\u001b[1m>\u001b[0massistant\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from autolabel import AutolabelDataset\n",
    "ds = AutolabelDataset(\"test.csv\", config=config)\n",
    "agent.plan(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd703025-54d8-4349-b0d6-736d2380e966",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6173cbe4c2d4f768a3fbe1aea7fa4b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2892b11dcea44a880da528ff3fa1d63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "403de53bb22d46e19095db5c6da64ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5607ea4af225406085f366e0e6dcedfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8479a99ffea54092a14f127f35a83630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2b8613dd06f4413a5a0d75116eb5aa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a70c5fd1de74da2b3086d7b84996c74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "473b811ee7ba4f11a23afd3f39940dc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fad905b6580f4f70967f166ceee7f251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "103f9eecd1994663a248261924f20834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be1978127cab49a7be71745c0d2e1d32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baac5ccdcd8b428a90a1d517fe65316e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6ac7f0f749f404baece3c8d3fe204eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bae6084a4c248a19b99e01b2c13eb6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "764690ea065c44009bcf7bef9acc85b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c60883d5f73f41188a60ae98b0f7e9c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbb3318cdfac43d2a8fdacf0449713da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a64864becba4281afd5dbb59cc956cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75de9d0b77794ad085787c1a42c1b708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02580b819e604c4da94fe76919fb9e8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd4edfa1d3d9447dafa4385f90a9121f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "145748642631410abd2905d8da4d2e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d89bc35b0f64dacbec054f1a639a03f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2023-12-29 16:39:40 autolabel.tasks.base WARNING: LLM response is not in the labels list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "595f65fdd18b419b80ee0a86cdb033bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7d3cc58aeb4cb2969adf6a380345db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd3b3b9f489a419c8c44f696d93656d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f9667078da54bab8903cda8d95180f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "188117d6838e482dab68b20c43b79bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca36a272584b468b86f06c77a3ba5653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "087d76a5a0b3422f9bf9fa14ee6e00e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6085857b72e745d38dc2b2bc5bcf9a5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6901d1462e54dada8fe5caf159da9bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2023-12-29 16:40:43 autolabel.tasks.base WARNING: LLM response is not in the labels list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0fc5061be7c46eea73586af9cd1a324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf0b4c64f6d429492a8a3cce9b3e89d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "368f3e4cb72e4ffd91518792db0afb9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d48fdb4c4e44f2d8d1130ef214c5a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df21856db6134eb3939144b091946966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "893457a2b83a43289642cc906e2b2ccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "708e4d172a7f429a8f7c47b73615b85e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a0262e5089f42cc961ad33d028cf237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9214540a48f4ce095fa947552f0efb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7892308216bb455db1d00ce340c361ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7bb181b22094441abdb7f80a22aa2d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f08224f885042829e10766ecceac54b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f4ae4bd26ce4b8fa5792015ccc0f2f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5ea308017df49009bc10bc68dc8f1d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f176c88567941b3a74d5c74d44582a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c824512fc63a47d58bd76f71e1a9a980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e6fe0ed724344508307a1d9d04daf75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae8b632cc234b69a440e9d5e7ac8ebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2023-12-29 16:42:54 autolabel.tasks.base WARNING: LLM response is not in the labels list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cac00ec41a9543859a0887f89508f816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73f5c69abedb46d994566d4fb664732f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98026fd4ec6d45fcb3eacb60a799c3e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f564e1290e76459085f4f80d60d368c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99469c16198b45478a552ceeb68d05d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0e295276ca4b58b04da7348e8be41e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b1b5c56c29d4eb094fabced03e5c050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "defacb1541e44e62b490fef7f3cc7406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2378c7ac6c2d403c8c6527dd41ae3804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56b0e66c17354818a19dca7b11c3ace3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbbce2daba1f4b2880d09892b9442dde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a934395f14b445c29fe9f846dddae7b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ce0c70cd2f44e36904acf113e220cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a899cbf9f24ae4a64055f47a325b64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe0fdf1b134a4e2daa025badfced3d0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e088bc2fe5410d9a21575b43fcd0d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79e11b1a85d64e618d1d0543e6094bc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79629280a53b4d69b54c221e3fe220b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0dab2b9717f42c783535212c20fb130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37d5a06361694c6db47356173a769e9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fabe124908d4cf58ea187387b9d69ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00fb588ce10648058ee7db577b66c842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aac3f59a38e043bda6a81d3359c50d86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98de42db8e6741a690a5a1854e133aff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2023-12-29 16:46:02 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 720.00 MiB. GPU 1 has a total capacty of 79.15 GiB of which 625.25 MiB is free. Process 39095 has 78.53 GiB memory in use. Of the allocated memory 77.25 GiB is allocated by PyTorch, and 808.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 720.00 MiB. GPU 1 has a total capacty of 79.15 GiB\n",
       "of which 625.25 MiB is free. Process 39095 has 78.53 GiB memory in use. Of the allocated memory 77.48 GiB is \n",
       "allocated by PyTorch, and 577.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 720.00 MiB. GPU 1 has a total capacty of 79.15 GiB\n",
       "of which 625.25 MiB is free. Process 39095 has 78.53 GiB memory in use. Of the allocated memory 77.48 GiB is \n",
       "allocated by PyTorch, and 577.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35452e6103db41d197656b580fcf9155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d663f6e5e39942068052952a801e3618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d281d15358024402b56e478eb3b3b1bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a8cd10b7f345e59a8d2dbb1f69ae55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9801cc37eb74e2b8de14214dfd6655f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62788803c72044f5a7dc2987762c8eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "934be141747e4c1caa8b9b9aa4fc2cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1743e51ea0bd4d1688332163d87fc71f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2023-12-29 16:47:02 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 1.03 GiB. GPU 1 has a total capacty of 79.15 GiB of which 165.25 MiB is free. Process 39095 has 78.98 GiB memory in use. Of the allocated memory 77.47 GiB is allocated by PyTorch, and 1.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 1.03 GiB. GPU 1 has a total capacty of 79.15 GiB \n",
       "of which 165.25 MiB is free. Process 39095 has 78.98 GiB memory in use. Of the allocated memory 77.74 GiB is \n",
       "allocated by PyTorch, and 762.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 1.03 GiB. GPU 1 has a total capacty of 79.15 GiB \n",
       "of which 165.25 MiB is free. Process 39095 has 78.98 GiB memory in use. Of the allocated memory 77.74 GiB is \n",
       "allocated by PyTorch, and 762.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d0488e49ce74574916d614459748995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df6f74e02f6f4d4280e24f0c5a0c5ea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0985f526131b4c17b0ab969c25276dbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14c2ce556b744bb1a49916b1b91e0e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7382edfb350437eb714c8ebd7c867e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd2bcc5623ca46b9a1fbae77f665739f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fae0861f62545699457af9e02f71770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63626398cc9f4865bd2d3bbfe61ec711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b008fd450b642eb846d6942c4eb0320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49151cf86dc342ebb0ad7f67327ff8cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be4099d2b1704d8fa5202656626272ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0bc7adac92b4861825fc0baf154491b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f110ec44b7ef4678ac85b9ab9c1bd834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2023-12-29 16:48:42 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 714.00 MiB. GPU 1 has a total capacty of 79.15 GiB of which 653.25 MiB is free. Process 39095 has 78.50 GiB memory in use. Of the allocated memory 77.22 GiB is allocated by PyTorch, and 806.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 714.00 MiB. GPU 1 has a total capacty of 79.15 GiB\n",
       "of which 653.25 MiB is free. Process 39095 has 78.50 GiB memory in use. Of the allocated memory 77.45 GiB is \n",
       "allocated by PyTorch, and 576.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 714.00 MiB. GPU 1 has a total capacty of 79.15 GiB\n",
       "of which 653.25 MiB is free. Process 39095 has 78.50 GiB memory in use. Of the allocated memory 77.45 GiB is \n",
       "allocated by PyTorch, and 576.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c5b0007439249c0aa47dcd595c6a127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "483e8630e53d476c9b14a30d9e6af3be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2023-12-29 16:49:01 autolabel.tasks.base WARNING: LLM response is not in the labels list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fd947312a99408a90e9e420ece0c537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab0e5f2d2104463d974d4422d03abb31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad5fb9ee8d354803a6cb8e7645fa6c22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce943730ea374dedb067ee88f90998d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d17029b4da042ed9c722d94060483d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9dc61d9896f4d8582026455cf704d5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b08ae38aaf67452aba2f4381c85a3134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a9c93fb6c74956a853ee902cd801eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "720e57a43c2f455496822e8964532325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "298000b5728f488ab314fd53b0043d8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2023-12-29 16:50:30 autolabel.tasks.base WARNING: LLM response is not in the labels list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a788fa1d7824a48af6259c77782325f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d07635e7aa24a7a9ac5e49fd71654ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e56789ce87594c91a8681bdfa6449a23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fb93f13e4df4ba48c88cf51b7c0db14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc5534df88a4cb992ae88f948980fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3b0f021d0cb4b6bb7d987a7ba086a4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b423a3f5d46746f3a29abae115263fec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "124f828f582c424cb6882e2966ffa4dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f3bd446fe2646498224d8df31f181a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0545502475c403bb8a4fea0060e3e67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a903039ddfd4ae48cfa90992a0366a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba750798852f4b5d9356a91d6c110b81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a298b768e41411d943d6a92ef6481e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d70c290221a44915ae6c8f5a36fc7bb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76268c6105d74409891acf2c06904cf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10b1bac9403140248edd0f9e0e522b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2331e48884e84c599ae7894e5c9f0b64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54e7e1f04d9046618d2f64c92130294d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f219a68fc1fe494b854e60f1c8684ce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d4905917445416983b4124c99158828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5799f3e981a47fcad470b09d2b0e6a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70503714af52403288bca92c5a32869d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "727aa4781dbf4ffe8ba095be6a5fab76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85531a2813c24e61b3a25ac5c4beb601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "080657375d964053820d44e656cebbcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "319954ccf8c94f9ea16876222d58984c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b25800d8b314ba6b21234a9b28f56b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239162b1f9c245048f183d16c4653114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe1b068e4474014b1b7c246f85dcf75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddee9e847249489f8abb9fb788949562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f2b0df30cb648b4af68e5e02a1f8e05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2629de55b5b0491dac94215fa355e047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd93ee0403104d9da970fd34f6fbb0e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d99e7b53d6a4988a5d374a0d97389e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95dd9ed6bf2b4700bdfe394d639cd0ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec895fb068384257b21023f0c541e79e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "927b3f0715874e7fb98ad811652c7ba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29327b499aa74b6da40ab203e43eb47e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f993f459cc247f09ed3a6968f7fd2f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de4d85ce16e14fcc846649b5d6f6316e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2023-12-29 16:56:06 autolabel.tasks.base WARNING: LLM response is not in the labels list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c618c4d9d7824345a9f410a305bada23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce6888a178a642ebaeb4e0caf8b0ca34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "677bc941549a4bd2879633064cdaff84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6ce04bc15ce483da6ae38fcfaee9031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45f33dd4bcb04a3f90a08e031199c0e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2023-12-29 16:56:45 autolabel.tasks.base WARNING: LLM response is not in the labels list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc0c5e0ecd249c4bf3730ab6177f434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6110b03b95994fd68389b9863c1d9c2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b674b0b9f9c421c9a2dcd153ec89710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a053052c27f54a089a3897eaa2c21eab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbb4a511d0b248148ef5d4f235597c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7edb7888153c4e0b82dd5ba779420fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a8f2d9653fd4ca3b2f259528ff1d633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f826bfcf28af46dd8753302e2c34e057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee5a4336ba65494ca9b81cfb408df69b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea9ce4a72c34f77af0dabab86bb90ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db112dbd25aa4967929a72161c94ea78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd3b387ec7fe4a49abb79105c75f6691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8916e4cc46744a48879f04ce40271dde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "911129dc502648e1b5afb18595aba44b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc1efe4c571f43d9898fa48779a95a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dba23b66536446b89fa67ed2d7310feb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45415df32c104dbb9529c3ac3c282e25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c16e54ba4f344e01ba97367ed53828d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2023-12-29 16:59:12 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 944.00 MiB. GPU 1 has a total capacty of 79.15 GiB of which 99.25 MiB is free. Process 39095 has 79.04 GiB memory in use. Of the allocated memory 77.53 GiB is allocated by PyTorch, and 1.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 944.00 MiB. GPU 1 has a total capacty of 79.15 GiB\n",
       "of which 99.25 MiB is free. Process 39095 has 79.04 GiB memory in use. Of the allocated memory 77.79 GiB is \n",
       "allocated by PyTorch, and 776.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 944.00 MiB. GPU 1 has a total capacty of 79.15 GiB\n",
       "of which 99.25 MiB is free. Process 39095 has 79.04 GiB memory in use. Of the allocated memory 77.79 GiB is \n",
       "allocated by PyTorch, and 776.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03c0083523064e1586aa6456a5e6fd1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00b4c7217a944a179be5d3b3caa3515b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1565cf576d3f41d9bdb1a6c0e66f8c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "995087e565964916b744846c9426a343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "132d4f48bf0342efb3e763c3b88eba6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "541479d1fcc24954bda3b89deb489029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90336c014b87462d916aed00b7a9ecb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b224f1fd9a84ecfb5b4b4a277d1aa89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dd47b4c068d41239979fc5f236252c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5caa90faf3194d3bb2e8ef93947f9710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7149507649b845c0afdf862109cd436a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f60957954bb4af08bfd7218ebb79325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d2fdf9a550b404dbc057530f0555199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31ebfb77e61d473eb6f5b86f0f8cf5ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "029bf5758f8f4c88a69e2b0d29d8e75a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2023-12-29 17:01:40 autolabel.tasks.base WARNING: LLM response is not in the labels list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9232482eb97e4918bd8902a44dff4182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6465fc5f512440a9933dad3772c8999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85a47969ece849f49731e8074669d847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64602a19fe0e438d9cb417952a8e1ae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6a4e697454a4c539df999439e3accf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6644c6060abd42dbb2c0be7ab9aa9783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf9e28a0b4d4373b8663c3b475c7bff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2591a54ec18146b6859ab3d446ba7836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f21a159a46554c1cb4b2e09b946237ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "972733ba6421451f8ef52c6019bfcc4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcda2f9640c44b68b74f1bdcd85eff28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "977957907fd34d90b58718560f4a69ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df693ba2a37f4fa2a4ecc5e0b3a268c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2023-12-29 17:03:23 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 1.42 GiB. GPU 1 has a total capacty of 79.15 GiB of which 759.25 MiB is free. Process 39095 has 78.40 GiB memory in use. Of the allocated memory 77.89 GiB is allocated by PyTorch, and 17.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 1.42 GiB. GPU 1 has a total capacty of 79.15 GiB \n",
       "of which 759.25 MiB is free. Process 39095 has 78.40 GiB memory in use. Of the allocated memory 76.80 GiB is \n",
       "allocated by PyTorch, and 1.10 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 1.42 GiB. GPU 1 has a total capacty of 79.15 GiB \n",
       "of which 759.25 MiB is free. Process 39095 has 78.40 GiB memory in use. Of the allocated memory 76.80 GiB is \n",
       "allocated by PyTorch, and 1.10 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27b723a8d9704807a309fe4ecf05ff46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "979e889ca025423bb911c84c739d9621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "951ddb7c12f34e7380131f237abc8e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1312ae41e7144aaea80fb92babb6b755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5af61311c764535be3456231a35ff9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b8657ca6f8a4d19bf0825ad0eef1264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e43ffa5251c42f68c8c6613e3254df0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ace164e67bf4d6686d72a372e000d56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a1bd3f0dee94401b95ea335fcbcb80b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac1c8ddceff546a8b7605ef21fe632ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e69f3610d6c4d09956d4ac3147d29d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2804808e81204558ad63b947609ceb05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad01742035bd445fbc2cf42947c43363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da22ca4e7677494d8680c4df80b013f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dda991fb0394729a93797a15bd310d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce82fd1fa2124d8fa029b2423695d700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e978b26fd84271b45331c4767b84d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a334992d6d2f498bb1fd84372637ed4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce624929e6cf432d85ebb6d6806d90e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2f2446bf31645e4b258af364c401814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06d2dee474014fb1bd6a7d69f1f72f71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2023-12-29 17:06:03 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 908.00 MiB. GPU 1 has a total capacty of 79.15 GiB of which 183.25 MiB is free. Process 39095 has 78.96 GiB memory in use. Of the allocated memory 77.48 GiB is allocated by PyTorch, and 1009.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 908.00 MiB. GPU 1 has a total capacty of 79.15 GiB\n",
       "of which 183.25 MiB is free. Process 39095 has 78.96 GiB memory in use. Of the allocated memory 77.74 GiB is \n",
       "allocated by PyTorch, and 747.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 908.00 MiB. GPU 1 has a total capacty of 79.15 GiB\n",
       "of which 183.25 MiB is free. Process 39095 has 78.96 GiB memory in use. Of the allocated memory 77.74 GiB is \n",
       "allocated by PyTorch, and 747.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13592f4f805e4c85989f0fa4a4efc059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff4530be827f4fdeaece4ec092e7e658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2023-12-29 17:06:25 autolabel.tasks.base WARNING: LLM response is not in the labels list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb778c8125e547a18c9bf43e54cd286a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d80af3d9b3b4c69a3ec304edb455063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fa0e11f15ce4dbd9f11b6dcd042e58b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4966d26dc304cbda60b7dac92ba4c74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b72e9f80bf44faab42ee5ca9affae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2023-12-29 17:07:05 autolabel.tasks.base WARNING: LLM response is not in the labels list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7fe7e52ce964fb5b971f7f0306ff355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d64425358da14c3ca3bef49aa87d5a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "636736711d2f42998f8c5f3e6a2825c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "750810e8db7746a7a98ea67667bf8896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b96e1c555d245b1b42ef611254e0da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2023-12-29 17:07:51 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 768.00 MiB. GPU 1 has a total capacty of 79.15 GiB of which 507.25 MiB is free. Process 39095 has 78.64 GiB memory in use. Of the allocated memory 77.31 GiB is allocated by PyTorch, and 860.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 768.00 MiB. GPU 1 has a total capacty of 79.15 GiB\n",
       "of which 507.25 MiB is free. Process 39095 has 78.64 GiB memory in use. Of the allocated memory 77.55 GiB is \n",
       "allocated by PyTorch, and 620.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 768.00 MiB. GPU 1 has a total capacty of 79.15 GiB\n",
       "of which 507.25 MiB is free. Process 39095 has 78.64 GiB memory in use. Of the allocated memory 77.55 GiB is \n",
       "allocated by PyTorch, and 620.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9644f17b0147434bbd5db5f5265efa4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2ce8cb950364347975cf2167ac46660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e90a84f5f1b418690f4807e581e8ec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b107391f6d74c3fb264b3a1cdadb3d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dd82785ecbd457fac5c6d4b9e3a6453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "068ba1a238b7431bad632e3ccd139291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64ca3b9bdcf340ef94e51413fa6daa54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4af1215adab745f88e796679308780c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9da103d982d4123a1367546dc08e3ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9563b66d0965423eb2d33dc713087bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2023-12-29 17:09:25 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 780.00 MiB. GPU 1 has a total capacty of 79.15 GiB of which 461.25 MiB is free. Process 39095 has 78.69 GiB memory in use. Of the allocated memory 77.33 GiB is allocated by PyTorch, and 893.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 780.00 MiB. GPU 1 has a total capacty of 79.15 GiB\n",
       "of which 461.25 MiB is free. Process 39095 has 78.69 GiB memory in use. Of the allocated memory 77.56 GiB is \n",
       "allocated by PyTorch, and 651.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 780.00 MiB. GPU 1 has a total capacty of 79.15 GiB\n",
       "of which 461.25 MiB is free. Process 39095 has 78.69 GiB memory in use. Of the allocated memory 77.56 GiB is \n",
       "allocated by PyTorch, and 651.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8d165e81a7d4eee8d7c445eca62bcf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e171d2e4e14543bd9f524203c49d11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3e50f57a90a4e469ce8abf0d095300b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc9cd78821d6487389f7145ed12102e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83ce075b24784d31854b8f9ed7f79db2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5efc43c945064249b36977f21f062112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3a9942a40224593847cb14ac8eebfeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebc83cca821149c485acb2a4549aed60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a64c19e034ff4eb8b32b6c06cd17296e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2023-12-29 17:10:42 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 866.00 MiB. GPU 1 has a total capacty of 79.15 GiB of which 329.25 MiB is free. Process 39095 has 78.82 GiB memory in use. Of the allocated memory 77.41 GiB is allocated by PyTorch, and 935.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 866.00 MiB. GPU 1 has a total capacty of 79.15 GiB\n",
       "of which 329.25 MiB is free. Process 39095 has 78.82 GiB memory in use. Of the allocated memory 77.66 GiB is \n",
       "allocated by PyTorch, and 680.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 866.00 MiB. GPU 1 has a total capacty of 79.15 GiB\n",
       "of which 329.25 MiB is free. Process 39095 has 78.82 GiB memory in use. Of the allocated memory 77.66 GiB is \n",
       "allocated by PyTorch, and 680.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b180ebde997b4fa781bb29da3ea66509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2023-12-29 17:10:50 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 796.00 MiB. GPU 1 has a total capacty of 79.15 GiB of which 329.25 MiB is free. Process 39095 has 78.82 GiB memory in use. Of the allocated memory 77.36 GiB is allocated by PyTorch, and 988.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 796.00 MiB. GPU 1 has a total capacty of 79.15 GiB\n",
       "of which 329.25 MiB is free. Process 39095 has 78.82 GiB memory in use. Of the allocated memory 77.60 GiB is \n",
       "allocated by PyTorch, and 743.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 796.00 MiB. GPU 1 has a total capacty of 79.15 GiB\n",
       "of which 329.25 MiB is free. Process 39095 has 78.82 GiB memory in use. Of the allocated memory 77.60 GiB is \n",
       "allocated by PyTorch, and 743.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a56bf723ee64185968e9087d8f1398e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "545150be588849959841e45cdd7fff26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15413956236f4d08b26885e2eafe6cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b77c49cd58034859be7039a38116fbd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05f407c5a372496f9f3924e31646b64c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa39b6d9f63458b8d9976d09cecf130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c3e2447629a41c59dfc05a755674b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b3277600729469389ac9a7e5b792ce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14fe4ee9e83445db8cdc1d90941688fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c75300f2644d4076b473edb47f4c696e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef5f8d63473b4068b5719e057d9051ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d0a5ccb87624a6481650c7561aa3597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6a452d23f574306b2d17cfb051e4e58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eda3a9b0cacd483ea5b0b2acb6890446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4670b5bb24b4e25a536945e8974fa91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "903b1d7e34774031a614391ef09f9cfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24b31438fdc24063a1539030f6885f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b069bfa50e54ee590a4d450a4b10c53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e12a9b02e2f419f954f9daf923fa594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7650dba50e2449afb3beb2bccc9e1075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f84613a073440d997c8a444566fba31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bf34a98e63446d1969741c05bd7968f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2fd7435f41d401abec872dac5af161c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abe0f1205916485c86e6b3bfab946e18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ce7a38fa804ff78242e32e35fa9f65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2023-12-29 17:14:13 autolabel.tasks.base WARNING: LLM response is not in the labels list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73d8eb6b44b941fe8dff0ac06c0b0295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "536b18c0dec945b59b6e95c71d5b4d65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93eca59512964624a713c42afd529363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2023-12-29 17:14:44 autolabel.tasks.base WARNING: LLM response is not in the labels list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8acb7a608d0486a901fc094cf014d97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b72d2add41a046078573fb85ab933de1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a46920cf3f8a439486ce51532933394a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a724df7dd084929be4839c0b8eb1c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c80fb7011ee436789ea59908ce65bfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43fa48e422174e3eb93bec9e5aa34044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f2df920586f4dba9ef0cca6c0b6f7db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0cc6c0bb9848b88d4efafe8c3eaf78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c481ca137934358a60f7d078cdfad15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2023-12-29 17:15:39 autolabel.tasks.base WARNING: LLM response is not in the labels list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e2d934a97504b27bbed663684759aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "502f2be9142c4d61a1f4a67c60ed8b2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a565213d10204f57b05d9f7f3e2ae6d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "753dd5b45128467ea2424c3109ca8c5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a95dfaadeb294dbab50f387c3f8de4fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9276299d74b4e6fbcb38428650fc598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae28cdc44e6440a39c9da261e4437063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2023-12-29 17:16:29 autolabel.tasks.base WARNING: LLM response is not in the labels list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76219ced46974043bd05feeab4d7ceba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c4c6b5dd6604067bce63185f948d991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "597225055a1e44fcaf36b10cdc7ecabd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdfe6938f3964250b7c0f2fcf6e9e3c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a87633e0dd944a8ae8b0b26693ba1b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2023-12-29 17:17:23 autolabel.tasks.base WARNING: LLM response is not in the labels list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71d77257486d4bd29a019ddee1958c00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67b27c5c36354c2da562b7dc73a3ec23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d80f4568a214b4eb53efa81a5f7c6f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6803ad90fcd641c6ab02cee2f10a8275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c8a6d111af4813b8fc7b76fc643085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cfc1599d82c4d54b6c8dbb5e66d699c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2ec13a18e944b7790595af38fb31a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be214ddb2c384c248b6d0755c9aae8d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e78c0d9816b4d3f8bb95a28064e90f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7dee2e296b44bb0956c6ede8f1b8f0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2023-12-29 17:18:34 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 1 has a total capacty of 79.15 GiB of which 65.25 MiB is free. Process 39095 has 79.07 GiB memory in use. Of the allocated memory 77.57 GiB is allocated by PyTorch, and 1.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 1 has a total capacty of 79.15 \n",
       "GiB of which 65.25 MiB is free. Process 39095 has 79.07 GiB memory in use. Of the allocated memory 77.84 GiB is \n",
       "allocated by PyTorch, and 765.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 1 has a total capacty of 79.15 \n",
       "GiB of which 65.25 MiB is free. Process 39095 has 79.07 GiB memory in use. Of the allocated memory 77.84 GiB is \n",
       "allocated by PyTorch, and 765.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8151f9bcdf34292ba479d2ac01dba1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "559d2b2b853149418d92d000778e20ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad30c68fb18149ca97545156a0f3f6ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb1452ea98c849049589fddebacf331c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1540470cfb442878670dc0d86648398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21f0646306f94ff690ea01571050d0c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "940643fa872e4c39944cfcf3bef04806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be7a13fdb82046e48632205c043f93ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42b041b8c68e4c9ab67cdf2c05a73bf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2023-12-29 17:19:38 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 904.00 MiB. GPU 1 has a total capacty of 79.15 GiB of which 247.25 MiB is free. Process 39095 has 78.90 GiB memory in use. Of the allocated memory 77.46 GiB is allocated by PyTorch, and 969.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 904.00 MiB. GPU 1 has a total capacty of 79.15 GiB\n",
       "of which 247.25 MiB is free. Process 39095 has 78.90 GiB memory in use. Of the allocated memory 77.72 GiB is \n",
       "allocated by PyTorch, and 707.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 904.00 MiB. GPU 1 has a total capacty of 79.15 GiB\n",
       "of which 247.25 MiB is free. Process 39095 has 78.90 GiB memory in use. Of the allocated memory 77.72 GiB is \n",
       "allocated by PyTorch, and 707.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e2c3946835942d88cc71271e844d40b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "481c3afeee6e469eae55226c83e0b07f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b71eee573a364f2b95a372c919145296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a261c6918f441e99422f333dc4d067b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1046e3855ec4401ea8a4b351d4d338a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbd416b98d694afe8491f03aa6c8905e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f1871f642334c1aa6f372926eb2b9e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17efc1ea321545dcb11588704770ee82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e52b44ff3b054d76baaa86657265331e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b08b83a7f8143858a51a81d839b6d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f575dfeb514f498a07c88c1e38cd4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ad1d494d6742cca0b0b1b60c8c272c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf32c15a71db4ee299649591c63ef66e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de64af0d96b64a18bbac3399409efe10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae3118221f54de18da21b06b215b711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b76a66acbab04ff78214956b8ea6544f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c654a2b92fbc46e5affa6d59649ef5b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d1f96e9083248b581abdede5e79126c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6282c23cc6604cf08763dc707e9240d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "249d38ba0430450caea85265f8a3f42b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a3cd221805c4125874739e95c27054e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33d43e366cdd4be7a94df86b7af36366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff48090b6f154cc48373fb888b882d4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58a21ca98e0a4c5796ee87e0eef75d1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44dfa796d51947299c8b1e8195b005b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "751064bc105b492780a4b10d1e47a4f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b01033a9c3e49668c63068f49f4d681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4c847895f7c48af9584b44b7bca65de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ec35d97f3eb4888b30bab1b9eb9fb70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb3f7208dc04f36a2b8545d272ced85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2023-12-29 17:24:24 autolabel.tasks.base WARNING: LLM response is not in the labels list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88a54a8e2cf14779a6bcd9815b827ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac2dc9645a084c08b4fe74627b7ff419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f87c1cd9d8b4bd6b83cfc6aca791c10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96691709d19f46c4952a0e819c42a25b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ccb37f969964c9395ed9459c37ae012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52eed3e72797460bab37c5dcb40e17da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "128b5d0a5245448fb277c727cfa62881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8372ecd3a4cd410cb8745bb7378b9280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2023-12-29 17:25:20 autolabel.tasks.base WARNING: LLM response is not in the labels list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91819eac273f4d178510dd2dd3d7925f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c7275b5624e49b5b8a5f5fb62ff8b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02527ab0ab6447b18b5c01632e52eed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f55daeec13c4224888fd6d8de472622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0055fecd388b44219175f47cf5cf4f98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa84104de91a478984bb55008a59bb21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba8cede11aef4e649ecf2eb46f3b633b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b1ad28a66b44e58c89383080c6cb37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4da0dd82d06e48a9945ed49eba47a19a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c3fe842acd447eb1460e1650c077ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f682b6cece784815b9fe72a1dcfb9bbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beac956734c54678ab768b9dc5f125b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e8a0d249914f4eaa708f3b8f0b1896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d9053f5b61a4d528924bd262ae31c39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6873b7d3d7148f3a6bb1373c440e075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "340569577ce84f1eafd823b2c6fe32a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "338a1ed5fd9d47f2b755c0d7b71c5f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3570b9680e6842cc9f500b8b284785de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bae6c0c41bc48b18b52f46047d51c45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d258b9f3a9754acbb6e750a8f282b285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2023-12-29 17:27:53 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 1 has a total capacty of 79.15 GiB of which 271.25 MiB is free. Process 39095 has 78.87 GiB memory in use. Of the allocated memory 77.41 GiB is allocated by PyTorch, and 1001.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 1 has a total capacty of 79.15 \n",
       "GiB of which 271.25 MiB is free. Process 39095 has 78.87 GiB memory in use. Of the allocated memory 77.68 GiB is \n",
       "allocated by PyTorch, and 723.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 1 has a total capacty of 79.15 \n",
       "GiB of which 271.25 MiB is free. Process 39095 has 78.87 GiB memory in use. Of the allocated memory 77.68 GiB is \n",
       "allocated by PyTorch, and 723.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cc6eb1a1e674a759918574c5e7241b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f43cef8f68d47cab8caf9c01b61173b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1fd6f22b46e4c5084f845ed6d15a3f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2023-12-29 17:28:21 autolabel.tasks.base WARNING: LLM response is not in the labels list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e934f48cdbed48b3ac773fe74e39cb68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae5f11303d864ee6a8bb01fe0ff81e43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2023-12-29 17:28:32 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 808.00 MiB. GPU 1 has a total capacty of 79.15 GiB of which 391.25 MiB is free. Process 39095 has 78.76 GiB memory in use. Of the allocated memory 77.42 GiB is allocated by PyTorch, and 869.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 808.00 MiB. GPU 1 has a total capacty of 79.15 GiB\n",
       "of which 391.25 MiB is free. Process 39095 has 78.76 GiB memory in use. Of the allocated memory 77.66 GiB is \n",
       "allocated by PyTorch, and 623.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 808.00 MiB. GPU 1 has a total capacty of 79.15 GiB\n",
       "of which 391.25 MiB is free. Process 39095 has 78.76 GiB memory in use. Of the allocated memory 77.66 GiB is \n",
       "allocated by PyTorch, and 623.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84ad15b2e284103ab73aa21333840f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc4dcce287044cf1be273f3afb6eb400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "414d36a9c4e24e3985782772b3d61520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2023-12-29 17:28:51 autolabel.tasks.base WARNING: LLM response is not in the labels list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb50e83d7bdc470dbc47639e03714a2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed92dc07280f4244ab40524461941b12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a288101b2fa47b18cc18616d4b73930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2023-12-29 17:29:20 autolabel.tasks.base WARNING: LLM response is not in the labels list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0402386755314cdd8e0912d6f7474b12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2023-12-29 17:29:27 autolabel.tasks.base WARNING: LLM response is not in the labels list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21e62156387245ac989ca576df0bbb76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2023-12-29 17:29:35 autolabel.tasks.base WARNING: LLM response is not in the labels list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "324e90eacf6243379ef941c74ddd7b86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2023-12-29 17:29:40 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 1.04 GiB. GPU 1 has a total capacty of 79.15 GiB of which 127.25 MiB is free. Process 39095 has 79.01 GiB memory in use. Of the allocated memory 77.48 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 1.04 GiB. GPU 1 has a total capacty of 79.15 GiB \n",
       "of which 127.25 MiB is free. Process 39095 has 79.01 GiB memory in use. Of the allocated memory 77.76 GiB is \n",
       "allocated by PyTorch, and 781.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 1.04 GiB. GPU 1 has a total capacty of 79.15 GiB \n",
       "of which 127.25 MiB is free. Process 39095 has 79.01 GiB memory in use. Of the allocated memory 77.76 GiB is \n",
       "allocated by PyTorch, and 781.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e199190a466477494a39b68aa6840b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c22897fe3a6a471389d22e316da939d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cbf12d91b534e0eaa742fd29ca46f00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ca7c5d63564435b2c6a8aaa8eecc8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8187735972b74d1aa0ddcb07995e9c62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57500bfe7fc442e189a839f75fa29746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a226e9e5a0df442dabf1787456be1c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd4dc935498348b2bf885acbac81abd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "285a215fbd494a5ba3897fee6056c4e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6437e4a4eb9d4835a981573efa3f6928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86dff771c81342aabb521e6b7d5de8d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83ac4b8d51d1490fa497f14bc3ba424c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2023-12-29 17:31:15 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 1.13 GiB. GPU 1 has a total capacty of 79.15 GiB of which 1.06 GiB is free. Process 39095 has 78.08 GiB memory in use. Of the allocated memory 77.56 GiB is allocated by PyTorch, and 28.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 1.13 GiB. GPU 1 has a total capacty of 79.15 GiB \n",
       "of which 1.06 GiB is free. Process 39095 has 78.08 GiB memory in use. Of the allocated memory 76.72 GiB is \n",
       "allocated by PyTorch, and 888.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 1.13 GiB. GPU 1 has a total capacty of 79.15 GiB \n",
       "of which 1.06 GiB is free. Process 39095 has 78.08 GiB memory in use. Of the allocated memory 76.72 GiB is \n",
       "allocated by PyTorch, and 888.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6141d61724f543d8bec8bc3ba658ed93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b6a459903c945c28af99ab511cb9cc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "557c7e1088944250903ade1354ec01a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c94ad64b77a04799b7462d7687ad5ea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52a36a5c0af842cf9ae5968caa85f00a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf354632de9d448eb4ed4c294241ceb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab741917c1a14eed99bd91ab210d5d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be2d377c167c42d097ec45d48ac3d96b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2023-12-29 17:32:29 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 800.00 MiB. GPU 1 has a total capacty of 79.15 GiB of which 415.25 MiB is free. Process 39095 has 78.73 GiB memory in use. Of the allocated memory 77.35 GiB is allocated by PyTorch, and 913.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 800.00 MiB. GPU 1 has a total capacty of 79.15 GiB\n",
       "of which 415.25 MiB is free. Process 39095 has 78.73 GiB memory in use. Of the allocated memory 77.59 GiB is \n",
       "allocated by PyTorch, and 668.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 800.00 MiB. GPU 1 has a total capacty of 79.15 GiB\n",
       "of which 415.25 MiB is free. Process 39095 has 78.73 GiB memory in use. Of the allocated memory 77.59 GiB is \n",
       "allocated by PyTorch, and 668.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa40feca49a84184aa7b9020bd5c9ac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96718d9cf21843dea9fb112e4becc986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdcf3db02c8e49b7a0853205d6df2228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79b9359646084403b178a2914121e11c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12ae4b6dc5f34650bff03a0c50a4b99a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a0c3e5b901d4035840c1b17b042d584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d24fb8ac1492445f88551f517022c746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2023-12-29 17:33:27 autolabel.tasks.base WARNING: LLM response is not in the labels list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddfeeee4f9134ec5909a77ddb6ead9b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f42d04443d040e391180108ade8f75b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d77b263d51554ef5aa849b8ddb255bb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c11a8c6074245e0b3b4209d998e6a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6283de24d4041ea95d3f711b7975630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6806375734294389876f69a80b9ae61b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f87aa01a3e48bab045d824f693d9bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9bc7ba9c623462fb140c8cc3cbfd3f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d57d2da403574b71bdbecad9032dd2a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80513f3522024caabdc17d35b3deca87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "956c32c2924945c98baecb7305dcf702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bfe8202f66b4776b7b78071ea094851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bac3cb2aabe417da73359fc78697c50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b50bf893284c908817de347e225a92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1288c372e48b46fbaced09988a0dc5a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4026772fb8841cbbcbf254088c71c49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2023-12-29 17:35:34 autolabel.tasks.base WARNING: LLM response is not in the labels list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a85509dcc9e144bd8dd03686a11de754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc72704cc8bd40ebbeb0dc426f27d038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e552e8c3f1c5457ab2522a87597276d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bc72de274864c21b13f6727ca3cc414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c20b80edf4534bcebfd8f866f3f59760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2023-12-29 17:37:46 autolabel.tasks.base WARNING: LLM response is not in the labels list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de4602efea8842cdb6bafcde1a6132df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdee77b2851843cdab715cc6564dd66a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b350722ef08a44b8be46a69b9f7c702b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6c8abf397a742a49b8b44be723c4073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97b9bd3a7a144ff9a85c764526361a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81881eda59984582b0c9a7ded14348f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa4d108fcc91470e9af6512149d40840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e46d063162a4fb28538c91bd1e065f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad66b379d01b43aa9ee25f2a8c9cf665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b21aba561f144c1d882f71a725fdea31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ce9f5cfc90b4d5bb6de4acfb3ce080f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2023-12-29 17:39:05 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 932.00 MiB. GPU 1 has a total capacty of 79.15 GiB of which 119.25 MiB is free. Process 39095 has 79.02 GiB memory in use. Of the allocated memory 77.54 GiB is allocated by PyTorch, and 1014.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 932.00 MiB. GPU 1 has a total capacty of 79.15 GiB\n",
       "of which 119.25 MiB is free. Process 39095 has 79.02 GiB memory in use. Of the allocated memory 77.80 GiB is \n",
       "allocated by PyTorch, and 748.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 932.00 MiB. GPU 1 has a total capacty of 79.15 GiB\n",
       "of which 119.25 MiB is free. Process 39095 has 79.02 GiB memory in use. Of the allocated memory 77.80 GiB is \n",
       "allocated by PyTorch, and 748.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24bf24b7e1d5443cb670e07bab2d7226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "310cd4cb930d485a96706c75526b0ff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d742f8479d924d6abc3c66563c8707f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10a96882d3eb4a2bb28bde0318faf380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "719a5d16188f41bdbf3fb02933b48ac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a83ccb6053e438d85def3a90f8c8966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c1c14fe04d487c87e05e66f1e33528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8edb7a3f7bef441b92600dacc93699f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12e75129dbb14a908f072f91e5b5475a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: \n",
       "`do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based \n",
       "generation modes. You should set `do_sample=True` or unset `temperature`.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# now, do the actual labeling\n",
    "ds = agent.run(ds, max_items=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278e12eb",
   "metadata": {},
   "source": [
    "We are at 71% accuracy when labeling the first 100 examples. Let's see if we can use confidence scores to improve accuracy further by removing the less confident examples from our labeled set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7645ab",
   "metadata": {},
   "source": [
    "### Compute confidence scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2ae2d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start computing confidence scores (using Refuel's LLMs)\n",
    "os.environ['REFUEL_API_KEY'] = 'xxxxxxxxxxxxxxxxx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5fbc1264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set `compute_confidence` -> True\n",
    "config[\"model\"][\"compute_confidence\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1998f5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = LabelingAgent(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "119e6f22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 15:03:28 langchain.embeddings.openai WARNING: Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/opt/homebrew/lib/python3.10/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/opt/homebrew/lib/python3.10/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┌──────────────────────────┬─────────┐\n",
       "│<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Total Estimated Cost     </span>│<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> $8.334  </span>│\n",
       "│<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Number of Examples       </span>│<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> 2000    </span>│\n",
       "│<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Average cost per example </span>│<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> $0.0042 </span>│\n",
       "└──────────────────────────┴─────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┌──────────────────────────┬─────────┐\n",
       "│\u001b[1;35m \u001b[0m\u001b[1;35mTotal Estimated Cost    \u001b[0m\u001b[1;35m \u001b[0m│\u001b[1;32m \u001b[0m\u001b[1;32m$8.334 \u001b[0m\u001b[1;32m \u001b[0m│\n",
       "│\u001b[1;35m \u001b[0m\u001b[1;35mNumber of Examples      \u001b[0m\u001b[1;35m \u001b[0m│\u001b[1;32m \u001b[0m\u001b[1;32m2000   \u001b[0m\u001b[1;32m \u001b[0m│\n",
       "│\u001b[1;35m \u001b[0m\u001b[1;35mAverage cost per example\u001b[0m\u001b[1;35m \u001b[0m│\u001b[1;32m \u001b[0m\u001b[1;32m$0.0042\u001b[0m\u001b[1;32m \u001b[0m│\n",
       "└──────────────────────────┴─────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">───────────────────────────────────────────────── </span>Prompt Example<span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ──────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m───────────────────────────────────────────────── \u001b[0mPrompt Example\u001b[92m ──────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert at understanding legal contracts. Your job is to correctly classify legal provisions in contracts into one of the following categories.\n",
      "Categories:Agreements\n",
      "Amendments\n",
      "Anti-Corruption Laws\n",
      "Applicable Laws\n",
      "Approvals\n",
      "Arbitration\n",
      "Assignments\n",
      "Assigns\n",
      "Authority\n",
      "Authorizations\n",
      "Base Salary\n",
      "Benefits\n",
      "Binding Effects\n",
      "Books\n",
      "Brokers\n",
      "Capitalization\n",
      "Change In Control\n",
      "Closings\n",
      "Compliance With Laws\n",
      "Confidentiality\n",
      "Consent To Jurisdiction\n",
      "Consents\n",
      "Construction\n",
      "Cooperation\n",
      "Costs\n",
      "Counterparts\n",
      "Death\n",
      "Defined Terms\n",
      "Definitions\n",
      "Disability\n",
      "Disclosures\n",
      "Duties\n",
      "Effective Dates\n",
      "Effectiveness\n",
      "Employment\n",
      "Enforceability\n",
      "Enforcements\n",
      "Entire Agreements\n",
      "Erisa\n",
      "Existence\n",
      "Expenses\n",
      "Fees\n",
      "Financial Statements\n",
      "Forfeitures\n",
      "Further Assurances\n",
      "General\n",
      "Governing Laws\n",
      "Headings\n",
      "Indemnifications\n",
      "Indemnity\n",
      "Insurances\n",
      "Integration\n",
      "Intellectual Property\n",
      "Interests\n",
      "Interpretations\n",
      "Jurisdictions\n",
      "Liens\n",
      "Litigations\n",
      "Miscellaneous\n",
      "Modifications\n",
      "No Conflicts\n",
      "No Defaults\n",
      "No Waivers\n",
      "Non-Disparagement\n",
      "Notices\n",
      "Organizations\n",
      "Participations\n",
      "Payments\n",
      "Positions\n",
      "Powers\n",
      "Publicity\n",
      "Qualifications\n",
      "Records\n",
      "Releases\n",
      "Remedies\n",
      "Representations\n",
      "Sales\n",
      "Sanctions\n",
      "Severability\n",
      "Solvency\n",
      "Specific Performance\n",
      "Submission To Jurisdiction\n",
      "Subsidiaries\n",
      "Successors\n",
      "Survival\n",
      "Tax Withholdings\n",
      "Taxes\n",
      "Terminations\n",
      "Terms\n",
      "Titles\n",
      "Transactions With Affiliates\n",
      "Use Of Proceeds\n",
      "Vacations\n",
      "Venues\n",
      "Vesting\n",
      "Waiver Of Jury Trials\n",
      "Waivers\n",
      "Warranties\n",
      "Withholdings\n",
      "\n",
      "\n",
      "You will return the answer with just one element: \"the correct label\"\n",
      "\n",
      "Some examples with their output answers are provided below:\n",
      "\n",
      "Example: The Company hereby employs Executive and Executive accepts employment under the terms and conditions of this Agreement.\n",
      "Output: Employment\n",
      "\n",
      "Example: This Agreement, including its attached Exhibit A, constitutes the entire employment agreement between Executive and the Company regarding the terms and conditions of his employment. This Agreement supersedes all prior negotiations, representations or agreements between Executive and the Company, whether written or oral, concerning Executive’s employment.\n",
      "Output: Entire Agreements\n",
      "\n",
      "Example: Executive agrees that during the Employment Period, Executive will devote his/her full business time, energies and talents to serving as the Senior Vice President Community Banking of the Employer, at the direction of the Chief Executive Officer of the Employwer  (the “ CEO ”).  Executive shall have such duties and responsibilities as may be assigned to Executive from time to time by the CEO, which duties and responsibilities shall be commensurate with Executive’s position, shall perform all duties assigned to Executive faithfully and efficiently, subject to the direction of the CEO, and shall have such authorities and powers as are inherent to the undertakings applicable to Executive’s position and necessary to carry out the responsibilities and duties required of Executive hereunder.  Executive will perform the duties required by this Agreement at the Company’s principal place of business unless the nature of such duties requires otherwise.  Notwithstanding the foregoing, during the Employment Period, Executive may devote reasonable time to activities other than those required under this Agreement, including activities of a charitable, educational, religious or similar nature (including professional associations) to the extent such activities do not, in the reasonable judgment of the CEO, inhibit, prohibit, interfere with or conflict with Executive’s duties under this Agreement or conflict in any material way with the business of the Employer and its Affiliates; provided, however, that Executive shall not serve on the board of directors of any business (other than the Employer or its Affiliates) or hold any other position with any business without receiving the prior written consent of the CEO.\n",
      "Output: Duties\n",
      "\n",
      "Example: Executive represents that Executive's employment by the Company and the performance by Executive of her obligations under this Agreement do not, and shall not, breach any agreement, including, but not limited to, any agreement that obligates him to keep in confidence any trade secrets or confidential or proprietary information of her or of any other party, to perform services for any other party or to refrain from competing, directly or indirectly, with the business of any other party. Executive shall not disclose to the Company or use any trade secrets or confidential or proprietary information of any other party.\n",
      "Output: Representations\n",
      "\n",
      "Now I want you to label the following example:\n",
      "Example: Executive agrees to be employed with the Company, and the Company agrees to employ Executive, during the Term and on the terms and conditions set forth in this Agreement. Executive agrees during the term of this Agreement to devote substantially all of Executive’s business time, efforts, skills and abilities to the performance of Executive’s duties to the Company and to the furtherance of the Company's business.\n",
      "Output: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from autolabel import AutolabelDataset\n",
    "ds = AutolabelDataset(\"test.csv\", config=config)\n",
    "agent.plan(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63c74705",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 15:04:00 autolabel.labeler INFO: Task run already exists.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">There is an existing task with following details: <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'3001625978'</span> <span style=\"color: #808000; text-decoration-color: #808000\">created_at</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">datetime</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.datetime</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">42</span>,\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">51</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">302325</span><span style=\"font-weight: bold\">)</span> <span style=\"color: #808000; text-decoration-color: #808000\">task_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'0890358f0f66b2f3e1284f6a180dc84a'</span> <span style=\"color: #808000; text-decoration-color: #808000\">dataset_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'a7f684e9cdafe3181a12b46035965fbe'</span> \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">current_index</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span> <span style=\"color: #808000; text-decoration-color: #808000\">output_file</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'test_labeled.csv'</span> <span style=\"color: #808000; text-decoration-color: #808000\">status</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">TaskStatus.ACTIVE:</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'active'</span><span style=\"font-weight: bold\">&gt;</span> <span style=\"color: #808000; text-decoration-color: #808000\">error</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span> <span style=\"color: #808000; text-decoration-color: #808000\">metrics</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "There is an existing task with following details: \u001b[33mid\u001b[0m=\u001b[32m'3001625978'\u001b[0m \u001b[33mcreated_at\u001b[0m=\u001b[1;35mdatetime\u001b[0m\u001b[1;35m.datetime\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2023\u001b[0m, \u001b[1;36m6\u001b[0m, \u001b[1;36m14\u001b[0m, \u001b[1;36m14\u001b[0m, \u001b[1;36m42\u001b[0m,\n",
       "\u001b[1;36m51\u001b[0m, \u001b[1;36m302325\u001b[0m\u001b[1m)\u001b[0m \u001b[33mtask_id\u001b[0m=\u001b[32m'0890358f0f66b2f3e1284f6a180dc84a'\u001b[0m \u001b[33mdataset_id\u001b[0m=\u001b[32m'a7f684e9cdafe3181a12b46035965fbe'\u001b[0m \n",
       "\u001b[33mcurrent_index\u001b[0m=\u001b[1;36m100\u001b[0m \u001b[33moutput_file\u001b[0m=\u001b[32m'test_labeled.csv'\u001b[0m \u001b[33mstatus\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mTaskStatus.ACTIVE:\u001b[0m\u001b[39m \u001b[0m\u001b[32m'active'\u001b[0m\u001b[1m>\u001b[0m \u001b[33merror\u001b[0m=\u001b[3;35mNone\u001b[0m \u001b[33mmetrics\u001b[0m=\u001b[3;35mNone\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluating the existing task<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluating the existing task\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric: auroc: 0.5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> support </span>┃<span style=\"font-weight: bold\"> threshold </span>┃<span style=\"font-weight: bold\"> accuracy </span>┃<span style=\"font-weight: bold\"> completion_rate </span>┃\n",
       "┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 100     </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> -inf      </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.71     </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 1.0             </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 100     </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.5       </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.71     </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 1.0             </span>│\n",
       "└─────────┴───────────┴──────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1msupport\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mthreshold\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1maccuracy\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mcompletion_rate\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[1;36m \u001b[0m\u001b[1;36m100    \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m-inf     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.71    \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m1.0            \u001b[0m\u001b[1;36m \u001b[0m│\n",
       "│\u001b[1;36m \u001b[0m\u001b[1;36m100    \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.5      \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.71    \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m1.0            \u001b[0m\u001b[1;36m \u001b[0m│\n",
       "└─────────┴───────────┴──────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span> examples have been labeled so far.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m100\u001b[0m examples have been labeled so far.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">───────────────────────────────────────────── </span>Last Annotated Example<span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ──────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m───────────────────────────────────────────── \u001b[0mLast Annotated Example\u001b[92m ──────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Prompt</span>: </pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mPrompt\u001b[0m: "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert at understanding legal contracts. Your job is to correctly classify legal provisions in contracts into one of the following categories.\n",
      "Categories:Agreements\n",
      "Amendments\n",
      "Anti-Corruption Laws\n",
      "Applicable Laws\n",
      "Approvals\n",
      "Arbitration\n",
      "Assignments\n",
      "Assigns\n",
      "Authority\n",
      "Authorizations\n",
      "Base Salary\n",
      "Benefits\n",
      "Binding Effects\n",
      "Books\n",
      "Brokers\n",
      "Capitalization\n",
      "Change In Control\n",
      "Closings\n",
      "Compliance With Laws\n",
      "Confidentiality\n",
      "Consent To Jurisdiction\n",
      "Consents\n",
      "Construction\n",
      "Cooperation\n",
      "Costs\n",
      "Counterparts\n",
      "Death\n",
      "Defined Terms\n",
      "Definitions\n",
      "Disability\n",
      "Disclosures\n",
      "Duties\n",
      "Effective Dates\n",
      "Effectiveness\n",
      "Employment\n",
      "Enforceability\n",
      "Enforcements\n",
      "Entire Agreements\n",
      "Erisa\n",
      "Existence\n",
      "Expenses\n",
      "Fees\n",
      "Financial Statements\n",
      "Forfeitures\n",
      "Further Assurances\n",
      "General\n",
      "Governing Laws\n",
      "Headings\n",
      "Indemnifications\n",
      "Indemnity\n",
      "Insurances\n",
      "Integration\n",
      "Intellectual Property\n",
      "Interests\n",
      "Interpretations\n",
      "Jurisdictions\n",
      "Liens\n",
      "Litigations\n",
      "Miscellaneous\n",
      "Modifications\n",
      "No Conflicts\n",
      "No Defaults\n",
      "No Waivers\n",
      "Non-Disparagement\n",
      "Notices\n",
      "Organizations\n",
      "Participations\n",
      "Payments\n",
      "Positions\n",
      "Powers\n",
      "Publicity\n",
      "Qualifications\n",
      "Records\n",
      "Releases\n",
      "Remedies\n",
      "Representations\n",
      "Sales\n",
      "Sanctions\n",
      "Severability\n",
      "Solvency\n",
      "Specific Performance\n",
      "Submission To Jurisdiction\n",
      "Subsidiaries\n",
      "Successors\n",
      "Survival\n",
      "Tax Withholdings\n",
      "Taxes\n",
      "Terminations\n",
      "Terms\n",
      "Titles\n",
      "Transactions With Affiliates\n",
      "Use Of Proceeds\n",
      "Vacations\n",
      "Venues\n",
      "Vesting\n",
      "Waiver Of Jury Trials\n",
      "Waivers\n",
      "Warranties\n",
      "Withholdings\n",
      "\n",
      "\n",
      "You will return the answer with just one element: \"the correct label\"\n",
      "\n",
      "Some examples with their output answers are provided below:\n",
      "\n",
      "Example: The waiver by either party of a breach of any provision of this Agreement, or failure to insist upon strict compliance with the terms of this Agreement, shall not be deemed a waiver of any subsequent breach or relinquishment of any right or power under this Agreement.\n",
      "Output: Waivers\n",
      "\n",
      "Example: This Agreement and various rights and obligations arising hereunder shall inure to the benefit of and be binding upon the Parties hereto and their successors and permitted assigns.  Neither this Agreement nor any of the rights, interests, or obligations hereunder shall be transferred, delegated, or assigned (by operation of Law or otherwise) by any Party hereto without the prior written consent of the other Parties.\n",
      "Output: Assignments\n",
      "\n",
      "Example: No term or condition of the Plan shall be deemed to have been waived, nor shall there be an estoppel against the enforcement of any provision of the Plan, except by written instrument of the party charged with such waiver or estoppel. No such written waiver shall be deemed a continuing waiver unless specifically stated therein, and each such waiver shall operate only as to the specific term or condition waived and shall not constitute a waiver of such term or condition for the future or as to any act other than that specifically waived. Any waiver by any party of a breach of any provision of the Plan by another party shall not operate or be construed as a waiver by such party of any subsequent breach thereof.\n",
      "Output: Waivers\n",
      "\n",
      "Example: Although the Guarantors party hereto have been informed of the matters set forth herein and have agreed to the same, each such Guarantor understands, acknowledges and agrees that none of the Secured Parties has any obligations to inform such Guarantor of such matters in the future or to seek such Guarantor’s acknowledgment or agreement to future amendments, restatements, supplements, changes, modifications, waivers or consents, and nothing herein shall create such a duty.\n",
      "Output: Agreements\n",
      "\n",
      "Now I want you to label the following example:\n",
      "Example: The waiver by a Party hereto of any right hereunder shall not be deemed a waiver of any other right hereunder, whether of a similar nature or otherwise.\n",
      "Output: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Annotation</span>: </pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mAnnotation\u001b[0m: "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waivers\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Do you want to resume the task? <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">[y/n]</span>: </pre>\n"
      ],
      "text/plain": [
       "Do you want to resume the task? \u001b[1;35m[y/n]\u001b[0m: "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Deleted the existing task and starting a new one<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Deleted the existing task and starting a new one\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/opt/homebrew/lib/python3.10/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/opt/homebrew/lib/python3.10/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/opt/homebrew/lib/python3.10/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/opt/homebrew/lib/python3.10/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 15:08:29 openai INFO: error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-06-14 15:08:29 langchain.chat_models.openai WARNING: Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised APIError: The server had an error while processing your request. Sorry about that! {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error while processing your request. Sorry about that!\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that!', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Wed, 14 Jun 2023 22:08:29 GMT', 'Content-Type': 'application/json', 'Content-Length': '176', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'gpt-3.5-turbo-0301', 'openai-organization': 'refuel', 'openai-processing-ms': '331', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3500', 'x-ratelimit-limit-tokens': '90000', 'x-ratelimit-remaining-requests': '3499', 'x-ratelimit-remaining-tokens': '88015', 'x-ratelimit-reset-requests': '17ms', 'x-ratelimit-reset-tokens': '1.322s', 'x-request-id': 'e3bc90e7e76a1d78bf936aabe0639728', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7d75e463c8402368-SJC', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/opt/homebrew/lib/python3.10/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/opt/homebrew/lib/python3.10/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric: auroc: 0.7217\n",
      "Actual Cost: 0.01\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> support </span>┃<span style=\"font-weight: bold\"> threshold </span>┃<span style=\"font-weight: bold\"> accuracy </span>┃<span style=\"font-weight: bold\"> completion_rate </span>┃\n",
       "┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 100     </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> -inf      </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.71     </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 1.0             </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 1       </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.9996    </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 1.0      </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.01            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 14      </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.9969    </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 1.0      </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.14            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 15      </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.9965    </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.9333   </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.15            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 16      </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.9964    </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.9375   </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.16            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 18      </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.9953    </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.8333   </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.18            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 31      </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.9868    </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.9032   </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.31            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 32      </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.9866    </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.875    </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.32            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 35      </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.9759    </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.8857   </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.35            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 36      </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.9759    </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.8611   </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.36            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 39      </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.9725    </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.8718   </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.39            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 40      </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.972     </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.85     </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.4             </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 43      </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.9635    </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.8605   </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.43            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 44      </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.9558    </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.8409   </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.44            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 48      </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.9539    </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.8542   </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.48            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 49      </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.9476    </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.8367   </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.49            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 51      </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.9422    </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.8431   </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.51            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 52      </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.9342    </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.8269   </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.52            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 56      </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.9226    </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.8393   </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.56            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 57      </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.9201    </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.8246   </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.57            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 58      </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.9167    </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.8276   </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.58            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 59      </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.9158    </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.8136   </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.59            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 62      </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.8988    </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.8226   </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.62            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 63      </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.8985    </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.8095   </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.63            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 68      </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.8268    </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.8235   </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.68            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 70      </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.8003    </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.8      </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.7             </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 73      </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.755     </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.8082   </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.73            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 76      </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.7465    </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.7763   </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.76            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 77      </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.7354    </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.7792   </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.77            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 78      </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.734     </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.7692   </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.78            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 82      </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.7119    </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.7805   </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.82            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 86      </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.6721    </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.7442   </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.86            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 88      </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.5911    </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.75     </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.88            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 89      </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.5795    </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.7416   </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.89            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 90      </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.5614    </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.7444   </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.9             </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 92      </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.53      </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.7283   </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.92            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 94      </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.5223    </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.734    </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.94            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 97      </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.3197    </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.7113   </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.97            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 98      </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.1733    </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.7143   </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.98            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 99      </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.0402    </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.7071   </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.99            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 100     </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.02      </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.71     </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 1.0             </span>│\n",
       "└─────────┴───────────┴──────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1msupport\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mthreshold\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1maccuracy\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mcompletion_rate\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[1;36m \u001b[0m\u001b[1;36m100    \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m-inf     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.71    \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m1.0            \u001b[0m\u001b[1;36m \u001b[0m│\n",
       "│\u001b[1;36m \u001b[0m\u001b[1;36m1      \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.9996   \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m1.0     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.01           \u001b[0m\u001b[1;36m \u001b[0m│\n",
       "│\u001b[1;36m \u001b[0m\u001b[1;36m14     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.9969   \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m1.0     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.14           \u001b[0m\u001b[1;36m \u001b[0m│\n",
       "│\u001b[1;36m \u001b[0m\u001b[1;36m15     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.9965   \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.9333  \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.15           \u001b[0m\u001b[1;36m \u001b[0m│\n",
       "│\u001b[1;36m \u001b[0m\u001b[1;36m16     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.9964   \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.9375  \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.16           \u001b[0m\u001b[1;36m \u001b[0m│\n",
       "│\u001b[1;36m \u001b[0m\u001b[1;36m18     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.9953   \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.8333  \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.18           \u001b[0m\u001b[1;36m \u001b[0m│\n",
       "│\u001b[1;36m \u001b[0m\u001b[1;36m31     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.9868   \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.9032  \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.31           \u001b[0m\u001b[1;36m \u001b[0m│\n",
       "│\u001b[1;36m \u001b[0m\u001b[1;36m32     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.9866   \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.875   \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.32           \u001b[0m\u001b[1;36m \u001b[0m│\n",
       "│\u001b[1;36m \u001b[0m\u001b[1;36m35     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.9759   \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.8857  \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.35           \u001b[0m\u001b[1;36m \u001b[0m│\n",
       "│\u001b[1;36m \u001b[0m\u001b[1;36m36     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.9759   \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.8611  \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.36           \u001b[0m\u001b[1;36m \u001b[0m│\n",
       "│\u001b[1;36m \u001b[0m\u001b[1;36m39     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.9725   \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.8718  \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.39           \u001b[0m\u001b[1;36m \u001b[0m│\n",
       "│\u001b[1;36m \u001b[0m\u001b[1;36m40     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.972    \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.85    \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.4            \u001b[0m\u001b[1;36m \u001b[0m│\n",
       "│\u001b[1;36m \u001b[0m\u001b[1;36m43     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.9635   \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.8605  \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.43           \u001b[0m\u001b[1;36m \u001b[0m│\n",
       "│\u001b[1;36m \u001b[0m\u001b[1;36m44     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.9558   \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.8409  \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.44           \u001b[0m\u001b[1;36m \u001b[0m│\n",
       "│\u001b[1;36m \u001b[0m\u001b[1;36m48     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.9539   \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.8542  \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.48           \u001b[0m\u001b[1;36m \u001b[0m│\n",
       "│\u001b[1;36m \u001b[0m\u001b[1;36m49     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.9476   \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.8367  \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.49           \u001b[0m\u001b[1;36m \u001b[0m│\n",
       "│\u001b[1;36m \u001b[0m\u001b[1;36m51     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.9422   \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.8431  \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.51           \u001b[0m\u001b[1;36m \u001b[0m│\n",
       "│\u001b[1;36m \u001b[0m\u001b[1;36m52     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.9342   \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.8269  \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.52           \u001b[0m\u001b[1;36m \u001b[0m│\n",
       "│\u001b[1;36m \u001b[0m\u001b[1;36m56     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.9226   \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.8393  \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.56           \u001b[0m\u001b[1;36m \u001b[0m│\n",
       "│\u001b[1;36m \u001b[0m\u001b[1;36m57     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.9201   \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.8246  \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.57           \u001b[0m\u001b[1;36m \u001b[0m│\n",
       "│\u001b[1;36m \u001b[0m\u001b[1;36m58     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.9167   \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.8276  \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.58           \u001b[0m\u001b[1;36m \u001b[0m│\n",
       "│\u001b[1;36m \u001b[0m\u001b[1;36m59     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.9158   \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.8136  \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.59           \u001b[0m\u001b[1;36m \u001b[0m│\n",
       "│\u001b[1;36m \u001b[0m\u001b[1;36m62     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.8988   \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.8226  \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.62           \u001b[0m\u001b[1;36m \u001b[0m│\n",
       "│\u001b[1;36m \u001b[0m\u001b[1;36m63     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.8985   \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.8095  \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.63           \u001b[0m\u001b[1;36m \u001b[0m│\n",
       "│\u001b[1;36m \u001b[0m\u001b[1;36m68     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.8268   \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.8235  \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.68           \u001b[0m\u001b[1;36m \u001b[0m│\n",
       "│\u001b[1;36m \u001b[0m\u001b[1;36m70     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.8003   \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.8     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.7            \u001b[0m\u001b[1;36m \u001b[0m│\n",
       "│\u001b[1;36m \u001b[0m\u001b[1;36m73     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.755    \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.8082  \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.73           \u001b[0m\u001b[1;36m \u001b[0m│\n",
       "│\u001b[1;36m \u001b[0m\u001b[1;36m76     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.7465   \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.7763  \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.76           \u001b[0m\u001b[1;36m \u001b[0m│\n",
       "│\u001b[1;36m \u001b[0m\u001b[1;36m77     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.7354   \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.7792  \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.77           \u001b[0m\u001b[1;36m \u001b[0m│\n",
       "│\u001b[1;36m \u001b[0m\u001b[1;36m78     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.734    \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.7692  \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.78           \u001b[0m\u001b[1;36m \u001b[0m│\n",
       "│\u001b[1;36m \u001b[0m\u001b[1;36m82     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.7119   \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.7805  \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.82           \u001b[0m\u001b[1;36m \u001b[0m│\n",
       "│\u001b[1;36m \u001b[0m\u001b[1;36m86     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.6721   \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.7442  \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.86           \u001b[0m\u001b[1;36m \u001b[0m│\n",
       "│\u001b[1;36m \u001b[0m\u001b[1;36m88     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.5911   \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.75    \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.88           \u001b[0m\u001b[1;36m \u001b[0m│\n",
       "│\u001b[1;36m \u001b[0m\u001b[1;36m89     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.5795   \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.7416  \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.89           \u001b[0m\u001b[1;36m \u001b[0m│\n",
       "│\u001b[1;36m \u001b[0m\u001b[1;36m90     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.5614   \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.7444  \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.9            \u001b[0m\u001b[1;36m \u001b[0m│\n",
       "│\u001b[1;36m \u001b[0m\u001b[1;36m92     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.53     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.7283  \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.92           \u001b[0m\u001b[1;36m \u001b[0m│\n",
       "│\u001b[1;36m \u001b[0m\u001b[1;36m94     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.5223   \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.734   \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.94           \u001b[0m\u001b[1;36m \u001b[0m│\n",
       "│\u001b[1;36m \u001b[0m\u001b[1;36m97     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.3197   \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.7113  \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.97           \u001b[0m\u001b[1;36m \u001b[0m│\n",
       "│\u001b[1;36m \u001b[0m\u001b[1;36m98     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.1733   \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.7143  \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.98           \u001b[0m\u001b[1;36m \u001b[0m│\n",
       "│\u001b[1;36m \u001b[0m\u001b[1;36m99     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.0402   \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.7071  \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.99           \u001b[0m\u001b[1;36m \u001b[0m│\n",
       "│\u001b[1;36m \u001b[0m\u001b[1;36m100    \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.02     \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.71    \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m1.0            \u001b[0m\u001b[1;36m \u001b[0m│\n",
       "└─────────┴───────────┴──────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Total number of failures: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Total number of failures: \u001b[1;36m0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = agent.run(ds, max_items=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89ff664",
   "metadata": {},
   "source": [
    "Looking at the table above, we can see that if we set the confidence threshold at `0.755`, we are able to label at 80.82% accuracy and getting a completion rate of 73%. This means, we would ignore all the data points where confidence score is less than `0.755` (which would end up being around 27% of all samples). This would, however, guarantee a very high quality labeled dataset for us. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
