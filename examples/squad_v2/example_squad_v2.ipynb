{"cells":[{"cell_type":"markdown","id":"1fe6e643-9453-4381-9445-bd471685fb96","metadata":{"id":"1fe6e643-9453-4381-9445-bd471685fb96"},"source":["## Exploring the SQUADv2 dataset using Autolabel"]},{"cell_type":"markdown","id":"80110a5b-2b3e-45e2-a2da-f6fa00200dff","metadata":{"id":"80110a5b-2b3e-45e2-a2da-f6fa00200dff"},"source":["#### Setup the API Keys for providers that you want to use"]},{"cell_type":"code","execution_count":1,"id":"92993c83-4473-4e05-9510-f543b070c7d0","metadata":{"executionInfo":{"elapsed":160,"status":"ok","timestamp":1686855643474,"user":{"displayName":"Abhinav Naikawadi","userId":"14001727525105340618"},"user_tz":420},"id":"92993c83-4473-4e05-9510-f543b070c7d0"},"outputs":[],"source":["import os\n","\n","# provide your own OpenAI API key here\n","os.environ['OPENAI_API_KEY'] = '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'"]},{"cell_type":"markdown","id":"9c246f85","metadata":{"id":"9c246f85"},"source":["#### Install the autolabel library"]},{"cell_type":"code","execution_count":null,"id":"bc181e31","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":21972,"status":"ok","timestamp":1686855440303,"user":{"displayName":"Abhinav Naikawadi","userId":"14001727525105340618"},"user_tz":420},"id":"bc181e31","outputId":"e675d713-8425-4e57-d0cc-ccb78e34685b"},"outputs":[],"source":["!pip install 'refuel-autolabel[openai]'"]},{"cell_type":"markdown","id":"2883ca14","metadata":{"id":"2883ca14"},"source":["#### Download the dataset"]},{"cell_type":"code","execution_count":null,"id":"16ce0de2","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12140,"status":"ok","timestamp":1686855487231,"user":{"displayName":"Abhinav Naikawadi","userId":"14001727525105340618"},"user_tz":420},"id":"16ce0de2","outputId":"655b92dc-d6d5-46da-f5a2-9fa644e4a135"},"outputs":[],"source":["from autolabel import get_data\n","\n","get_data('squad_v2')"]},{"cell_type":"markdown","id":"b37754b9","metadata":{"id":"b37754b9"},"source":["This downloads two datasets:\n","* `test.csv`: This is the larger dataset we are trying to label using LLMs\n","* `seed.csv`: This is a small dataset where we already have human-provided labels"]},{"cell_type":"markdown","id":"84b014d1-f45c-4479-9acc-0d20870b1786","metadata":{"id":"84b014d1-f45c-4479-9acc-0d20870b1786"},"source":["## Start the labeling process!\n","\n","Labeling with Autolabel is a 3-step process:\n","* First, we specify a labeling configuration (see `config.json` below)\n","* Next, we do a dry-run on our dataset using the LLM specified in `config.json` by running `agent.plan`\n","* Finally, we run the labeling with `agent.run`"]},{"cell_type":"code","execution_count":2,"id":"c093fe91-3508-4140-8bd6-217034e3cce6","metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1686855487232,"user":{"displayName":"Abhinav Naikawadi","userId":"14001727525105340618"},"user_tz":420},"id":"c093fe91-3508-4140-8bd6-217034e3cce6"},"outputs":[],"source":["import json\n","\n","from autolabel import LabelingAgent"]},{"cell_type":"code","execution_count":3,"id":"c93fae0b","metadata":{"executionInfo":{"elapsed":124,"status":"ok","timestamp":1686855559810,"user":{"displayName":"Abhinav Naikawadi","userId":"14001727525105340618"},"user_tz":420},"id":"c93fae0b"},"outputs":[],"source":["# load the config\n","with open('config_squad_v2.json', 'r') as f:\n","    config = json.load(f)"]},{"cell_type":"markdown","id":"5cb3f033","metadata":{"id":"5cb3f033"},"source":["Let's review the configuration file below. You'll notice the following useful keys:\n","* `task_type`: `question_answering` (since it's a question answering task)\n","* `model`: `{'provider': 'openai', 'name': 'gpt-3.5-turbo'}` (use a specific OpenAI model)\n","* `prompt.task_guidelines`: `'You are an expert at answering questions based on wikipedia articles` (how we describe the task to the LLM)\n","* `prompt.few_shot_num`: 3 (how many labeled examples to provide to the LLM)"]},{"cell_type":"code","execution_count":4,"id":"e7297a74","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":248,"status":"ok","timestamp":1686855564540,"user":{"displayName":"Abhinav Naikawadi","userId":"14001727525105340618"},"user_tz":420},"id":"e7297a74","outputId":"42563607-94e9-40f7-cd03-575a8df65cc1"},"outputs":[{"data":{"text/plain":["{'task_name': 'OpenbookQAWikipedia',\n"," 'task_type': 'question_answering',\n"," 'dataset': {'label_column': 'answer', 'delimiter': ','},\n"," 'model': {'provider': 'openai', 'name': 'gpt-3.5-turbo-instruct'},\n"," 'prompt': {'task_guidelines': 'You are an expert at answering questions based on wikipedia articles. Your job is to answer the following questions using the context provided with the question.\\nThe answer is a continuous span of words from the context.\\nUse the context to answer the question. If the question cannot be answered using the context and the context alone without any outside knowledge, answer the question as unanswerable.',\n","  'output_guidelines': 'You will return the answer one element: \"the correct answer\". If the question is unanswerable, return the answer as \"unanswerable\"\\n',\n","  'few_shot_examples': 'seed.csv',\n","  'few_shot_selection': 'semantic_similarity',\n","  'few_shot_num': 10,\n","  'example_template': 'Context: {context}\\nQuestion: {question}\\nAnswer: {answer}'}}"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["config"]},{"cell_type":"code","execution_count":5,"id":"97ce9a02","metadata":{"executionInfo":{"elapsed":113,"status":"ok","timestamp":1686855649734,"user":{"displayName":"Abhinav Naikawadi","userId":"14001727525105340618"},"user_tz":420},"id":"97ce9a02"},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-09-19 18:07:46 autolabel.models.openai WARNING: Current engine: completion\n"]}],"source":["# create an agent for labeling\n","agent = LabelingAgent(config=config)"]},{"cell_type":"markdown","id":"982b96e6","metadata":{},"source":[]},{"cell_type":"code","execution_count":6,"id":"92667a39","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":580,"referenced_widgets":["836e2c02160c4f03903eb0583bb9d700","c937612e207b4f80996a0f5265672f59"]},"executionInfo":{"elapsed":1964,"status":"ok","timestamp":1686855655890,"user":{"displayName":"Abhinav Naikawadi","userId":"14001727525105340618"},"user_tz":420},"id":"92667a39","outputId":"b4c947f7-d63a-4118-cac0-8ef7feaca947"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"11c535d958384fcbaba0149db9a564d1","version_major":2,"version_minor":0},"text/plain":["Output()"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"],"text/plain":[]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n","</pre>\n"],"text/plain":["\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┌──────────────────────────┬──────────┐\n","│<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Total Estimated Cost     </span>│<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> $10.6444 </span>│\n","│<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Number of Examples       </span>│<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> 2000     </span>│\n","│<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Average cost per example </span>│<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> $0.0053  </span>│\n","└──────────────────────────┴──────────┘\n","</pre>\n"],"text/plain":["┌──────────────────────────┬──────────┐\n","│\u001b[1;35m \u001b[0m\u001b[1;35mTotal Estimated Cost    \u001b[0m\u001b[1;35m \u001b[0m│\u001b[1;32m \u001b[0m\u001b[1;32m$10.6444\u001b[0m\u001b[1;32m \u001b[0m│\n","│\u001b[1;35m \u001b[0m\u001b[1;35mNumber of Examples      \u001b[0m\u001b[1;35m \u001b[0m│\u001b[1;32m \u001b[0m\u001b[1;32m2000    \u001b[0m\u001b[1;32m \u001b[0m│\n","│\u001b[1;35m \u001b[0m\u001b[1;35mAverage cost per example\u001b[0m\u001b[1;35m \u001b[0m│\u001b[1;32m \u001b[0m\u001b[1;32m$0.0053 \u001b[0m\u001b[1;32m \u001b[0m│\n","└──────────────────────────┴──────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">───────────────────────────────────────────────── </span>Prompt Example<span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ──────────────────────────────────────────────────</span>\n","</pre>\n"],"text/plain":["\u001b[92m───────────────────────────────────────────────── \u001b[0mPrompt Example\u001b[92m ──────────────────────────────────────────────────\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">You are an expert at answering questions based on wikipedia articles. Your job is to answer the following questions\n","using the context provided with the question.\n","The answer is a continuous span of words from the context.\n","Use the context to answer the question. If the question cannot be answered using the context and the context alone \n","without any outside knowledge, answer the question as unanswerable.\n","\n","You will return the answer one element: <span style=\"color: #008000; text-decoration-color: #008000\">\"the correct answer\"</span>. If the question is unanswerable, return the answer as\n","<span style=\"color: #008000; text-decoration-color: #008000\">\"unanswerable\"</span>\n","\n","\n","Some examples with their output answers are provided below:\n","\n","Context: The final major evolution of the steam engine design was the use of steam turbines starting in the late \n","part of the 19th century. Steam turbines are generally more efficient than reciprocating piston type steam engines \n","<span style=\"font-weight: bold\">(</span>for outputs above several hundred horsepower<span style=\"font-weight: bold\">)</span>, have fewer moving parts, and provide rotary power directly instead \n","of through a connecting rod system or similar means. Steam turbines virtually replaced reciprocating engines in \n","electricity generating stations early in the 20th century, where their efficiency, higher speed appropriate to \n","generator service, and smooth rotation were advantages. Today most electric power is provided by steam turbines. In\n","the United States <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">90</span>% of the electric power is produced in this way using a variety of heat sources. Steam turbines\n","were extensively applied for propulsion of large ships throughout most of the 20th century.\n","Question: Above what horsepower are steam turbines usually more efficient than steam engines that use reciprocating\n","pistons?\n","Answer: several hundred\n","\n","Context: Near the end of the 19th century compound engines came into widespread use. Compound engines exhausted \n","steam in to successively larger cylinders to accommodate the higher volumes at reduced pressures, giving improved \n","efficiency. These stages were called expansions, with double and triple expansion engines being common, especially \n","in shipping where efficiency was important to reduce the weight of coal carried. Steam engines remained the \n","dominant source of power until the early 20th century, when advances in the design of electric motors and internal \n","combustion engines gradually resulted in the replacement of reciprocating <span style=\"font-weight: bold\">(</span>piston<span style=\"font-weight: bold\">)</span> steam engines, with shipping in \n","the 20th-century relying upon the steam turbine.\n","Question: In what field were double and triple expansion engines common?\n","Answer: shipping\n","\n","Context: It is possible to use a mechanism based on a pistonless rotary engine such as the Wankel engine in place \n","of the cylinders and valve gear of a conventional reciprocating steam engine. Many such engines have been designed,\n","from the time of James Watt to the present day, but relatively few were actually built and even fewer went into \n","quantity production; see link at bottom of article for more details. The major problem is the difficulty of sealing\n","the rotors to make them steam-tight in the face of wear and thermal expansion; the resulting leakage made them very\n","inefficient. Lack of expansive working, or any means of control of the cutoff is also a serious problem with many \n","such designs.\n","Question: What development, along with wear, makes it difficult to seal the rotors in an engine that lacks steam? \n","Answer: unanswerable\n","\n","Context: The Rankine cycle is sometimes referred to as a practical Carnot cycle because, when an efficient turbine \n","is used, the TS diagram begins to resemble the Carnot cycle. The main difference is that heat addition <span style=\"font-weight: bold\">(</span>in the \n","boiler<span style=\"font-weight: bold\">)</span> and rejection <span style=\"font-weight: bold\">(</span>in the condenser<span style=\"font-weight: bold\">)</span> are isobaric <span style=\"font-weight: bold\">(</span>constant pressure<span style=\"font-weight: bold\">)</span> processes in the Rankine cycle and \n","isothermal <span style=\"font-weight: bold\">(</span>constant temperature<span style=\"font-weight: bold\">)</span> processes in the theoretical Carnot cycle. In this cycle a pump is used to \n","pressurize the working fluid which is received from the condenser as a liquid not as a gas. Pumping the working \n","fluid in liquid form during the cycle requires a small fraction of the energy to transport it compared to the \n","energy needed to compress the working fluid in gaseous form in a compressor <span style=\"font-weight: bold\">(</span>as in the Carnot cycle<span style=\"font-weight: bold\">)</span>. The cycle of \n","a reciprocating steam engine differs from that of turbines because of condensation and re-evaporation occurring in \n","the cylinder or in the steam inlet passages.\n","Question: Where does heat rejection occur in the Rankine cycle?\n","Answer: in the condenser\n","\n","Context: The heat required for boiling the water and supplying the steam can be derived from various sources, most \n","commonly from burning combustible materials with an appropriate supply of air in a closed space <span style=\"font-weight: bold\">(</span>called variously \n","combustion chamber, firebox<span style=\"font-weight: bold\">)</span>. In some cases the heat source is a nuclear reactor, geothermal energy, solar energy \n","or waste heat from an internal combustion engine or industrial process. In the case of model or toy steam engines, \n","the heat source can be an electric heating element.\n","Question: What is the usual source of heat for boiling water in the industrial process?\n","Answer: unanswerable\n","\n","Context: The adoption of compounding was common for industrial units, for road engines and almost universal for \n","marine engines after <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1880</span>; it was not universally popular in railway locomotives where it was often perceived as \n","complicated. This is partly due to the harsh railway operating environment and limited space afforded by the \n","loading gauge <span style=\"font-weight: bold\">(</span>particularly in Britain, where compounding was never common and not employed after <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1930</span><span style=\"font-weight: bold\">)</span>. However, \n","although never in the majority, it was popular in many other countries.\n","Question: Along with marine engines and industrial units, in what machines was compounding popular?\n","Answer: road engines\n","\n","Context: Trevithick continued his own experiments using a trio of locomotives, concluding with the Catch Me Who Can\n","in <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1808</span>. Only four years later, the successful twin-cylinder locomotive Salamanca by Matthew Murray was used by the\n","edge railed rack and pinion Middleton Railway. In <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1825</span> George Stephenson built the Locomotion for the Stockton and \n","Darlington Railway. This was the first public steam railway in the world and then in <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1829</span>, he built The Rocket \n","which was entered in and won the Rainhill Trials. The Liverpool and Manchester Railway opened in <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1830</span> making \n","exclusive use of steam power for both passenger and freight trains.\n","Question: What type of locomotive was Darlington?\n","Answer: unanswerable\n","\n","Context: In the late 17th century, Robert Boyle proved that air is necessary for combustion. English chemist John \n","Mayow <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1641</span>–<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1679</span><span style=\"font-weight: bold\">)</span> refined this work by showing that fire requires only a part of air that he called spiritus \n","nitroaereus or just nitroaereus. In one experiment he found that placing either a mouse or a lit candle in a closed\n","container over water caused the water to rise and replace one-fourteenth of the air's volume before extinguishing \n","the subjects. From this he surmised that nitroaereus is consumed in both respiration and combustion.\n","Question: When did Robert Mayow prove his theories?\n","Answer: unanswerable\n","\n","Context: With Istanbul as its capital and control of lands around the Mediterranean basin, the Ottoman Empire was \n","at the center of interactions between the Eastern and Western worlds for six centuries. Following a long period of \n","military setbacks against European powers, the Ottoman Empire gradually declined into the late nineteenth century. \n","The empire allied with Germany in the early 20th century, with the imperial ambition of recovering its lost \n","territories, but it dissolved in the aftermath of World War I, leading to the emergence of the new state of Turkey \n","in the Ottoman Anatolian heartland, as well as the creation of modern Balkan and Middle Eastern states, thus ending\n","Turkish colonial ambitions.\n","Question: What ended Turkish imperial Ambitions?\n","Answer: World War I\n","\n","Context: An increase in imported cars into North America forced General Motors, Ford and Chrysler to introduce \n","smaller and fuel-efficient models for domestic sales. The Dodge Omni <span style=\"color: #800080; text-decoration-color: #800080\">/</span> Plymouth Horizon from Chrysler, the Ford \n","Fiesta and the Chevrolet Chevette all had four-cylinder engines and room for at least four passengers by the late \n","1970s. By <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1985</span>, the average American vehicle moved <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17.4</span> miles per gallon, compared to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13.5</span> in <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1970</span>. The \n","improvements stayed even though the price of a barrel of oil remained constant at $<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> from <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1974</span> to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1979</span>. Sales of \n","large sedans for most makes <span style=\"font-weight: bold\">(</span>except Chrysler products<span style=\"font-weight: bold\">)</span> recovered within two model years of the <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1973</span> crisis. The \n","Cadillac DeVille and Fleetwood, Buick Electra, Oldsmobile <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">98</span>, Lincoln Continental, Mercury Marquis, and various \n","other luxury oriented sedans became popular again in the mid-1970s. The only full-size models that did not recover \n","were lower price models such as the Chevrolet Bel Air, and Ford Galaxie <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">500</span>. Slightly smaller, mid-size models such\n","as the Oldsmobile Cutlass, Chevrolet Monte Carlo, Ford Thunderbird and various other models sold well.\n","Question: What was the price of a barrel of oil in <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1970</span>?\n","Answer: unanswerable\n","\n","Now I want you to label the following example:\n","Context: The final major evolution of the steam engine design was the use of steam turbines starting in the late \n","part of the 19th century. Steam turbines are generally more efficient than reciprocating piston type steam engines \n","<span style=\"font-weight: bold\">(</span>for outputs above several hundred horsepower<span style=\"font-weight: bold\">)</span>, have fewer moving parts, and provide rotary power directly instead \n","of through a connecting rod system or similar means. Steam turbines virtually replaced reciprocating engines in \n","electricity generating stations early in the 20th century, where their efficiency, higher speed appropriate to \n","generator service, and smooth rotation were advantages. Today most electric power is provided by steam turbines. In\n","the United States <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">90</span>% of the electric power is produced in this way using a variety of heat sources. Steam turbines\n","were extensively applied for propulsion of large ships throughout most of the 20th century.\n","Question: Most power of what sort is generated by steam turbines today?\n","Answer: \n","</pre>\n"],"text/plain":["You are an expert at answering questions based on wikipedia articles. Your job is to answer the following questions\n","using the context provided with the question.\n","The answer is a continuous span of words from the context.\n","Use the context to answer the question. If the question cannot be answered using the context and the context alone \n","without any outside knowledge, answer the question as unanswerable.\n","\n","You will return the answer one element: \u001b[32m\"the correct answer\"\u001b[0m. If the question is unanswerable, return the answer as\n","\u001b[32m\"unanswerable\"\u001b[0m\n","\n","\n","Some examples with their output answers are provided below:\n","\n","Context: The final major evolution of the steam engine design was the use of steam turbines starting in the late \n","part of the 19th century. Steam turbines are generally more efficient than reciprocating piston type steam engines \n","\u001b[1m(\u001b[0mfor outputs above several hundred horsepower\u001b[1m)\u001b[0m, have fewer moving parts, and provide rotary power directly instead \n","of through a connecting rod system or similar means. Steam turbines virtually replaced reciprocating engines in \n","electricity generating stations early in the 20th century, where their efficiency, higher speed appropriate to \n","generator service, and smooth rotation were advantages. Today most electric power is provided by steam turbines. In\n","the United States \u001b[1;36m90\u001b[0m% of the electric power is produced in this way using a variety of heat sources. Steam turbines\n","were extensively applied for propulsion of large ships throughout most of the 20th century.\n","Question: Above what horsepower are steam turbines usually more efficient than steam engines that use reciprocating\n","pistons?\n","Answer: several hundred\n","\n","Context: Near the end of the 19th century compound engines came into widespread use. Compound engines exhausted \n","steam in to successively larger cylinders to accommodate the higher volumes at reduced pressures, giving improved \n","efficiency. These stages were called expansions, with double and triple expansion engines being common, especially \n","in shipping where efficiency was important to reduce the weight of coal carried. Steam engines remained the \n","dominant source of power until the early 20th century, when advances in the design of electric motors and internal \n","combustion engines gradually resulted in the replacement of reciprocating \u001b[1m(\u001b[0mpiston\u001b[1m)\u001b[0m steam engines, with shipping in \n","the 20th-century relying upon the steam turbine.\n","Question: In what field were double and triple expansion engines common?\n","Answer: shipping\n","\n","Context: It is possible to use a mechanism based on a pistonless rotary engine such as the Wankel engine in place \n","of the cylinders and valve gear of a conventional reciprocating steam engine. Many such engines have been designed,\n","from the time of James Watt to the present day, but relatively few were actually built and even fewer went into \n","quantity production; see link at bottom of article for more details. The major problem is the difficulty of sealing\n","the rotors to make them steam-tight in the face of wear and thermal expansion; the resulting leakage made them very\n","inefficient. Lack of expansive working, or any means of control of the cutoff is also a serious problem with many \n","such designs.\n","Question: What development, along with wear, makes it difficult to seal the rotors in an engine that lacks steam? \n","Answer: unanswerable\n","\n","Context: The Rankine cycle is sometimes referred to as a practical Carnot cycle because, when an efficient turbine \n","is used, the TS diagram begins to resemble the Carnot cycle. The main difference is that heat addition \u001b[1m(\u001b[0min the \n","boiler\u001b[1m)\u001b[0m and rejection \u001b[1m(\u001b[0min the condenser\u001b[1m)\u001b[0m are isobaric \u001b[1m(\u001b[0mconstant pressure\u001b[1m)\u001b[0m processes in the Rankine cycle and \n","isothermal \u001b[1m(\u001b[0mconstant temperature\u001b[1m)\u001b[0m processes in the theoretical Carnot cycle. In this cycle a pump is used to \n","pressurize the working fluid which is received from the condenser as a liquid not as a gas. Pumping the working \n","fluid in liquid form during the cycle requires a small fraction of the energy to transport it compared to the \n","energy needed to compress the working fluid in gaseous form in a compressor \u001b[1m(\u001b[0mas in the Carnot cycle\u001b[1m)\u001b[0m. The cycle of \n","a reciprocating steam engine differs from that of turbines because of condensation and re-evaporation occurring in \n","the cylinder or in the steam inlet passages.\n","Question: Where does heat rejection occur in the Rankine cycle?\n","Answer: in the condenser\n","\n","Context: The heat required for boiling the water and supplying the steam can be derived from various sources, most \n","commonly from burning combustible materials with an appropriate supply of air in a closed space \u001b[1m(\u001b[0mcalled variously \n","combustion chamber, firebox\u001b[1m)\u001b[0m. In some cases the heat source is a nuclear reactor, geothermal energy, solar energy \n","or waste heat from an internal combustion engine or industrial process. In the case of model or toy steam engines, \n","the heat source can be an electric heating element.\n","Question: What is the usual source of heat for boiling water in the industrial process?\n","Answer: unanswerable\n","\n","Context: The adoption of compounding was common for industrial units, for road engines and almost universal for \n","marine engines after \u001b[1;36m1880\u001b[0m; it was not universally popular in railway locomotives where it was often perceived as \n","complicated. This is partly due to the harsh railway operating environment and limited space afforded by the \n","loading gauge \u001b[1m(\u001b[0mparticularly in Britain, where compounding was never common and not employed after \u001b[1;36m1930\u001b[0m\u001b[1m)\u001b[0m. However, \n","although never in the majority, it was popular in many other countries.\n","Question: Along with marine engines and industrial units, in what machines was compounding popular?\n","Answer: road engines\n","\n","Context: Trevithick continued his own experiments using a trio of locomotives, concluding with the Catch Me Who Can\n","in \u001b[1;36m1808\u001b[0m. Only four years later, the successful twin-cylinder locomotive Salamanca by Matthew Murray was used by the\n","edge railed rack and pinion Middleton Railway. In \u001b[1;36m1825\u001b[0m George Stephenson built the Locomotion for the Stockton and \n","Darlington Railway. This was the first public steam railway in the world and then in \u001b[1;36m1829\u001b[0m, he built The Rocket \n","which was entered in and won the Rainhill Trials. The Liverpool and Manchester Railway opened in \u001b[1;36m1830\u001b[0m making \n","exclusive use of steam power for both passenger and freight trains.\n","Question: What type of locomotive was Darlington?\n","Answer: unanswerable\n","\n","Context: In the late 17th century, Robert Boyle proved that air is necessary for combustion. English chemist John \n","Mayow \u001b[1m(\u001b[0m\u001b[1;36m1641\u001b[0m–\u001b[1;36m1679\u001b[0m\u001b[1m)\u001b[0m refined this work by showing that fire requires only a part of air that he called spiritus \n","nitroaereus or just nitroaereus. In one experiment he found that placing either a mouse or a lit candle in a closed\n","container over water caused the water to rise and replace one-fourteenth of the air's volume before extinguishing \n","the subjects. From this he surmised that nitroaereus is consumed in both respiration and combustion.\n","Question: When did Robert Mayow prove his theories?\n","Answer: unanswerable\n","\n","Context: With Istanbul as its capital and control of lands around the Mediterranean basin, the Ottoman Empire was \n","at the center of interactions between the Eastern and Western worlds for six centuries. Following a long period of \n","military setbacks against European powers, the Ottoman Empire gradually declined into the late nineteenth century. \n","The empire allied with Germany in the early 20th century, with the imperial ambition of recovering its lost \n","territories, but it dissolved in the aftermath of World War I, leading to the emergence of the new state of Turkey \n","in the Ottoman Anatolian heartland, as well as the creation of modern Balkan and Middle Eastern states, thus ending\n","Turkish colonial ambitions.\n","Question: What ended Turkish imperial Ambitions?\n","Answer: World War I\n","\n","Context: An increase in imported cars into North America forced General Motors, Ford and Chrysler to introduce \n","smaller and fuel-efficient models for domestic sales. The Dodge Omni \u001b[35m/\u001b[0m Plymouth Horizon from Chrysler, the Ford \n","Fiesta and the Chevrolet Chevette all had four-cylinder engines and room for at least four passengers by the late \n","1970s. By \u001b[1;36m1985\u001b[0m, the average American vehicle moved \u001b[1;36m17.4\u001b[0m miles per gallon, compared to \u001b[1;36m13.5\u001b[0m in \u001b[1;36m1970\u001b[0m. The \n","improvements stayed even though the price of a barrel of oil remained constant at $\u001b[1;36m12\u001b[0m from \u001b[1;36m1974\u001b[0m to \u001b[1;36m1979\u001b[0m. Sales of \n","large sedans for most makes \u001b[1m(\u001b[0mexcept Chrysler products\u001b[1m)\u001b[0m recovered within two model years of the \u001b[1;36m1973\u001b[0m crisis. The \n","Cadillac DeVille and Fleetwood, Buick Electra, Oldsmobile \u001b[1;36m98\u001b[0m, Lincoln Continental, Mercury Marquis, and various \n","other luxury oriented sedans became popular again in the mid-1970s. The only full-size models that did not recover \n","were lower price models such as the Chevrolet Bel Air, and Ford Galaxie \u001b[1;36m500\u001b[0m. Slightly smaller, mid-size models such\n","as the Oldsmobile Cutlass, Chevrolet Monte Carlo, Ford Thunderbird and various other models sold well.\n","Question: What was the price of a barrel of oil in \u001b[1;36m1970\u001b[0m?\n","Answer: unanswerable\n","\n","Now I want you to label the following example:\n","Context: The final major evolution of the steam engine design was the use of steam turbines starting in the late \n","part of the 19th century. Steam turbines are generally more efficient than reciprocating piston type steam engines \n","\u001b[1m(\u001b[0mfor outputs above several hundred horsepower\u001b[1m)\u001b[0m, have fewer moving parts, and provide rotary power directly instead \n","of through a connecting rod system or similar means. Steam turbines virtually replaced reciprocating engines in \n","electricity generating stations early in the 20th century, where their efficiency, higher speed appropriate to \n","generator service, and smooth rotation were advantages. Today most electric power is provided by steam turbines. In\n","the United States \u001b[1;36m90\u001b[0m% of the electric power is produced in this way using a variety of heat sources. Steam turbines\n","were extensively applied for propulsion of large ships throughout most of the 20th century.\n","Question: Most power of what sort is generated by steam turbines today?\n","Answer: \n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n","</pre>\n"],"text/plain":["\u001b[92m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"]},"metadata":{},"output_type":"display_data"}],"source":["from autolabel import AutolabelDataset\n","ds = AutolabelDataset(\"test.csv\", config=config)\n","agent.plan(ds)"]},{"cell_type":"code","execution_count":7,"id":"dd703025-54d8-4349-b0d6-736d2380e966","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":276,"referenced_widgets":["46432161e76f4bb38de65858971a6430","3885fbcb1a0f4c05aad284705b71c797"]},"executionInfo":{"elapsed":81023,"status":"ok","timestamp":1686855741117,"user":{"displayName":"Abhinav Naikawadi","userId":"14001727525105340618"},"user_tz":420},"id":"dd703025-54d8-4349-b0d6-736d2380e966","outputId":"959eb486-adea-4139-bae9-a1a2ff377ac0"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f596150dbc8446e8aefb9f04d30fb254","version_major":2,"version_minor":0},"text/plain":["Output()"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2023-09-19 18:08:22 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4656 tokens (3656 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n","2023-09-19 18:08:22 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4656 tokens (3656 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4656 tokens \n","(3656 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n","</pre>\n"],"text/plain":["Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4656 tokens \n","(3656 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2023-09-19 18:08:54 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4638 tokens (3638 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n","2023-09-19 18:08:54 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4638 tokens (3638 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4638 tokens \n","(3638 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n","</pre>\n"],"text/plain":["Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4638 tokens \n","(3638 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2023-09-19 18:08:56 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4123 tokens (3123 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"]},{"name":"stderr","output_type":"stream","text":["2023-09-19 18:08:57 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4123 tokens (3123 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4123 tokens \n","(3123 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n","</pre>\n"],"text/plain":["Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4123 tokens \n","(3123 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2023-09-19 18:09:49 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4557 tokens (3557 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"]},{"name":"stderr","output_type":"stream","text":["2023-09-19 18:09:50 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4557 tokens (3557 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4557 tokens \n","(3557 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n","</pre>\n"],"text/plain":["Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4557 tokens \n","(3557 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2023-09-19 18:09:56 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4424 tokens (3424 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"]},{"name":"stderr","output_type":"stream","text":["2023-09-19 18:09:57 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4424 tokens (3424 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4424 tokens \n","(3424 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n","</pre>\n"],"text/plain":["Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4424 tokens \n","(3424 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2023-09-19 18:10:03 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4213 tokens (3213 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n","2023-09-19 18:10:03 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4213 tokens (3213 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4213 tokens \n","(3213 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n","</pre>\n"],"text/plain":["Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4213 tokens \n","(3213 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2023-09-19 18:10:30 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4785 tokens (3785 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n","2023-09-19 18:10:31 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4785 tokens (3785 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4785 tokens \n","(3785 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n","</pre>\n"],"text/plain":["Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4785 tokens \n","(3785 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2023-09-19 18:10:42 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4488 tokens (3488 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"]},{"name":"stderr","output_type":"stream","text":["2023-09-19 18:10:43 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4488 tokens (3488 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4488 tokens \n","(3488 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n","</pre>\n"],"text/plain":["Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4488 tokens \n","(3488 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2023-09-19 18:10:54 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4350 tokens (3350 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"]},{"name":"stderr","output_type":"stream","text":["2023-09-19 18:10:54 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4350 tokens (3350 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4350 tokens \n","(3350 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n","</pre>\n"],"text/plain":["Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4350 tokens \n","(3350 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2023-09-19 18:10:55 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4173 tokens (3173 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n","2023-09-19 18:10:56 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4173 tokens (3173 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4173 tokens \n","(3173 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n","</pre>\n"],"text/plain":["Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4173 tokens \n","(3173 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2023-09-19 18:10:56 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4348 tokens (3348 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"]},{"name":"stderr","output_type":"stream","text":["2023-09-19 18:10:56 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4348 tokens (3348 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4348 tokens \n","(3348 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n","</pre>\n"],"text/plain":["Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4348 tokens \n","(3348 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2023-09-19 18:11:23 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4813 tokens (3813 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"]},{"name":"stderr","output_type":"stream","text":["2023-09-19 18:11:23 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4813 tokens (3813 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4813 tokens \n","(3813 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n","</pre>\n"],"text/plain":["Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4813 tokens \n","(3813 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2023-09-19 18:11:30 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4447 tokens (3447 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"]},{"name":"stderr","output_type":"stream","text":["2023-09-19 18:11:30 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4447 tokens (3447 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4447 tokens \n","(3447 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n","</pre>\n"],"text/plain":["Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4447 tokens \n","(3447 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2023-09-19 18:11:35 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4480 tokens (3480 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n","2023-09-19 18:11:35 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4480 tokens (3480 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4480 tokens \n","(3480 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n","</pre>\n"],"text/plain":["Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4480 tokens \n","(3480 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2023-09-19 18:11:51 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4423 tokens (3423 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n","2023-09-19 18:11:51 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4423 tokens (3423 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4423 tokens \n","(3423 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n","</pre>\n"],"text/plain":["Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4423 tokens \n","(3423 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2023-09-19 18:11:58 autolabel.tasks.base WARNING: LLM response is empty\n"]},{"name":"stderr","output_type":"stream","text":["2023-09-19 18:12:00 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4447 tokens (3447 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n","2023-09-19 18:12:00 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4447 tokens (3447 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4447 tokens \n","(3447 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n","</pre>\n"],"text/plain":["Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4447 tokens \n","(3447 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2023-09-19 18:12:16 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4348 tokens (3348 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"]},{"name":"stderr","output_type":"stream","text":["2023-09-19 18:12:16 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4348 tokens (3348 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4348 tokens \n","(3348 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n","</pre>\n"],"text/plain":["Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4348 tokens \n","(3348 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2023-09-19 18:12:19 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4402 tokens (3402 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"]},{"name":"stderr","output_type":"stream","text":["2023-09-19 18:12:19 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4402 tokens (3402 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4402 tokens \n","(3402 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n","</pre>\n"],"text/plain":["Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4402 tokens \n","(3402 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2023-09-19 18:12:53 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4356 tokens (3356 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"]},{"name":"stderr","output_type":"stream","text":["2023-09-19 18:12:53 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4356 tokens (3356 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4356 tokens \n","(3356 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n","</pre>\n"],"text/plain":["Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4356 tokens \n","(3356 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2023-09-19 18:12:59 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4369 tokens (3369 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n","2023-09-19 18:12:59 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4369 tokens (3369 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4369 tokens \n","(3369 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n","</pre>\n"],"text/plain":["Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4369 tokens \n","(3369 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2023-09-19 18:13:23 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4500 tokens (3500 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n","2023-09-19 18:13:24 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4500 tokens (3500 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4500 tokens \n","(3500 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n","</pre>\n"],"text/plain":["Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4500 tokens \n","(3500 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2023-09-19 18:13:42 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4453 tokens (3453 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n","2023-09-19 18:13:42 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4453 tokens (3453 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4453 tokens \n","(3453 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n","</pre>\n"],"text/plain":["Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4453 tokens \n","(3453 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2023-09-19 18:13:56 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4794 tokens (3794 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n","2023-09-19 18:13:56 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4794 tokens (3794 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4794 tokens \n","(3794 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n","</pre>\n"],"text/plain":["Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4794 tokens \n","(3794 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2023-09-19 18:14:15 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4451 tokens (3451 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n","2023-09-19 18:14:16 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4451 tokens (3451 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4451 tokens \n","(3451 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n","</pre>\n"],"text/plain":["Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4451 tokens \n","(3451 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2023-09-19 18:14:16 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4449 tokens (3449 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"]},{"name":"stderr","output_type":"stream","text":["2023-09-19 18:14:16 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4449 tokens (3449 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4449 tokens \n","(3449 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n","</pre>\n"],"text/plain":["Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4449 tokens \n","(3449 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2023-09-19 18:14:38 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4433 tokens (3433 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n","2023-09-19 18:14:38 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4433 tokens (3433 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4433 tokens \n","(3433 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n","</pre>\n"],"text/plain":["Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4433 tokens \n","(3433 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2023-09-19 18:14:38 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4423 tokens (3423 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"]},{"name":"stderr","output_type":"stream","text":["2023-09-19 18:14:38 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4423 tokens (3423 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4423 tokens \n","(3423 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n","</pre>\n"],"text/plain":["Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4423 tokens \n","(3423 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2023-09-19 18:15:00 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4553 tokens (3553 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n","2023-09-19 18:15:00 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4553 tokens (3553 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4553 tokens \n","(3553 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n","</pre>\n"],"text/plain":["Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4553 tokens \n","(3553 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2023-09-19 18:15:30 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4362 tokens (3362 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"]},{"name":"stderr","output_type":"stream","text":["2023-09-19 18:15:30 openai INFO: error_code=None error_message=\"This model's maximum context length is 4097 tokens, however you requested 4362 tokens (3362 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4362 tokens \n","(3362 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n","</pre>\n"],"text/plain":["Error generating from LLM: This model's maximum context length is 4097 tokens, however you requested 4362 tokens \n","(3362 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"],"text/plain":[]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Actual Cost: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.3804</span>\n","</pre>\n"],"text/plain":["Actual Cost: \u001b[1;36m3.3804\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> accuracy </span>┃<span style=\"font-weight: bold\"> support </span>┃<span style=\"font-weight: bold\"> completion_rate </span>┃<span style=\"font-weight: bold\"> f1    </span>┃\n","┡━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━┩\n","│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.3588   </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 1000    </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.97            </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.471 </span>│\n","└──────────┴─────────┴─────────────────┴───────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1maccuracy\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1msupport\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mcompletion_rate\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mf1   \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━┩\n","│\u001b[1;36m \u001b[0m\u001b[1;36m0.3588  \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m1000   \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.97           \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.471\u001b[0m\u001b[1;36m \u001b[0m│\n","└──────────┴─────────┴─────────────────┴───────┘\n"]},"metadata":{},"output_type":"display_data"}],"source":["ds = agent.run(ds, max_items=1000)"]},{"cell_type":"markdown","id":"ef51ce12","metadata":{"id":"ef51ce12"},"source":["We are at 59% accuracy when labeling the first 100 examples. Let's see if we can use confidence scores to improve accuracy further by removing the less confident examples from our labeled set."]},{"cell_type":"markdown","id":"4d7645ab","metadata":{"id":"4d7645ab"},"source":["### Compute confidence scores\n"]},{"cell_type":"code","execution_count":null,"id":"4aa4e28a","metadata":{"executionInfo":{"elapsed":146,"status":"ok","timestamp":1686861679354,"user":{"displayName":"Abhinav Naikawadi","userId":"14001727525105340618"},"user_tz":420},"id":"4aa4e28a"},"outputs":[],"source":["# Start computing confidence scores (using Refuel's LLMs)\n","os.environ['REFUEL_API_KEY'] = 'sk-xxxxxxxxxxxxxxxx'"]},{"cell_type":"code","execution_count":null,"id":"5fbc1264","metadata":{"executionInfo":{"elapsed":198,"status":"ok","timestamp":1686861680521,"user":{"displayName":"Abhinav Naikawadi","userId":"14001727525105340618"},"user_tz":420},"id":"5fbc1264"},"outputs":[],"source":["config[\"model\"][\"compute_confidence\"] = True"]},{"cell_type":"code","execution_count":null,"id":"1998f5e4","metadata":{"executionInfo":{"elapsed":179,"status":"ok","timestamp":1686861681560,"user":{"displayName":"Abhinav Naikawadi","userId":"14001727525105340618"},"user_tz":420},"id":"1998f5e4"},"outputs":[],"source":["agent = LabelingAgent(config=config)"]},{"cell_type":"code","execution_count":null,"id":"119e6f22","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":580,"referenced_widgets":["b8282a45d3eb4aecab56744e1472ee6d","50c5bb31a96841b89dcaee466a056d8c"]},"executionInfo":{"elapsed":1680,"status":"ok","timestamp":1686861684444,"user":{"displayName":"Abhinav Naikawadi","userId":"14001727525105340618"},"user_tz":420},"id":"119e6f22","outputId":"9837fb0d-f77e-4634-de0a-15cf586e9272"},"outputs":[],"source":["from autolabel import AutolabelDataset\n","ds = AutolabelDataset(\"test.csv\", config=config)\n","agent.plan(ds)"]},{"cell_type":"code","execution_count":null,"id":"63c74705","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":795,"referenced_widgets":["d442ea6009ca4add8c72ce808f6a8730","7aa870d8611d4b1c9de1a2ba445b627c"]},"executionInfo":{"elapsed":218988,"status":"ok","timestamp":1686861903886,"user":{"displayName":"Abhinav Naikawadi","userId":"14001727525105340618"},"user_tz":420},"id":"63c74705","outputId":"874d8448-8ff2-4871-f25e-f0715cd2afff"},"outputs":[],"source":["ds = agent.run(ds, max_items=100)"]},{"cell_type":"markdown","id":"abed883b","metadata":{"id":"abed883b"},"source":["Looking at the table above, we can see that if we set the confidence threshold at `0.8449`, we are able to label at 80.65% accuracy and getting a completion rate of 65%. This means, we would ignore all the data points where confidence score is less than `0.8449` (which would end up being around 35% of all samples). This would, however, guarantee a very high quality labeled dataset for us."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.10.8 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"vscode":{"interpreter":{"hash":"b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"3885fbcb1a0f4c05aad284705b71c797":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46432161e76f4bb38de65858971a6430":{"model_module":"@jupyter-widgets/output","model_module_version":"1.0.0","model_name":"OutputModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_3885fbcb1a0f4c05aad284705b71c797","msg_id":"","outputs":[{"data":{"text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">100/100</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:01:20</span> <span style=\"color: #008080; text-decoration-color: #008080\">0:00:00</span>\nCost in $=0.18, f1=0.7019, support=100, threshold=-inf, accuracy=0.5900, completion_rate=1.0000\n</pre>\n","text/plain":"\u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100/100\u001b[0m \u001b[33m0:01:20\u001b[0m \u001b[36m0:00:00\u001b[0m\nCost in $=0.18, f1=0.7019, support=100, threshold=-inf, accuracy=0.5900, completion_rate=1.0000\n"},"metadata":{},"output_type":"display_data"}]}},"50c5bb31a96841b89dcaee466a056d8c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7aa870d8611d4b1c9de1a2ba445b627c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"836e2c02160c4f03903eb0583bb9d700":{"model_module":"@jupyter-widgets/output","model_module_version":"1.0.0","model_name":"OutputModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_c937612e207b4f80996a0f5265672f59","msg_id":"","outputs":[{"data":{"text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Generating Prompts... <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">100/100</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:01</span> <span style=\"color: #008080; text-decoration-color: #008080\">0:00:00</span>\n</pre>\n","text/plain":"Generating Prompts... \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100/100\u001b[0m \u001b[33m0:00:01\u001b[0m \u001b[36m0:00:00\u001b[0m\n"},"metadata":{},"output_type":"display_data"}]}},"b8282a45d3eb4aecab56744e1472ee6d":{"model_module":"@jupyter-widgets/output","model_module_version":"1.0.0","model_name":"OutputModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_50c5bb31a96841b89dcaee466a056d8c","msg_id":"","outputs":[{"data":{"text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Generating Prompts... <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">100/100</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:00</span> <span style=\"color: #008080; text-decoration-color: #008080\">0:00:00</span>\n</pre>\n","text/plain":"Generating Prompts... \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100/100\u001b[0m \u001b[33m0:00:00\u001b[0m \u001b[36m0:00:00\u001b[0m\n"},"metadata":{},"output_type":"display_data"}]}},"c937612e207b4f80996a0f5265672f59":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d442ea6009ca4add8c72ce808f6a8730":{"model_module":"@jupyter-widgets/output","model_module_version":"1.0.0","model_name":"OutputModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_7aa870d8611d4b1c9de1a2ba445b627c","msg_id":"","outputs":[{"data":{"text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">100/100</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:03:38</span> <span style=\"color: #008080; text-decoration-color: #008080\">0:00:00</span>\nCost in $=0.01, f1=0.7019, support=100, threshold=-inf, accuracy=0.5900, completion_rate=1.0000\n</pre>\n","text/plain":"\u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100/100\u001b[0m \u001b[33m0:03:38\u001b[0m \u001b[36m0:00:00\u001b[0m\nCost in $=0.01, f1=0.7019, support=100, threshold=-inf, accuracy=0.5900, completion_rate=1.0000\n"},"metadata":{},"output_type":"display_data"}]}}}}},"nbformat":4,"nbformat_minor":5}
