{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fe6e643-9453-4381-9445-bd471685fb96",
   "metadata": {},
   "source": [
    "## Exploring the Acronym dataset using Autolabel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80110a5b-2b3e-45e2-a2da-f6fa00200dff",
   "metadata": {},
   "source": [
    "#### Setup the API Keys for providers that you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92993c83-4473-4e05-9510-f543b070c7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# provide your own OpenAI API key here\n",
    "os.environ['OPENAI_API_KEY'] = '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'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c246f85",
   "metadata": {},
   "source": [
    "#### Install the autolabel library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc181e31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: refuel-autolabel[openai] in /opt/homebrew/lib/python3.10/site-packages (0.0.1)\n",
      "Requirement already satisfied: loguru>=0.5.0 in /opt/homebrew/lib/python3.10/site-packages (from refuel-autolabel[openai]) (0.5.3)\n",
      "Requirement already satisfied: numpy>=1.23.0 in /opt/homebrew/lib/python3.10/site-packages (from refuel-autolabel[openai]) (1.23.3)\n",
      "Requirement already satisfied: requests>=2.27.0 in /opt/homebrew/lib/python3.10/site-packages (from refuel-autolabel[openai]) (2.28.1)\n",
      "Requirement already satisfied: datasets>=2.7.0 in /opt/homebrew/lib/python3.10/site-packages (from refuel-autolabel[openai]) (2.11.0)\n",
      "Requirement already satisfied: langchain>=0.0.190 in /opt/homebrew/lib/python3.10/site-packages (from refuel-autolabel[openai]) (0.0.191)\n",
      "Requirement already satisfied: nervaluate>=0.1.8 in /opt/homebrew/lib/python3.10/site-packages (from refuel-autolabel[openai]) (0.1.8)\n",
      "Requirement already satisfied: pandas>=1.3.0 in /opt/homebrew/lib/python3.10/site-packages (from refuel-autolabel[openai]) (1.5.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /opt/homebrew/lib/python3.10/site-packages (from refuel-autolabel[openai]) (1.1.2)\n",
      "Requirement already satisfied: tenacity>=8.2.2 in /opt/homebrew/lib/python3.10/site-packages (from refuel-autolabel[openai]) (8.2.2)\n",
      "Requirement already satisfied: SQLAlchemy==1.4.47 in /opt/homebrew/lib/python3.10/site-packages (from refuel-autolabel[openai]) (1.4.47)\n",
      "Requirement already satisfied: regex>=2023.6.3 in /opt/homebrew/lib/python3.10/site-packages (from refuel-autolabel[openai]) (2023.6.3)\n",
      "Requirement already satisfied: rich>=13.3.5 in /opt/homebrew/lib/python3.10/site-packages (from refuel-autolabel[openai]) (13.4.1)\n",
      "Requirement already satisfied: scipy>=1.10.1 in /opt/homebrew/lib/python3.10/site-packages (from refuel-autolabel[openai]) (1.10.1)\n",
      "Requirement already satisfied: pydantic>=1.10.9 in /opt/homebrew/lib/python3.10/site-packages (from refuel-autolabel[openai]) (1.10.9)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/homebrew/lib/python3.10/site-packages (from refuel-autolabel[openai]) (1.13.0)\n",
      "Requirement already satisfied: matplotlib>=3.5.0 in /opt/homebrew/lib/python3.10/site-packages (from refuel-autolabel[openai]) (3.6.0)\n",
      "Requirement already satisfied: wget>=3.2 in /opt/homebrew/lib/python3.10/site-packages (from refuel-autolabel[openai]) (3.2)\n",
      "Requirement already satisfied: openai>=0.27.4 in /opt/homebrew/lib/python3.10/site-packages (from refuel-autolabel[openai]) (0.27.6)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /opt/homebrew/lib/python3.10/site-packages (from refuel-autolabel[openai]) (0.3.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/homebrew/lib/python3.10/site-packages (from datasets>=2.7.0->refuel-autolabel[openai]) (9.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/homebrew/lib/python3.10/site-packages (from datasets>=2.7.0->refuel-autolabel[openai]) (0.3.5.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/homebrew/lib/python3.10/site-packages (from datasets>=2.7.0->refuel-autolabel[openai]) (4.64.1)\n",
      "Requirement already satisfied: xxhash in /opt/homebrew/lib/python3.10/site-packages (from datasets>=2.7.0->refuel-autolabel[openai]) (3.1.0)\n",
      "Requirement already satisfied: multiprocess in /opt/homebrew/lib/python3.10/site-packages (from datasets>=2.7.0->refuel-autolabel[openai]) (0.70.13)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/homebrew/lib/python3.10/site-packages (from datasets>=2.7.0->refuel-autolabel[openai]) (2022.10.0)\n",
      "Requirement already satisfied: aiohttp in /opt/homebrew/lib/python3.10/site-packages (from datasets>=2.7.0->refuel-autolabel[openai]) (3.8.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /opt/homebrew/lib/python3.10/site-packages (from datasets>=2.7.0->refuel-autolabel[openai]) (0.14.1)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/lib/python3.10/site-packages (from datasets>=2.7.0->refuel-autolabel[openai]) (21.3)\n",
      "Requirement already satisfied: responses<0.19 in /opt/homebrew/lib/python3.10/site-packages (from datasets>=2.7.0->refuel-autolabel[openai]) (0.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.10/site-packages (from datasets>=2.7.0->refuel-autolabel[openai]) (6.0)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/homebrew/lib/python3.10/site-packages (from langchain>=0.0.190->refuel-autolabel[openai]) (4.0.2)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /opt/homebrew/lib/python3.10/site-packages (from langchain>=0.0.190->refuel-autolabel[openai]) (0.5.7)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /opt/homebrew/lib/python3.10/site-packages (from langchain>=0.0.190->refuel-autolabel[openai]) (2.8.4)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /opt/homebrew/lib/python3.10/site-packages (from langchain>=0.0.190->refuel-autolabel[openai]) (1.2.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/lib/python3.10/site-packages (from matplotlib>=3.5.0->refuel-autolabel[openai]) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/lib/python3.10/site-packages (from matplotlib>=3.5.0->refuel-autolabel[openai]) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/lib/python3.10/site-packages (from matplotlib>=3.5.0->refuel-autolabel[openai]) (4.37.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/homebrew/lib/python3.10/site-packages (from matplotlib>=3.5.0->refuel-autolabel[openai]) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/homebrew/lib/python3.10/site-packages (from matplotlib>=3.5.0->refuel-autolabel[openai]) (9.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/homebrew/lib/python3.10/site-packages (from matplotlib>=3.5.0->refuel-autolabel[openai]) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/homebrew/lib/python3.10/site-packages (from matplotlib>=3.5.0->refuel-autolabel[openai]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.10/site-packages (from pandas>=1.3.0->refuel-autolabel[openai]) (2022.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/homebrew/lib/python3.10/site-packages (from pydantic>=1.10.9->refuel-autolabel[openai]) (4.3.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/homebrew/lib/python3.10/site-packages (from requests>=2.27.0->refuel-autolabel[openai]) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.10/site-packages (from requests>=2.27.0->refuel-autolabel[openai]) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/lib/python3.10/site-packages (from requests>=2.27.0->refuel-autolabel[openai]) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.10/site-packages (from requests>=2.27.0->refuel-autolabel[openai]) (2022.9.14)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/homebrew/lib/python3.10/site-packages (from rich>=13.3.5->refuel-autolabel[openai]) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/homebrew/lib/python3.10/site-packages (from rich>=13.3.5->refuel-autolabel[openai]) (2.15.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /opt/homebrew/lib/python3.10/site-packages (from scikit-learn>=1.0.0->refuel-autolabel[openai]) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/lib/python3.10/site-packages (from scikit-learn>=1.0.0->refuel-autolabel[openai]) (3.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.0->refuel-autolabel[openai]) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.0->refuel-autolabel[openai]) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.0->refuel-autolabel[openai]) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.0->refuel-autolabel[openai]) (1.3.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.0->refuel-autolabel[openai]) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /opt/homebrew/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.190->refuel-autolabel[openai]) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /opt/homebrew/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.190->refuel-autolabel[openai]) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /opt/homebrew/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.190->refuel-autolabel[openai]) (0.8.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets>=2.7.0->refuel-autolabel[openai]) (3.8.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/homebrew/lib/python3.10/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=13.3.5->refuel-autolabel[openai]) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.5.0->refuel-autolabel[openai]) (1.12.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/homebrew/lib/python3.10/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.190->refuel-autolabel[openai]) (0.4.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install 'refuel-autolabel[openai]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5b1630",
   "metadata": {},
   "source": [
    "#### Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9382044e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading example dataset from https://autolabel-benchmarking.s3.us-west-2.amazonaws.com/acronym/seed.csv to seed.csv...\n",
      "Downloading example dataset from https://autolabel-benchmarking.s3.us-west-2.amazonaws.com/acronym/test.csv to test.csv...\n",
      "100% [........................................] [258086/258086] bytes\r"
     ]
    }
   ],
   "source": [
    "from autolabel import get_data\n",
    "\n",
    "get_data('acronym')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72542ae",
   "metadata": {},
   "source": [
    "This downloads two datasets:\n",
    "* `test.csv`: This is the larger dataset we are trying to label using LLMs\n",
    "* `seed.csv`: This is a small dataset where we already have human-provided labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb92a79",
   "metadata": {},
   "source": [
    "## Start the labeling process!\n",
    "\n",
    "Labeling with Autolabel is a 3-step process:\n",
    "* First, we specify a labeling configuration (see `config.json` below)\n",
    "* Next, we do a dry-run on our dataset using the LLM specified in `config.json` by running `agent.plan`\n",
    "* Finally, we run the labeling with `agent.run`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b014d1-f45c-4479-9acc-0d20870b1786",
   "metadata": {},
   "source": [
    "### First labeling run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c093fe91-3508-4140-8bd6-217034e3cce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from autolabel import LabelingAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c93fae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the config\n",
    "with open('config_acronym.json', 'r') as f:\n",
    "     config = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dfc1f9",
   "metadata": {},
   "source": [
    "Let's review the configuration file below. You'll notice the following useful keys:\n",
    "* `task_type`: `named_entity_recognition` (since it's a named entity recognition task)\n",
    "* `model`: `{'provider': 'openai', 'name': 'gpt-3.5-turbo'}` (use a specific OpenAI model)\n",
    "* `prompt.task_guidelines`: `'You are an expert at extracting Person, Organization, Location, and Miscellaneous entities...` (how we describe the task to the LLM)\n",
    "* `prompt.labels`: `[\n",
    "            \"Location\",\n",
    "            \"Organization\",\n",
    "            \"Person\",\n",
    "            \"Miscellaneous\"\n",
    "        ]` (the full list of labels to choose from)\n",
    "* `prompt.few_shot_num`: 3 (how many labeled examples to provide to the LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "450ad645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task_name': 'AcronymExtraction',\n",
       " 'task_type': 'named_entity_recognition',\n",
       " 'dataset': {'label_column': 'CategorizedLabels',\n",
       "  'text_column': 'example',\n",
       "  'delimiter': ','},\n",
       " 'model': {'provider': 'llama',\n",
       "  'name': '/workspace/hf-relevant-sampling-2483'},\n",
       " 'prompt': {'task_guidelines': 'Your job is to extract all short and long forms of acronyms mentioned in text, and classify them into one of Long and Short. Long refers to substrings that correspond to expanded (long) forms of acronyms while Short refers to substrings that correspond to the shortened acronyms. Output a JSON with keys \"Long\" and \"Short\", and values containing a list with all such strings.',\n",
       "  'labels': ['Long', 'Short'],\n",
       "  'example_template': 'Example: {example}\\nOutput:\\n{CategorizedLabels}',\n",
       "  'few_shot_examples': 'seed.csv',\n",
       "  'few_shot_selection': 'semantic_similarity',\n",
       "  'few_shot_num': 10}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acb4a3de-fa84-4b94-b17a-7a6fac892a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-04 15:18:28 llm_engine.py:72] Initializing an LLM engine with config: model='/workspace/hf-relevant-sampling-2483', tokenizer='/workspace/hf-relevant-sampling-2483', tokenizer_mode=auto, trust_remote_code=False, dtype=torch.float16, download_dir=None, load_format=auto, tensor_parallel_size=1, seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-04 15:18:29 torch.distributed.distributed_c10d INFO: Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "2023-10-04 15:18:29 torch.distributed.distributed_c10d INFO: Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.\n",
      "2023-10-04 15:18:29 torch.distributed.distributed_c10d INFO: Added key: store_based_barrier_key:2 to store for rank: 0\n",
      "2023-10-04 15:18:29 torch.distributed.distributed_c10d INFO: Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 1 nodes.\n",
      "2023-10-04 15:18:29 torch.distributed.distributed_c10d INFO: Added key: store_based_barrier_key:3 to store for rank: 0\n",
      "2023-10-04 15:18:29 torch.distributed.distributed_c10d INFO: Rank 0: Completed store-based barrier for key:store_based_barrier_key:3 with 1 nodes.\n",
      "2023-10-04 15:18:29 torch.distributed.distributed_c10d INFO: Added key: store_based_barrier_key:4 to store for rank: 0\n",
      "2023-10-04 15:18:29 torch.distributed.distributed_c10d INFO: Rank 0: Completed store-based barrier for key:store_based_barrier_key:4 with 1 nodes.\n",
      "2023-10-04 15:18:29 torch.distributed.distributed_c10d INFO: Added key: store_based_barrier_key:5 to store for rank: 0\n",
      "2023-10-04 15:18:29 torch.distributed.distributed_c10d INFO: Rank 0: Completed store-based barrier for key:store_based_barrier_key:5 with 1 nodes.\n",
      "2023-10-04 15:18:29 torch.distributed.distributed_c10d INFO: Added key: store_based_barrier_key:6 to store for rank: 0\n",
      "2023-10-04 15:18:29 torch.distributed.distributed_c10d INFO: Rank 0: Completed store-based barrier for key:store_based_barrier_key:6 with 1 nodes.\n",
      "2023-10-04 15:18:29 torch.distributed.distributed_c10d INFO: Added key: store_based_barrier_key:7 to store for rank: 0\n",
      "2023-10-04 15:18:29 torch.distributed.distributed_c10d INFO: Rank 0: Completed store-based barrier for key:store_based_barrier_key:7 with 1 nodes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-04 15:19:28 llm_engine.py:199] # GPU blocks: 1468, # CPU blocks: 327\n"
     ]
    }
   ],
   "source": [
    "# create an agent for labeling\n",
    "agent = LabelingAgent(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92667a39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4865a651dac2434890f4832ca7f9d9b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┌──────────────────────────┬──────┐\n",
       "│<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Total Estimated Cost     </span>│<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> $0.0 </span>│\n",
       "│<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Number of Examples       </span>│<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> 1000 </span>│\n",
       "│<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Average cost per example </span>│<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> $0.0 </span>│\n",
       "└──────────────────────────┴──────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┌──────────────────────────┬──────┐\n",
       "│\u001b[1;35m \u001b[0m\u001b[1;35mTotal Estimated Cost    \u001b[0m\u001b[1;35m \u001b[0m│\u001b[1;32m \u001b[0m\u001b[1;32m$0.0\u001b[0m\u001b[1;32m \u001b[0m│\n",
       "│\u001b[1;35m \u001b[0m\u001b[1;35mNumber of Examples      \u001b[0m\u001b[1;35m \u001b[0m│\u001b[1;32m \u001b[0m\u001b[1;32m1000\u001b[0m\u001b[1;32m \u001b[0m│\n",
       "│\u001b[1;35m \u001b[0m\u001b[1;35mAverage cost per example\u001b[0m\u001b[1;35m \u001b[0m│\u001b[1;32m \u001b[0m\u001b[1;32m$0.0\u001b[0m\u001b[1;32m \u001b[0m│\n",
       "└──────────────────────────┴──────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">───────────────────────────────────────────────── </span>Prompt Example<span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ──────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m───────────────────────────────────────────────── \u001b[0mPrompt Example\u001b[92m ──────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "    <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">s</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #000000; text-decoration-color: #000000\">INST</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span><span style=\"color: #000000; text-decoration-color: #000000\"> &lt;&lt;SYS&gt;&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    Your job is to extract all short and long forms of acronyms mentioned in text, and classify them into one of </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Long and Short. Long refers to substrings that correspond to expanded </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">long</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> forms of acronyms while Short refers </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">to substrings that correspond to the shortened acronyms. Output a JSON with keys </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Long\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> and </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Short\"</span><span style=\"color: #000000; text-decoration-color: #000000\">, and values </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">containing a list with all such strings.You will return the answer in JSON format, where keys are the above </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">categories and values is a list of substrings corresponding to that category.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Example: We leverage Bayesian Deep Learning </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\"> BDL </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> and environment mapping to generate full volumetric anatomy </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">representations derived from none to a few conditioning slices .</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Output:</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Long\"</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Bayesian Deep Learning\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Short\"</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"BDL\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]}</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Example: To maintain sinogram consistency , we introduce a Radon Consistency </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\"> RC </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> loss .</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Output:</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Long\"</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Radon Consistency\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Short\"</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"RC\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]}</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Example: Segmentation of pulmonary lobe based on Computed Tomography </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\"> CT </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> is an important task for Computer Aided</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Diagnosis Systems </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\"> CADs </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> .</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Output:</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Long\"</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Computed Tomography\"</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Computer Aided Diagnosis Systems\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Short\"</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"CT\"</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">\"CADs\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]}</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Example: In this thesis , we will introduce </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> mammograms for detecting breast cancers , the most frequently </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">diagnosed solid cancer for U.S. women , </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> lung Computed Tomography </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\"> CT </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> images for detecting lung cancers , the</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">most frequently diagnosed malignant cancer , and </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> head and neck CT images for automated delineation of organs at</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">risk in radiotherapy .</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Output:</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Long\"</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Computed Tomography\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Short\"</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"CT\"</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">\"CT\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]}</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Example: To demonstrate applicability of the proposed regularizer to image reconstruction problems we compare it </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">with the classical TV and combined TV- methods for CT and ET image reconstruction .</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Output:</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Long\"</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[]</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Short\"</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"TV\"</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">\"CT\"</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">\"ET\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]}</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Example: In this paper , we propose a deep encoder - Decoder Adversarial Reconstruction Network </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\"> DEAR </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> for 3D CT </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">from few - view data , featured by a direct mapping from a 3D input dataset to a 3D image volume .</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Output:</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Long\"</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Decoder Adversarial Reconstruction Network\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Short\"</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"DEAR\"</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">\"CT\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]}</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Example: Recent methods that use Recurrent Neural Networks </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\"> RNN </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> to handle raw skeletons only focus on the </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">contextual dependency in the temporal domain and neglect the spatial configurations of articulated skeletons .</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Output:</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Long\"</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Recurrent Neural Networks\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Short\"</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"RNN\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]}</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Example: There are multiple contributing factors : a </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> labeling medical data typically requires specially trained </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">doctors ; b </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> marking lesion boundaries can be hard even for experts because of low signal - to - noise ratio in </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">many medical images ; and c </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> for CT and Magnetic Resonance Imaging </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\"> MRI </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> images , the annotators need to label </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">the entire 3D volumetric data , which can be costly and time - consuming .</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Output:</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Long\"</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Magnetic Resonance Imaging\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Short\"</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"CT\"</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">\"MRI\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]}</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Example: Since the KITTI Average Orientation Similarity </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\"> AOS </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> evaluation is also dependent on 2D detection </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">performance , the projection of each 3D box is replaced with its corresponding 2D box from the 2D detector , which </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">allows AOS to be evaluated .</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Output:</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Long\"</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Average Orientation Similarity\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Short\"</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"KITTI\"</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">\"AOS\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]}</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Example: For error of fit we used the estimated Root Mean Square Error </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\"> RMSE </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> between the MLE - estimated </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Nakagami values and the observed voxels values in each voxel lattice starting at size </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\"> , as there would be no </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">meaning if a lattice had a size of </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000\"> .</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Output:</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Long\"</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Root Mean Square Error\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Short\"</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"RMSE\"</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">\"MLE\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]}</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    &lt;&lt;SYS&gt;</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "    Example: The generator used a single CT image slice to predict the dose distribution along that same plane \n",
       "without considering the vertical relationship between different slices .\n",
       "Output:\n",
       "<span style=\"font-weight: bold\">[</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">INST</span><span style=\"font-weight: bold\">]</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "    \u001b[1m<\u001b[0m\u001b[1;95ms\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mINST\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m <<SYS>>\u001b[0m\n",
       "\u001b[39m    Your job is to extract all short and long forms of acronyms mentioned in text, and classify them into one of \u001b[0m\n",
       "\u001b[39mLong and Short. Long refers to substrings that correspond to expanded \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mlong\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m forms of acronyms while Short refers \u001b[0m\n",
       "\u001b[39mto substrings that correspond to the shortened acronyms. Output a JSON with keys \u001b[0m\u001b[32m\"Long\"\u001b[0m\u001b[39m and \u001b[0m\u001b[32m\"Short\"\u001b[0m\u001b[39m, and values \u001b[0m\n",
       "\u001b[39mcontaining a list with all such strings.You will return the answer in JSON format, where keys are the above \u001b[0m\n",
       "\u001b[39mcategories and values is a list of substrings corresponding to that category.\u001b[0m\n",
       "\u001b[39mExample: We leverage Bayesian Deep Learning \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39m BDL \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m and environment mapping to generate full volumetric anatomy \u001b[0m\n",
       "\u001b[39mrepresentations derived from none to a few conditioning slices .\u001b[0m\n",
       "\u001b[39mOutput:\u001b[0m\n",
       "\u001b[1;39m{\u001b[0m\u001b[32m\"Long\"\u001b[0m\u001b[39m: \u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m\"Bayesian Deep Learning\"\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[32m\"Short\"\u001b[0m\u001b[39m: \u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m\"BDL\"\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m}\u001b[0m\n",
       "\n",
       "\u001b[39mExample: To maintain sinogram consistency , we introduce a Radon Consistency \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39m RC \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m loss .\u001b[0m\n",
       "\u001b[39mOutput:\u001b[0m\n",
       "\u001b[1;39m{\u001b[0m\u001b[32m\"Long\"\u001b[0m\u001b[39m: \u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m\"Radon Consistency\"\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[32m\"Short\"\u001b[0m\u001b[39m: \u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m\"RC\"\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m}\u001b[0m\n",
       "\n",
       "\u001b[39mExample: Segmentation of pulmonary lobe based on Computed Tomography \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39m CT \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m is an important task for Computer Aided\u001b[0m\n",
       "\u001b[39mDiagnosis Systems \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39m CADs \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m .\u001b[0m\n",
       "\u001b[39mOutput:\u001b[0m\n",
       "\u001b[1;39m{\u001b[0m\u001b[32m\"Long\"\u001b[0m\u001b[39m: \u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m\"Computed Tomography\"\u001b[0m\u001b[39m, \u001b[0m\u001b[32m\"Computer Aided Diagnosis Systems\"\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[32m\"Short\"\u001b[0m\u001b[39m: \u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m\"CT\"\u001b[0m\u001b[39m, \u001b[0m\u001b[32m\"CADs\"\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m}\u001b[0m\n",
       "\n",
       "\u001b[39mExample: In this thesis , we will introduce \u001b[0m\u001b[1;36m1\u001b[0m\u001b[39m \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m mammograms for detecting breast cancers , the most frequently \u001b[0m\n",
       "\u001b[39mdiagnosed solid cancer for U.S. women , \u001b[0m\u001b[1;36m2\u001b[0m\u001b[39m \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m lung Computed Tomography \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39m CT \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m images for detecting lung cancers , the\u001b[0m\n",
       "\u001b[39mmost frequently diagnosed malignant cancer , and \u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m head and neck CT images for automated delineation of organs at\u001b[0m\n",
       "\u001b[39mrisk in radiotherapy .\u001b[0m\n",
       "\u001b[39mOutput:\u001b[0m\n",
       "\u001b[1;39m{\u001b[0m\u001b[32m\"Long\"\u001b[0m\u001b[39m: \u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m\"Computed Tomography\"\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[32m\"Short\"\u001b[0m\u001b[39m: \u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m\"CT\"\u001b[0m\u001b[39m, \u001b[0m\u001b[32m\"CT\"\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m}\u001b[0m\n",
       "\n",
       "\u001b[39mExample: To demonstrate applicability of the proposed regularizer to image reconstruction problems we compare it \u001b[0m\n",
       "\u001b[39mwith the classical TV and combined TV- methods for CT and ET image reconstruction .\u001b[0m\n",
       "\u001b[39mOutput:\u001b[0m\n",
       "\u001b[1;39m{\u001b[0m\u001b[32m\"Long\"\u001b[0m\u001b[39m: \u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[32m\"Short\"\u001b[0m\u001b[39m: \u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m\"TV\"\u001b[0m\u001b[39m, \u001b[0m\u001b[32m\"CT\"\u001b[0m\u001b[39m, \u001b[0m\u001b[32m\"ET\"\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m}\u001b[0m\n",
       "\n",
       "\u001b[39mExample: In this paper , we propose a deep encoder - Decoder Adversarial Reconstruction Network \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39m DEAR \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m for 3D CT \u001b[0m\n",
       "\u001b[39mfrom few - view data , featured by a direct mapping from a 3D input dataset to a 3D image volume .\u001b[0m\n",
       "\u001b[39mOutput:\u001b[0m\n",
       "\u001b[1;39m{\u001b[0m\u001b[32m\"Long\"\u001b[0m\u001b[39m: \u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m\"Decoder Adversarial Reconstruction Network\"\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[32m\"Short\"\u001b[0m\u001b[39m: \u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m\"DEAR\"\u001b[0m\u001b[39m, \u001b[0m\u001b[32m\"CT\"\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m}\u001b[0m\n",
       "\n",
       "\u001b[39mExample: Recent methods that use Recurrent Neural Networks \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39m RNN \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m to handle raw skeletons only focus on the \u001b[0m\n",
       "\u001b[39mcontextual dependency in the temporal domain and neglect the spatial configurations of articulated skeletons .\u001b[0m\n",
       "\u001b[39mOutput:\u001b[0m\n",
       "\u001b[1;39m{\u001b[0m\u001b[32m\"Long\"\u001b[0m\u001b[39m: \u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m\"Recurrent Neural Networks\"\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[32m\"Short\"\u001b[0m\u001b[39m: \u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m\"RNN\"\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m}\u001b[0m\n",
       "\n",
       "\u001b[39mExample: There are multiple contributing factors : a \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m labeling medical data typically requires specially trained \u001b[0m\n",
       "\u001b[39mdoctors ; b \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m marking lesion boundaries can be hard even for experts because of low signal - to - noise ratio in \u001b[0m\n",
       "\u001b[39mmany medical images ; and c \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m for CT and Magnetic Resonance Imaging \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39m MRI \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m images , the annotators need to label \u001b[0m\n",
       "\u001b[39mthe entire 3D volumetric data , which can be costly and time - consuming .\u001b[0m\n",
       "\u001b[39mOutput:\u001b[0m\n",
       "\u001b[1;39m{\u001b[0m\u001b[32m\"Long\"\u001b[0m\u001b[39m: \u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m\"Magnetic Resonance Imaging\"\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[32m\"Short\"\u001b[0m\u001b[39m: \u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m\"CT\"\u001b[0m\u001b[39m, \u001b[0m\u001b[32m\"MRI\"\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m}\u001b[0m\n",
       "\n",
       "\u001b[39mExample: Since the KITTI Average Orientation Similarity \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39m AOS \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m evaluation is also dependent on 2D detection \u001b[0m\n",
       "\u001b[39mperformance , the projection of each 3D box is replaced with its corresponding 2D box from the 2D detector , which \u001b[0m\n",
       "\u001b[39mallows AOS to be evaluated .\u001b[0m\n",
       "\u001b[39mOutput:\u001b[0m\n",
       "\u001b[1;39m{\u001b[0m\u001b[32m\"Long\"\u001b[0m\u001b[39m: \u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m\"Average Orientation Similarity\"\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[32m\"Short\"\u001b[0m\u001b[39m: \u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m\"KITTI\"\u001b[0m\u001b[39m, \u001b[0m\u001b[32m\"AOS\"\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m}\u001b[0m\n",
       "\n",
       "\u001b[39mExample: For error of fit we used the estimated Root Mean Square Error \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39m RMSE \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m between the MLE - estimated \u001b[0m\n",
       "\u001b[39mNakagami values and the observed voxels values in each voxel lattice starting at size \u001b[0m\u001b[1;36m2\u001b[0m\u001b[39m , as there would be no \u001b[0m\n",
       "\u001b[39mmeaning if a lattice had a size of \u001b[0m\u001b[1;36m1\u001b[0m\u001b[39m .\u001b[0m\n",
       "\u001b[39mOutput:\u001b[0m\n",
       "\u001b[1;39m{\u001b[0m\u001b[32m\"Long\"\u001b[0m\u001b[39m: \u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m\"Root Mean Square Error\"\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, \u001b[0m\u001b[32m\"Short\"\u001b[0m\u001b[39m: \u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m\"RMSE\"\u001b[0m\u001b[39m, \u001b[0m\u001b[32m\"MLE\"\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m}\u001b[0m\n",
       "\u001b[39m    <<SYS>\u001b[0m\u001b[1m>\u001b[0m\n",
       "    Example: The generator used a single CT image slice to predict the dose distribution along that same plane \n",
       "without considering the vertical relationship between different slices .\n",
       "Output:\n",
       "\u001b[1m[\u001b[0m\u001b[35m/\u001b[0m\u001b[95mINST\u001b[0m\u001b[1m]\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dry-run -- this tells us how much this will cost and shows an example prompt\n",
    "from autolabel import AutolabelDataset\n",
    "ds = AutolabelDataset(\"test.csv\", config=config)\n",
    "agent.plan(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd703025-54d8-4349-b0d6-736d2380e966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e15ae3386c44272a5914608d89282f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-04 15:58:32 autolabel.tasks.named_entity_recognition ERROR: invalid syntax. Perhaps you forgot a comma? (<unknown>, line 1). Could not parse LLM output: {'Long': ['Kronecker graph model', 'Block Two - Level Erdos - Renyi', 'Chung - Lu\\\\'s configuration model', 'Degree Corrected Stochastic Block Model', 'stochastic Hyperedge Replacement Grammar'], 'Short': ['Kronecker graph model', 'BTER', 'Chung - Lu\\\\'s configuration model', 'DC - SBM', 'HRG']}\n",
      "2023-10-04 15:59:37 autolabel.tasks.named_entity_recognition ERROR: unterminated string literal (detected at line 1) (<unknown>, line 1). Could not parse LLM output: {'Long': ['Named Entity Recognition', 'Coreference Resolution', 'Named Entity Linking', 'Relation Extraction', 'Knowledge Base reasoning', 'Natural Language Processing', 'Machine Translation', 'Question - Answering System', 'Natural Language Understanding', 'Text Summarization', 'Digital Assistants', 'Siri', 'Cortana', 'Google Now'], 'Short': ['IE', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP', 'NLP\n",
      "2023-10-04 16:01:50 autolabel.tasks.named_entity_recognition ERROR: unterminated string literal (detected at line 1) (<unknown>, line 1). Could not parse LLM output: {\"Long\": [], \"Short\": [\"OFDM\", \"CR\", \"PUs\", \"SUs\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\n",
      "2023-10-04 16:05:05 autolabel.tasks.named_entity_recognition ERROR: unterminated string literal (detected at line 1) (<unknown>, line 1). Could not parse LLM output: {'Long': [], 'Short': ['ASR', 'MT', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', 'IA', '\n",
      "2023-10-04 16:07:18 autolabel.tasks.named_entity_recognition ERROR: unterminated string literal (detected at line 1) (<unknown>, line 1). Could not parse LLM output: {'Long': [], 'Short': ['UAV', 'drones', 'UAV', 'drones', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'UAV', 'U\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Actual Cost: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Actual Cost: \u001b[1;36m0.0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━┳━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> support </span>┃<span style=\"font-weight: bold\"> completion_rate </span>┃<span style=\"font-weight: bold\"> f1   </span>┃<span style=\"font-weight: bold\"> accuracy </span>┃\n",
       "┡━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━╇━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 1000    </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 1.0             </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.75 </span>│<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> 0.6414   </span>│\n",
       "└─────────┴─────────────────┴──────┴──────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━┳━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1msupport\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mcompletion_rate\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mf1  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1maccuracy\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━╇━━━━━━━━━━┩\n",
       "│\u001b[1;36m \u001b[0m\u001b[1;36m1000   \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m1.0            \u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.75\u001b[0m\u001b[1;36m \u001b[0m│\u001b[1;36m \u001b[0m\u001b[1;36m0.6414  \u001b[0m\u001b[1;36m \u001b[0m│\n",
       "└─────────┴─────────────────┴──────┴──────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now, do the actual labeling\n",
    "ds = agent.run(ds, max_items=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb1c3c0-0c87-4313-b556-206b398e0498",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
