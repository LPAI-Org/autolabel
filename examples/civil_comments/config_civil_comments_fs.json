{
    "task_name": "ToxicCommentClassification",
    "task_type": "classification",
    "dataset": {
        "label_column": "label",
        "delimiter": ","
    },
    "model": {
        "provider": "llama",
        "name": "/workspace/hf-relevant-sampling-2483"
    },
    "embedding": {
        "provider": "openai"
    },
    "prompt": {
        "task_guidelines": "Output Yes if the input contains any toxicity (harmful, malicious, derogatory, threat, insult, identity attck, etc.). Output No if the input is not toxic at all. If you aren't sure about the label, output not sure",
        "output_guidelines": "",
        "labels": [
            "Yes",
            "No"
        ],
        "few_shot_examples": "seed.csv",
        "few_shot_selection": "label_diversity_similarity",
        "few_shot_num": 0,
        "example_template": "\\nInput: {example}.\\nOutput: {label}"
    }
}