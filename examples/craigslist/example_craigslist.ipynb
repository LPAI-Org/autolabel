{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1fe6e643-9453-4381-9445-bd471685fb96",
   "metadata": {},
   "source": [
    "# Labeling the [craigslist](https://huggingface.co/datasets/craigslist_bargains) dataset using Autolabel\n",
    "\n",
    "This is a multi-class classification task where the input are conversations between buyers and sellers and we have to correctly classify the item being sold into one of 6 categories. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aacac4ae-c7f9-4dee-be3a-a2a6bfa099fa",
   "metadata": {},
   "source": [
    "## Install Autolabel\n",
    "Plus, setup your OpenAI API key, since we'll be using `gpt-3.5-turbo` as our LLM for labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc19059-2f63-44b7-9a32-8b38a11249aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'refuel-autolabel[openai]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbdeca2f-dd20-4634-b3f8-1b3ed45c4705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# provide your own OpenAI API key here\n",
    "os.environ['OPENAI_API_KEY'] = 'sk-'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8eb09918-494a-42ef-86a7-df93b3ce4284",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Download the dataset\n",
    "\n",
    "This dataset is available to install via Autolabel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4c116ce-3294-45c2-9158-eac929f03a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading example dataset from https://autolabel-benchmarking.s3.us-west-2.amazonaws.com/craigslist/seed.csv to seed.csv...\n",
      "Downloading example dataset from https://autolabel-benchmarking.s3.us-west-2.amazonaws.com/craigslist/test.csv to test.csv...\n",
      "100% [........................................] [595661/595661] bytes\r"
     ]
    }
   ],
   "source": [
    "from autolabel import get_data\n",
    "\n",
    "get_data('craigslist')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f038fcc-9ec0-4f2d-b84d-2e963656c6bd",
   "metadata": {},
   "source": [
    "This downloads two datasets:\n",
    "* `test.csv`: This is the larger dataset we are trying to label using LLMs\n",
    "* `seed.csv`: This is a small dataset where we already have human-provided labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84b014d1-f45c-4479-9acc-0d20870b1786",
   "metadata": {},
   "source": [
    "## Start the labeling process!\n",
    "\n",
    "Labeling with Autolabel is a 3-step process:\n",
    "* First, we specify a labeling configuration (see `config.json` below)\n",
    "* Next, we do a dry-run on our dataset using the LLM specified in `config.json` by running `agent.plan`\n",
    "* Finally, we run the labeling with `agent.run`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33d47b67-0718-4289-bb59-989d851d09ed",
   "metadata": {},
   "source": [
    "### First labeling run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c093fe91-3508-4140-8bd6-217034e3cce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from autolabel import LabelingAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c93fae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the config\n",
    "with open('config_craigslist_mixtral.json', 'r') as f:\n",
    "     config = json.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1fad4e85-d598-413d-9b01-9690653a05ad",
   "metadata": {},
   "source": [
    "Let's review the configuration file below. You'll notice the following useful keys:\n",
    "* `task_type`: `classification` (since it's a classification task)\n",
    "* `model`: `{'provider': 'openai', 'name': 'gpt-3.5-turbo'}` (use a specific OpenAI model)\n",
    "* `prompt.task_guidelines`: `'You are an expert at understanding bank customers support complaints and queries...` (how we describe the task to the LLM)\n",
    "* `prompt.labels`: `['age_limit', 'apple_pay_or_google_pay', 'atm_support', ...]` (the full list of labels to choose from)\n",
    "* `prompt.few_shot_num`: 10 (how many labeled examples to provide to the LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a3610fd-721e-44de-9b2c-2cd73ec86bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task_name': 'CraigslistConversationClassification',\n",
       " 'task_type': 'classification',\n",
       " 'dataset': {'label_column': 'label', 'delimiter': ','},\n",
       " 'model': {'provider': 'huggingface_pipeline',\n",
       "  'name': 'Open-Orca/Mixtral-SlimOrca-8x7B'},\n",
       " 'prompt': {'task_guidelines': 'You are an expert at understanding conversations.\\n Given a text passage as input comprising of dialogue of negotiations between a seller and a buyer about the sale of an item, your task is to classify the item being sold into one of the following categories:\\n{labels}',\n",
       "  'output_guidelines': 'You will answer with just the the correct output label and nothing else.',\n",
       "  'labels': ['housing', 'furniture', 'bike', 'phone', 'car', 'electronics'],\n",
       "  'few_shot_examples': 'seed.csv',\n",
       "  'few_shot_selection': 'semantic_similarity',\n",
       "  'few_shot_num': 10,\n",
       "  'example_template': 'Input: {example}\\nOutput: {label}'}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acb4a3de-fa84-4b94-b17a-7a6fac892a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a086d0ce54e747a8bd710007b9efc7ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-02 23:57:13 root WARNING: Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "The model 'MixtralForCausalLM' is not supported for text2text-generation. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n"
     ]
    }
   ],
   "source": [
    "# create an agent for labeling\n",
    "agent = LabelingAgent(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92667a39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-02 23:57:13 sentence_transformers.SentenceTransformer INFO: Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "2024-01-02 23:57:13 sentence_transformers.SentenceTransformer INFO: Use pytorch device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb5ca080d4549c28daf8132ed6463de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9396f947485841c3bdcf63bb56526465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef51e8a21cf0462c991cf78ad3b6e462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5109c909198247abbbf8cf1560706a41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bdb730460d74c64861c1fc5125c01b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76d05229b314495281d327626d2fd5cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "191dbc3e15bb4c7ab1da2456b3321125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81defbf02cf3457493f584e5c036c10b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3aa6af0ddd04ec1884af0622fdaf059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2db4ec71f73e4ad2b34e01ee1b640585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c0387bc9d747f6ad092f544bd0f8d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a91ca2b85a334573818a619ae7d4e6a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5b4f1d7bf0c4bbc9bafb35e493ec2d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80917b43e4734f27bc2c8178baf73b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7291886c405b4055a1ce82b04f5aef39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a10e3cab98a1405eab4aa2a90f8429e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd1be616e03e41dc8fea3f0a5086d3ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d4a324bc68449749b3450d00e981ed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1d41db267334405bcbec5de800f789e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5602ffda152848f19a2954b6db2234d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ad8dbe225a940069af583b428157888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9188425ad86f40849268e6b62194bfcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d0201650f6e4b5eb9138671ad0f3c61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a709103b1474316830111fb64a6a97e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74628ff216cc4fde804456056d9a2c1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ffb3d6936f94ef7816d687fafb8c136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cde3feec4ef74fcc9505b2cafd05e841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf54ca7d1614928809fa74c99978511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f68f5710f1d4239aae8b2ef767b7c10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f413fdbe7a874d63bef888edd87f00e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d735bd116ea34f85b5e049fa23dd4d62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2293c83e2c9943259a3b3642c47a188d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cd8fb6ef90a47f5a285a4c0ce2e13f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93e5d7c5fc2045d4b14d55c4df1d4f96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d36b72312e643e996fa280a1ed26142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "400ee10837854c24a9f68f7aec241492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64641088bd6f4e9f80682eb2a0367b6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e9da97faa6746ca8f575106dcd61526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e08f797369b3436cb6badccedd781779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf13c6d3749a4f09abd7bbd6cefe996b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b8db798833041fa83ecddd4217bf867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b83064dd44d422db8fd56759f40b85e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f32bf00c28f4ddf985490a2bb3ed91e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e353ccbceb8640fb959bf24f53f8d57f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42e98979a1ea4030869de654edf1ce30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b2140ff6a6b45febc513f752d14d60b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54fdd29c14b343f3a19ede91cfff75c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1042ae279a244136a8118a8738b1ca93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e992a6927d54289983dd614ad7a71d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a77fac16090472c8ed4d8de5a1fb8f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5cb1bc29de6479ab5f989687d66e988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7502284e331438084c54f2a65e569b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aae9d0906bcf40598b4938dffedb8559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18f781c66f7a4bdf94e94a20c65e8574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e796d294e4744cf38f9a0c3181c17bd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2712c5a7157a4865a32be6ac3c0d301c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a212e8af98ec4351aff152505cb500a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03f4df47c0d14f278697fb01c2642ac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "632194b4d2d6439997d0fcace9882b72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe8360688d843c8ad3e6a4647b2dcfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d870fcff0e49c4a2e69d45a567684a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c4c1c6e04e7478096981f9dc31453cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cb2b071c44142e2bf518e80ddfc167d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74737ffc4c774d99ae13a5ec81bf1173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5420adcae0214033a0f1e029f18065c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7147836f6a9941f7bd01a949f74d22a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f94f812c797e4c4994fdd0e4bf18d695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ee85d30e9044b73998dd2f876ff4141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ba188012f594a5491e779e02b3fd01f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e143b07f2d45689ee22cf675df06a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbc20673f0b14928a572701f4536d3b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d658abf2d549d7bf2e10f768565a52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cd8c02db7484bbdacb0d64022e47174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffcabfc6a9ec470ca9166cc49b434a65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6a6bd5d4e844c25aecad26ef0e4985c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f3883bd145f4785b2046e42229e7afb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbd6c53792494002827c01485db96e7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b81079b1199c402798d7b3d81e0fe229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0544e1ea910d4a3c96e72be414064e5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a603b390ce794832b71a042c61e09881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b940208ca2c45c4b2c663b748fedc03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a479a14094ef464baa3926bb32e0d694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbf58d0f045a4e9f9aeeea446d18ec70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "298493de3f524ed89e57dd10ec66f774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d10b26e3824e2d995f8fcb884adc66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ca467bbd5044a7ab095b7c55cd94fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a39541e817e4e99a58a5a3f8b53b3a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "673dbc577dc14982a51687142fcd0a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7856eec44354330aa5ae112626fd269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3970e61c38e44e5b8e14540c1a56f0aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11cb68e376144104a754ac2bb0d70094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66f7725cb58f40e38c14d3002f627416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7adfeb981e23444ba98103ccba84e2f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bf1e2b3c2a94843a4aa63cd69b8acb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa39a41635f43a49e81f54fcfc3b677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21306cedd769459bb462f8a174f70667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59fe775e29ac42869c3a3d6b8ae1d6ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "072e5f5fad514ec2aa15137b53ab9d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9d54492232c4c8ca17b9b5d088c488a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ca36716995843719eae8c8385706a39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7f053b55b1a4dedb0e4394349007f05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┌──────────────────────────┬──────┐\n",
       "│<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Total Estimated Cost     </span>│<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> $0.0 </span>│\n",
       "│<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Number of Examples       </span>│<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> 1000 </span>│\n",
       "│<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Average cost per example </span>│<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> $0.0 </span>│\n",
       "└──────────────────────────┴──────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┌──────────────────────────┬──────┐\n",
       "│\u001b[1;35m \u001b[0m\u001b[1;35mTotal Estimated Cost    \u001b[0m\u001b[1;35m \u001b[0m│\u001b[1;32m \u001b[0m\u001b[1;32m$0.0\u001b[0m\u001b[1;32m \u001b[0m│\n",
       "│\u001b[1;35m \u001b[0m\u001b[1;35mNumber of Examples      \u001b[0m\u001b[1;35m \u001b[0m│\u001b[1;32m \u001b[0m\u001b[1;32m1000\u001b[0m\u001b[1;32m \u001b[0m│\n",
       "│\u001b[1;35m \u001b[0m\u001b[1;35mAverage cost per example\u001b[0m\u001b[1;35m \u001b[0m│\u001b[1;32m \u001b[0m\u001b[1;32m$0.0\u001b[0m\u001b[1;32m \u001b[0m│\n",
       "└──────────────────────────┴──────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">───────────────────────────────────────────────── </span>Prompt Example<span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ──────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m───────────────────────────────────────────────── \u001b[0mPrompt Example\u001b[92m ──────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">You are an expert at understanding conversations.\n",
       " Given a text passage as input comprising of dialogue of negotiations between a seller and a buyer about the sale \n",
       "of an item, your task is to classify the item being sold into one of the following categories:\n",
       "housing\n",
       "furniture\n",
       "bike\n",
       "phone\n",
       "car\n",
       "electronics\n",
       "\n",
       "You will answer with just the the correct output label and nothing else.\n",
       "\n",
       "Some examples with their output answers are provided below:\n",
       "\n",
       "Input: Seller: hi, are you interested in this listing? Buyer: I am! Is it still up for sale? How long have you had \n",
       "it? Seller: i have had it for <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> years Buyer: Okay. If I picked it up would $<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">200</span> seem fair? Seller: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">200</span> is quite low\n",
       "but, if you can pick up i will go down to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">350</span>  Buyer: That works, I could do $<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">350</span>. Tomorrow @ <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>? Seller: sounds \n",
       "good! Seller:  Buyer: \n",
       "Output: furniture\n",
       "Input: Seller: Hi! Buyer: Hello. How are you? Seller: Great thanks! I am selling this great piece for a cheap \n",
       "price. It is going for $<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">250</span>. Buyer: So you found this trunk in a second hand store? Seller: Yes it was incredible! \n",
       "I love the piece and it is a shame that it has to go! But we want someone else to experience its beauty! Buyer: \n",
       "Could I pick it up today, with cash? Seller: Ok, I will give it to you for two easy payments of $<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">124.99</span>! Buyer: How\n",
       "about $<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">210</span> all at once? Seller: Oh! That is a tough one! How about <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">225</span>, and we have a deal! Buyer: Fine.  That'll \n",
       "do. Buyer:  Seller: \n",
       "Output: furniture\n",
       "Input: Seller: HI. I see that you are interested in the Bike Buyer: Hello. Is the bike still available? Buyer: Yes \n",
       "I am. How old is the bike? Seller: It was made in <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1973</span>. It is a very classic bike.  Buyer: Thanks. I see you have \n",
       "it listed for $<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50</span>. Would you accept $<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35</span>? Seller: $<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35</span> seems a little low. How about I deliver the bike to you for \n",
       "$<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">45</span>? Buyer: I will pick it up from you if you sell for $<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">43</span>? Seller: Yea great! I can do $<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">43</span>. Seller:  Buyer: \n",
       "Output: bike\n",
       "Input: Seller: Hello, are you interested in my chairs? I'll drive them to your house for $<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">150</span>, deal? Buyer: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">150</span> is \n",
       "too much for me, can you negotiate the price? Seller: \n",
       "Output: furniture\n",
       "Input: Seller: Hi, are you calling about the desk? Buyer: Yes I'm interested in the desk what kind of condition is \n",
       "it in? Seller: It is in excellent condition. Like new actually. I was going to usee it in my office, but then my \n",
       "wife left me. Now I need to sell it to pay her off. Buyer: Would you be willing to work with me on the price at \n",
       "all? Seller: I could probably come down a bit, but not too much. What were you thinking? Buyer: I was thinking \n",
       "about <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> bucks Seller: How about <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35</span> and I can include the chair as well. Buyer: You know what that sounds good to \n",
       "me we have a deal. Seller: very good <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35</span> with the chair. Seller:  Buyer: \n",
       "Output: furniture\n",
       "Input: Buyer: Hey there, I was intrested in your bike, but I have a few questions. Seller: I still have the bike, \n",
       "what do you want to know? Buyer: I was wondering how long you have had it, and the condition it is in Seller: I've \n",
       "only had it for <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> year and it's in excellent condition!  Buyer: That's good to hear. I am really interested, its a \n",
       "great price, but is there any way I could get you to come down a bit? maybe $<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1700</span>? Seller: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1700</span> is too low for the \n",
       "quality of my bike! It's hardly seen any use and I've already dropped the price so much in my listing. Please \n",
       "reconsider. Buyer: As true as it is, I really cant go that high. could we agree upon something a little lower? \n",
       "maybe $<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2000</span>? Seller: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2300</span> if you come to pick it up and I can agree let it go for that. Seller:  Buyer: Fair enough\n",
       "I suppose. Buyer: \n",
       "Output: bike\n",
       "Input: Buyer: Hi there. Is this available? Seller: yes Buyer: Is this used? Seller: Yes but it is in great shape \n",
       "Buyer: any scratches or rims? Seller: Nope. Like brand new. Buyer: okay good. The price is high for me can you \n",
       "accept <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">535</span> for it? Seller: It still has the service and warranty on it. I can go down to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">575</span>. Buyer: okay since its\n",
       "what I have been looking for months, I will take it. I haven't been able to get this for so long. Can you please \n",
       "ship it to me? Seller: Sure but depending on the cost I might have to add more to the price. Buyer: That's not a \n",
       "problem. Great then we've got the deal? Seller: Deal Seller:  Buyer: \n",
       "Output: phone\n",
       "Input: Seller: Hello, are you interested in the dining room table I have listed? Buyer: I noitice that your list \n",
       "price is $<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">95</span>, would you consider taking a lower offer? Seller: Yes, I will consider it, however I believe $<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">95</span> is \n",
       "quite fair because the table to quite big and it includes <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> chairs as well. Buyer: Yes, it would be a fair price \n",
       "but the table has scratches and I have to pick it up myself.  Seller: The scratches are extremely minor, but what \n",
       "is your offer? Buyer: I would like to offer $<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">40</span> Seller: I wouldn't be able to accept anything lower than $<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">80</span>.  \n",
       "Buyer: Do you think we could meet in the middle at $<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">60</span>? Seller: I think $<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">80</span> is more than fair for this table and \n",
       "chairs. Buyer: \n",
       "Output: furniture\n",
       "Input: Buyer: Hello im interested in your bike. How old is it Seller: The bike is a year old. I bought it used a \n",
       "few months ago. Buyer: Do you know how long the other owner had it? Are the tires and paint good? Seller: He \n",
       "received it as a Christmas gift in <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2016</span>. The paint is in excellent condition. I did not replace the tires before \n",
       "putting it up for sale so I would knock $<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">40</span> off the price so you can replace them. Buyer: Well its a bit old. \n",
       "Unfortunately I only have $<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">150.00</span> How does that sound? Seller: I don't think I want to go quite that low. Would you\n",
       "be willing to do $<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">200</span>? Buyer: I guess I can do that, thank you! Buyer:  Seller: \n",
       "Output: bike\n",
       "Input: Buyer: hello, is your bike still for sale? Seller: yes it sure is. Were you interested? Buyer: yes i am, it \n",
       "looks great! i was hoping maybe we could negotiate the price a little though. Seller: thanks, well let you give you\n",
       "a little information about it. It's a Univega Veva Sport <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> speed bicycle, made in Japan Buyer: i think it's cool \n",
       "that it's from japan, how long have you had the bike though? i was hoping i could pick it up from you for <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">180</span> or \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">190</span>. Seller: I know, me too. I've had it for a few years, mechanical wise it's great but it does have scratches, \n",
       "nicks and scuffs which is why I would be willing to go lower Buyer: oh okay, well i understand that then, i dont \n",
       "mind nicks and scruffs but i wouldn't want to pay more than <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">190</span> if that works for you. Seller: yes that works for \n",
       "me Buyer: great, thanks. Buyer:  Seller: \n",
       "Output: bike\n",
       "\n",
       "Now I want you to label the following example:\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">|im_end|</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;|im_start|&gt;user</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Input: Seller: Hi are you interested in buying my Pinwheel sculpture? Buyer: Yes I think I might be, it is very </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">interesting</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"color: #000000; text-decoration-color: #000000\"> have you had it a long time? Seller: Its been owned for </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\"> years, I'll sell it to you for </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">450</span><span style=\"color: #000000; text-decoration-color: #000000\">. Buyer:</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">That is quite high and is unfortunately out of my budget</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"color: #000000; text-decoration-color: #000000\"> would you consider negotiating the price if I can pick </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">it up in person? Seller: I can negotiate but don't low ball me, I'll offer it for </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">425</span><span style=\"color: #000000; text-decoration-color: #000000\">. Buyer: Well that is a bit </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">rude </span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"color: #000000; text-decoration-color: #000000\"> Buyer:  Seller: </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Output: &lt;|im_end|&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;|im_start|</span><span style=\"font-weight: bold\">&gt;</span>assistant\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "You are an expert at understanding conversations.\n",
       " Given a text passage as input comprising of dialogue of negotiations between a seller and a buyer about the sale \n",
       "of an item, your task is to classify the item being sold into one of the following categories:\n",
       "housing\n",
       "furniture\n",
       "bike\n",
       "phone\n",
       "car\n",
       "electronics\n",
       "\n",
       "You will answer with just the the correct output label and nothing else.\n",
       "\n",
       "Some examples with their output answers are provided below:\n",
       "\n",
       "Input: Seller: hi, are you interested in this listing? Buyer: I am! Is it still up for sale? How long have you had \n",
       "it? Seller: i have had it for \u001b[1;36m3\u001b[0m years Buyer: Okay. If I picked it up would $\u001b[1;36m200\u001b[0m seem fair? Seller: \u001b[1;36m200\u001b[0m is quite low\n",
       "but, if you can pick up i will go down to \u001b[1;36m350\u001b[0m  Buyer: That works, I could do $\u001b[1;36m350\u001b[0m. Tomorrow @ \u001b[1;36m3\u001b[0m? Seller: sounds \n",
       "good! Seller:  Buyer: \n",
       "Output: furniture\n",
       "Input: Seller: Hi! Buyer: Hello. How are you? Seller: Great thanks! I am selling this great piece for a cheap \n",
       "price. It is going for $\u001b[1;36m250\u001b[0m. Buyer: So you found this trunk in a second hand store? Seller: Yes it was incredible! \n",
       "I love the piece and it is a shame that it has to go! But we want someone else to experience its beauty! Buyer: \n",
       "Could I pick it up today, with cash? Seller: Ok, I will give it to you for two easy payments of $\u001b[1;36m124.99\u001b[0m! Buyer: How\n",
       "about $\u001b[1;36m210\u001b[0m all at once? Seller: Oh! That is a tough one! How about \u001b[1;36m225\u001b[0m, and we have a deal! Buyer: Fine.  That'll \n",
       "do. Buyer:  Seller: \n",
       "Output: furniture\n",
       "Input: Seller: HI. I see that you are interested in the Bike Buyer: Hello. Is the bike still available? Buyer: Yes \n",
       "I am. How old is the bike? Seller: It was made in \u001b[1;36m1973\u001b[0m. It is a very classic bike.  Buyer: Thanks. I see you have \n",
       "it listed for $\u001b[1;36m50\u001b[0m. Would you accept $\u001b[1;36m35\u001b[0m? Seller: $\u001b[1;36m35\u001b[0m seems a little low. How about I deliver the bike to you for \n",
       "$\u001b[1;36m45\u001b[0m? Buyer: I will pick it up from you if you sell for $\u001b[1;36m43\u001b[0m? Seller: Yea great! I can do $\u001b[1;36m43\u001b[0m. Seller:  Buyer: \n",
       "Output: bike\n",
       "Input: Seller: Hello, are you interested in my chairs? I'll drive them to your house for $\u001b[1;36m150\u001b[0m, deal? Buyer: \u001b[1;36m150\u001b[0m is \n",
       "too much for me, can you negotiate the price? Seller: \n",
       "Output: furniture\n",
       "Input: Seller: Hi, are you calling about the desk? Buyer: Yes I'm interested in the desk what kind of condition is \n",
       "it in? Seller: It is in excellent condition. Like new actually. I was going to usee it in my office, but then my \n",
       "wife left me. Now I need to sell it to pay her off. Buyer: Would you be willing to work with me on the price at \n",
       "all? Seller: I could probably come down a bit, but not too much. What were you thinking? Buyer: I was thinking \n",
       "about \u001b[1;36m20\u001b[0m bucks Seller: How about \u001b[1;36m35\u001b[0m and I can include the chair as well. Buyer: You know what that sounds good to \n",
       "me we have a deal. Seller: very good \u001b[1;36m35\u001b[0m with the chair. Seller:  Buyer: \n",
       "Output: furniture\n",
       "Input: Buyer: Hey there, I was intrested in your bike, but I have a few questions. Seller: I still have the bike, \n",
       "what do you want to know? Buyer: I was wondering how long you have had it, and the condition it is in Seller: I've \n",
       "only had it for \u001b[1;36m1\u001b[0m year and it's in excellent condition!  Buyer: That's good to hear. I am really interested, its a \n",
       "great price, but is there any way I could get you to come down a bit? maybe $\u001b[1;36m1700\u001b[0m? Seller: \u001b[1;36m1700\u001b[0m is too low for the \n",
       "quality of my bike! It's hardly seen any use and I've already dropped the price so much in my listing. Please \n",
       "reconsider. Buyer: As true as it is, I really cant go that high. could we agree upon something a little lower? \n",
       "maybe $\u001b[1;36m2000\u001b[0m? Seller: \u001b[1;36m2300\u001b[0m if you come to pick it up and I can agree let it go for that. Seller:  Buyer: Fair enough\n",
       "I suppose. Buyer: \n",
       "Output: bike\n",
       "Input: Buyer: Hi there. Is this available? Seller: yes Buyer: Is this used? Seller: Yes but it is in great shape \n",
       "Buyer: any scratches or rims? Seller: Nope. Like brand new. Buyer: okay good. The price is high for me can you \n",
       "accept \u001b[1;36m535\u001b[0m for it? Seller: It still has the service and warranty on it. I can go down to \u001b[1;36m575\u001b[0m. Buyer: okay since its\n",
       "what I have been looking for months, I will take it. I haven't been able to get this for so long. Can you please \n",
       "ship it to me? Seller: Sure but depending on the cost I might have to add more to the price. Buyer: That's not a \n",
       "problem. Great then we've got the deal? Seller: Deal Seller:  Buyer: \n",
       "Output: phone\n",
       "Input: Seller: Hello, are you interested in the dining room table I have listed? Buyer: I noitice that your list \n",
       "price is $\u001b[1;36m95\u001b[0m, would you consider taking a lower offer? Seller: Yes, I will consider it, however I believe $\u001b[1;36m95\u001b[0m is \n",
       "quite fair because the table to quite big and it includes \u001b[1;36m4\u001b[0m chairs as well. Buyer: Yes, it would be a fair price \n",
       "but the table has scratches and I have to pick it up myself.  Seller: The scratches are extremely minor, but what \n",
       "is your offer? Buyer: I would like to offer $\u001b[1;36m40\u001b[0m Seller: I wouldn't be able to accept anything lower than $\u001b[1;36m80\u001b[0m.  \n",
       "Buyer: Do you think we could meet in the middle at $\u001b[1;36m60\u001b[0m? Seller: I think $\u001b[1;36m80\u001b[0m is more than fair for this table and \n",
       "chairs. Buyer: \n",
       "Output: furniture\n",
       "Input: Buyer: Hello im interested in your bike. How old is it Seller: The bike is a year old. I bought it used a \n",
       "few months ago. Buyer: Do you know how long the other owner had it? Are the tires and paint good? Seller: He \n",
       "received it as a Christmas gift in \u001b[1;36m2016\u001b[0m. The paint is in excellent condition. I did not replace the tires before \n",
       "putting it up for sale so I would knock $\u001b[1;36m40\u001b[0m off the price so you can replace them. Buyer: Well its a bit old. \n",
       "Unfortunately I only have $\u001b[1;36m150.00\u001b[0m How does that sound? Seller: I don't think I want to go quite that low. Would you\n",
       "be willing to do $\u001b[1;36m200\u001b[0m? Buyer: I guess I can do that, thank you! Buyer:  Seller: \n",
       "Output: bike\n",
       "Input: Buyer: hello, is your bike still for sale? Seller: yes it sure is. Were you interested? Buyer: yes i am, it \n",
       "looks great! i was hoping maybe we could negotiate the price a little though. Seller: thanks, well let you give you\n",
       "a little information about it. It's a Univega Veva Sport \u001b[1;36m10\u001b[0m speed bicycle, made in Japan Buyer: i think it's cool \n",
       "that it's from japan, how long have you had the bike though? i was hoping i could pick it up from you for \u001b[1;36m180\u001b[0m or \n",
       "\u001b[1;36m190\u001b[0m. Seller: I know, me too. I've had it for a few years, mechanical wise it's great but it does have scratches, \n",
       "nicks and scuffs which is why I would be willing to go lower Buyer: oh okay, well i understand that then, i dont \n",
       "mind nicks and scruffs but i wouldn't want to pay more than \u001b[1;36m190\u001b[0m if that works for you. Seller: yes that works for \n",
       "me Buyer: great, thanks. Buyer:  Seller: \n",
       "Output: bike\n",
       "\n",
       "Now I want you to label the following example:\n",
       "\u001b[1m<\u001b[0m\u001b[1;95m|im_end|\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<|im_start|>user\u001b[0m\n",
       "\u001b[39mInput: Seller: Hi are you interested in buying my Pinwheel sculpture? Buyer: Yes I think I might be, it is very \u001b[0m\n",
       "\u001b[39minteresting\u001b[0m\u001b[33m...\u001b[0m\u001b[39m have you had it a long time? Seller: Its been owned for \u001b[0m\u001b[1;36m2\u001b[0m\u001b[39m years, I'll sell it to you for \u001b[0m\u001b[1;36m450\u001b[0m\u001b[39m. Buyer:\u001b[0m\n",
       "\u001b[39mThat is quite high and is unfortunately out of my budget\u001b[0m\u001b[33m...\u001b[0m\u001b[39m would you consider negotiating the price if I can pick \u001b[0m\n",
       "\u001b[39mit up in person? Seller: I can negotiate but don't low ball me, I'll offer it for \u001b[0m\u001b[1;36m425\u001b[0m\u001b[39m. Buyer: Well that is a bit \u001b[0m\n",
       "\u001b[39mrude \u001b[0m\u001b[33m...\u001b[0m\u001b[39m Buyer:  Seller: \u001b[0m\n",
       "\u001b[39mOutput: <|im_end|>\u001b[0m\n",
       "\u001b[39m<|im_start|\u001b[0m\u001b[1m>\u001b[0massistant\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dry-run -- this tells us how much this will cost and shows an example prompt\n",
    "from autolabel import AutolabelDataset\n",
    "ds = AutolabelDataset(\"test.csv\", config=config)\n",
    "agent.plan(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd703025-54d8-4349-b0d6-736d2380e966",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a5107ef619d46968af1f31a4408aefd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf12641128554bd8acf18c711d73eccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-02 23:57:31 autolabel.tasks.base WARNING: LLM response is not in the labels list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf68d2bb92d94d7b82fc74f91c8cda15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a54d2f5aaa9d4ac7ac398fae79660e10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38300b24cae94ef59d6ae478a16629d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be3bec3bec89407e8cef5070ec189953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-02 23:58:03 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 950.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 778.88 MiB is free. Process 1489750 has 78.33 GiB memory in use. Of the allocated memory 77.35 GiB is allocated by PyTorch, and 334.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 950.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 778.88 MiB is free. Process 1489750 has 78.33 GiB memory in use. Of the allocated memory 77.61 GiB is \n",
       "allocated by PyTorch, and 65.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 950.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 778.88 MiB is free. Process 1489750 has 78.33 GiB memory in use. Of the allocated memory 77.61 GiB is \n",
       "allocated by PyTorch, and 65.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "395a9c5fc99b462fb1c5b17cec0a6917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be967e60718348d5a6fa956967504870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "892940f0e7ad44a79985ad70530fad67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa61b906bc04475a7af921c4510adea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb16343a52d64b7fb9a8b5298d8e0b51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-02 23:58:38 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 876.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 862.88 MiB is free. Process 1489750 has 78.25 GiB memory in use. Of the allocated memory 77.27 GiB is allocated by PyTorch, and 336.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 876.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 862.88 MiB is free. Process 1489750 has 78.25 GiB memory in use. Of the allocated memory 77.52 GiB is \n",
       "allocated by PyTorch, and 78.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 876.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 862.88 MiB is free. Process 1489750 has 78.25 GiB memory in use. Of the allocated memory 77.52 GiB is \n",
       "allocated by PyTorch, and 78.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04b632d24d2e4b5e9ac25867b967a371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8441e1d188a47f3b7f8559099017412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b43c629b04ab4bcead8a95f73f046ef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b51989657644af79a132e2966bb55ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be0653099f14dc4b90f1a7724326ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a6101d683c4089a0cbca8e20a7fef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f90f3b8122494c30ba949cfcc7c7f896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1bbc577ae804770af9f15dc0bab348d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9160303806e240c9a7aa658aabc569a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72747de06420499289fc7cfbbf01df22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "609f802d5faf4a55b657ab27ee828938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c5e3d3fad24931ae0f8400c74e27ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24420a616147492abb536afb8f172824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b560325c47784ffe92d4edf64e7567ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd7d652a853a49b1a8375281c3bd13fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b35009290664230ab6c84cd12c6b3bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5b28745abd34a58a476bfe87b14ac0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:01:01 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 962.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 904.88 MiB is free. Process 1489750 has 78.21 GiB memory in use. Of the allocated memory 77.36 GiB is allocated by PyTorch, and 196.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 962.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 904.88 MiB is free. Process 1489750 has 78.21 GiB memory in use. Of the allocated memory 76.69 GiB is \n",
       "allocated by PyTorch, and 885.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 962.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 904.88 MiB is free. Process 1489750 has 78.21 GiB memory in use. Of the allocated memory 76.69 GiB is \n",
       "allocated by PyTorch, and 885.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6743651b70b74119b6d77181ff1435d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:01:03 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 154.88 MiB is free. Process 1489750 has 78.94 GiB memory in use. Of the allocated memory 77.31 GiB is allocated by PyTorch, and 998.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 154.88 MiB is free. Process 1489750 has 78.94 GiB memory in use. Of the allocated memory 77.54 GiB is \n",
       "allocated by PyTorch, and 761.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 750.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 154.88 MiB is free. Process 1489750 has 78.94 GiB memory in use. Of the allocated memory 77.54 GiB is \n",
       "allocated by PyTorch, and 761.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f106683cdcf4e72a4dfae90a321b519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "997f8ae9536144598f63870444399942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8cc917287e3422aa0df6f2ba76e691f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "117e4503624c4558849eb55484885f05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "328361278c8f4450a065f21c9bbc7a2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:01:44 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 680.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 424.88 MiB is free. Process 1489750 has 78.68 GiB memory in use. Of the allocated memory 77.27 GiB is allocated by PyTorch, and 765.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 680.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 424.88 MiB is free. Process 1489750 has 78.68 GiB memory in use. Of the allocated memory 77.49 GiB is \n",
       "allocated by PyTorch, and 541.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 680.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 424.88 MiB is free. Process 1489750 has 78.68 GiB memory in use. Of the allocated memory 77.49 GiB is \n",
       "allocated by PyTorch, and 541.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b2f401875e94a4b8aa5bf2cd2c846c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:01:45 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 786.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 666.88 MiB is free. Process 1489750 has 78.44 GiB memory in use. Of the allocated memory 77.16 GiB is allocated by PyTorch, and 637.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 786.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 666.88 MiB is free. Process 1489750 has 78.44 GiB memory in use. Of the allocated memory 77.40 GiB is \n",
       "allocated by PyTorch, and 395.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 786.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 666.88 MiB is free. Process 1489750 has 78.44 GiB memory in use. Of the allocated memory 77.40 GiB is \n",
       "allocated by PyTorch, and 395.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e4839ea9fa344258df8967d168c0008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b61e35a10d64d3b85e9039d1756fc4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44377438197c4c8285e7ab69d352cecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0593d79ac045471fba00c7585283af02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c1ee0cb19e4251a7eb5436eb691d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e983e02a27204fc8b2883191c6d6c0e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc402f90a2c943b9969052f075a8cee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f40946cbad4b4404a7956ebd497e0057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:02:53 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 908.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 884.88 MiB is free. Process 1489750 has 78.23 GiB memory in use. Of the allocated memory 77.30 GiB is allocated by PyTorch, and 277.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 908.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 884.88 MiB is free. Process 1489750 has 78.23 GiB memory in use. Of the allocated memory 76.67 GiB is \n",
       "allocated by PyTorch, and 922.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 908.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 884.88 MiB is free. Process 1489750 has 78.23 GiB memory in use. Of the allocated memory 76.67 GiB is \n",
       "allocated by PyTorch, and 922.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "219a731614fb4eebb14fc68600b793d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cfa1277b8e843fcb77ef77f7767bea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c56bd3dc7554d6e91ad73bf033f6652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09fab76bbb754cc1934cadb6788e977c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2548cb60037049a28c15254e571c03df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9416ec0e60fb4b459c1ef301d48d0ebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1040bf5422646479ff6f6f4166e8791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e86851fe08764925baa6c989960ee55a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a63364b60cdd4006b0d4cd7a31c7aaab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9512c92802b4f8fb272f2e299710532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f24c18178654a6d93d4d52a5ab30aa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544e3d638c654fa495da350e9443673c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0daf1ef5f7414c57a899d3948fd5f346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a5de9ad29ac4a90acbfbc73e5611b6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebfa7752d97a4898bf8046209be60c61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0d2095efa14da9b9214bb42b646d8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a7d8ba861484503b3362031e9ad0f29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cce28015df7463095e7aa1b7e5057dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a46da5d99b60450aaf0ff4234900931c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19a2628c8c844c43a0d46e4fbf2399a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5de2c875bcaa4f7f9fe465754ce28b6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f91b404a580140be8f2a36e8f18c4370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "182b70e8abd74ad3a6c0db8bca6e719d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d62a7276334f77a1ad1653162a4ac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ac877fd7d5405bb83ac20a6ba18371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c356e72d50b94f33b5bfb060cd67880f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ad21eb9f50244398453481690c4f68c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71e20b780bca481b9f9c93a1c8320b41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f4c33e5b4db4dfaba990104914f297f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44d1a1ca021a4713bce1a1d87c8e1f5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70c499919d7e45e6af13ad1a765ceec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:07:29 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 930.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 880.88 MiB is free. Process 1489750 has 78.23 GiB memory in use. Of the allocated memory 77.33 GiB is allocated by PyTorch, and 255.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 930.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 880.88 MiB is free. Process 1489750 has 78.23 GiB memory in use. Of the allocated memory 76.68 GiB is \n",
       "allocated by PyTorch, and 919.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 930.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 880.88 MiB is free. Process 1489750 has 78.23 GiB memory in use. Of the allocated memory 76.68 GiB is \n",
       "allocated by PyTorch, and 919.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d01d0395e86a4fb28471b7030dd89213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:07:31 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 800.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 80.88 MiB is free. Process 1489750 has 79.01 GiB memory in use. Of the allocated memory 77.41 GiB is allocated by PyTorch, and 966.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 800.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 80.88 MiB is free. Process 1489750 has 79.01 GiB memory in use. Of the allocated memory 77.65 GiB is \n",
       "allocated by PyTorch, and 721.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 800.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 80.88 MiB is free. Process 1489750 has 79.01 GiB memory in use. Of the allocated memory 77.65 GiB is \n",
       "allocated by PyTorch, and 721.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e07751af23413b876a32a41a3ecbcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:07:34 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 772.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 80.88 MiB is free. Process 1489750 has 79.01 GiB memory in use. Of the allocated memory 77.39 GiB is allocated by PyTorch, and 985.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 772.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 80.88 MiB is free. Process 1489750 has 79.01 GiB memory in use. Of the allocated memory 77.63 GiB is \n",
       "allocated by PyTorch, and 745.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 772.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 80.88 MiB is free. Process 1489750 has 79.01 GiB memory in use. Of the allocated memory 77.63 GiB is \n",
       "allocated by PyTorch, and 745.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d0789e21954a6e89987667b520c5cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09acf13eee7143698d8525a18d136c2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adc978fe6c974be6b09cc9cb15d78ab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69a56872e2a44260986041f9f85d80c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab29bcfbbcb64adc986fb08e05dce4cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84dc34ac5df94aff94250993211d08dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "921c8e49173a4ebe971789dfc7ab022c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c86c43750e54b19b51b6a9f4a55a315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aa16c5f4cd2498187e7cf5360f4de4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a93157cfc18469485ef9e3e6deed3f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:09:03 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 866.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 52.88 MiB is free. Process 1489750 has 79.04 GiB memory in use. Of the allocated memory 77.52 GiB is allocated by PyTorch, and 886.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 866.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 52.88 MiB is free. Process 1489750 has 79.04 GiB memory in use. Of the allocated memory 77.77 GiB is \n",
       "allocated by PyTorch, and 630.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 866.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 52.88 MiB is free. Process 1489750 has 79.04 GiB memory in use. Of the allocated memory 77.77 GiB is \n",
       "allocated by PyTorch, and 630.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7db3a17f356647a78f1bb5e23aaf9a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "431c7900519b430590b5c60020fcf02d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "943bc97a0f494c3d88efc344e245c71d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "492f479965ff4a6289c14c27e25d9e1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:09:35 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 866.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 90.88 MiB is free. Process 1489750 has 79.00 GiB memory in use. Of the allocated memory 77.48 GiB is allocated by PyTorch, and 892.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 866.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 90.88 MiB is free. Process 1489750 has 79.00 GiB memory in use. Of the allocated memory 77.73 GiB is \n",
       "allocated by PyTorch, and 635.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 866.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 90.88 MiB is free. Process 1489750 has 79.00 GiB memory in use. Of the allocated memory 77.73 GiB is \n",
       "allocated by PyTorch, and 635.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0670ce023fe84594925bfbcc863e76c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f92aaf17d8a44e43bf003db347cc8c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6762b0e6c4f0475793d636914df3c6fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4afff838f8a74510a9bb6d635607c522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c150f3d3a6a54fe4bcd0a93d6cb599f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0a64e4435c54ebeb54e8cabb5cc7b89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e12908bc854b4696d2bc8aabcdffc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f72663ac381b4098927a97531441c971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfa0f3a56ff74b94a1b0d60e83667021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e40f0b2c58243338dae53909635eb56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "835892d7acdb49419f1c81b87a5d3207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:11:02 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 972.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 888.88 MiB is free. Process 1489750 has 78.22 GiB memory in use. Of the allocated memory 77.37 GiB is allocated by PyTorch, and 201.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 972.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 888.88 MiB is free. Process 1489750 has 78.22 GiB memory in use. Of the allocated memory 76.69 GiB is \n",
       "allocated by PyTorch, and 898.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 972.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 888.88 MiB is free. Process 1489750 has 78.22 GiB memory in use. Of the allocated memory 76.69 GiB is \n",
       "allocated by PyTorch, and 898.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e64c8644eff04bdab483db5b69636c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:11:05 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 708.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 180.88 MiB is free. Process 1489750 has 78.92 GiB memory in use. Of the allocated memory 77.29 GiB is allocated by PyTorch, and 989.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 708.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 180.88 MiB is free. Process 1489750 has 78.92 GiB memory in use. Of the allocated memory 77.52 GiB is \n",
       "allocated by PyTorch, and 760.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 708.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 180.88 MiB is free. Process 1489750 has 78.92 GiB memory in use. Of the allocated memory 77.52 GiB is \n",
       "allocated by PyTorch, and 760.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc978ffbe4784feda6e155c725a9ba4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:11:07 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 794.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 94.88 MiB is free. Process 1489750 has 79.00 GiB memory in use. Of the allocated memory 77.37 GiB is allocated by PyTorch, and 1001.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 794.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 94.88 MiB is free. Process 1489750 has 79.00 GiB memory in use. Of the allocated memory 77.60 GiB is \n",
       "allocated by PyTorch, and 757.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 794.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 94.88 MiB is free. Process 1489750 has 79.00 GiB memory in use. Of the allocated memory 77.60 GiB is \n",
       "allocated by PyTorch, and 757.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ab1f8921648496eb2a0a8be3b870cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f5a9bcb06f4beea7a28a0a72395a04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:11:18 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 1.10 GiB. GPU 1 has a total capacty of 79.11 GiB of which 746.88 MiB is free. Process 1489750 has 78.36 GiB memory in use. Of the allocated memory 77.54 GiB is allocated by PyTorch, and 167.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 1.10 GiB. GPU 1 has a total capacty of 79.11 GiB \n",
       "of which 746.88 MiB is free. Process 1489750 has 78.36 GiB memory in use. Of the allocated memory 76.74 GiB is \n",
       "allocated by PyTorch, and 993.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 1.10 GiB. GPU 1 has a total capacty of 79.11 GiB \n",
       "of which 746.88 MiB is free. Process 1489750 has 78.36 GiB memory in use. Of the allocated memory 76.74 GiB is \n",
       "allocated by PyTorch, and 993.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a9317e7104a44c5aa6cba22eb5ee7b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:11:20 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 712.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 34.88 MiB is free. Process 1489750 has 79.06 GiB memory in use. Of the allocated memory 77.28 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 712.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 34.88 MiB is free. Process 1489750 has 79.06 GiB memory in use. Of the allocated memory 77.50 GiB is \n",
       "allocated by PyTorch, and 919.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 712.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 34.88 MiB is free. Process 1489750 has 79.06 GiB memory in use. Of the allocated memory 77.50 GiB is \n",
       "allocated by PyTorch, and 919.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d124de88a754e5bacc7b4ca6ef7523e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db7df7bba8bc4b15bf545568d48fdaf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f2f6385bcc24a5ca0bd9f51aa50379c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a9a0da3b9d74367889f2b7347fa9851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "265469f0739f4d2c9b18ad262e413cd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d47b5840459e4dfda52b7aba52c0229a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37dbcd4721e54d06864ed9ff99b1f8d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c58f2f61167e44079830aafd2310879f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b2c157e3d4b4482b0ad804b044f1d05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c97acef4b8d347d4886f925415590609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32095ef4bd9c475cb92edd8b0003350d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28def6bd9714a49b4e46ab5b3197e88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d7709d9a864f24bc1a69ef40ae67f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c850c5906a5f4dd39ca6660be319e9fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b42afc1592d2494ebf549aff06e9ed06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0310399fd0f4541aa059874a94d3c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:13:27 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 896.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 84.88 MiB is free. Process 1489750 has 79.01 GiB memory in use. Of the allocated memory 77.45 GiB is allocated by PyTorch, and 921.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 896.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 84.88 MiB is free. Process 1489750 has 79.01 GiB memory in use. Of the allocated memory 77.71 GiB is \n",
       "allocated by PyTorch, and 660.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 896.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 84.88 MiB is free. Process 1489750 has 79.01 GiB memory in use. Of the allocated memory 77.71 GiB is \n",
       "allocated by PyTorch, and 660.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca37d851b91e490c837aedfc5d3c0e39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98389e5c9f4e4d5aa5e9208a85d5b78d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:13:39 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 910.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 60.88 MiB is free. Process 1489750 has 79.03 GiB memory in use. Of the allocated memory 77.47 GiB is allocated by PyTorch, and 928.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 910.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 60.88 MiB is free. Process 1489750 has 79.03 GiB memory in use. Of the allocated memory 77.73 GiB is \n",
       "allocated by PyTorch, and 665.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 910.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 60.88 MiB is free. Process 1489750 has 79.03 GiB memory in use. Of the allocated memory 77.73 GiB is \n",
       "allocated by PyTorch, and 665.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0be55b9baa842b081087eacf08f3c82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f457a178e674aedb6026246b4cb01b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:13:50 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 1.09 GiB. GPU 1 has a total capacty of 79.11 GiB of which 762.88 MiB is free. Process 1489750 has 78.35 GiB memory in use. Of the allocated memory 77.53 GiB is allocated by PyTorch, and 165.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 1.09 GiB. GPU 1 has a total capacty of 79.11 GiB \n",
       "of which 762.88 MiB is free. Process 1489750 has 78.35 GiB memory in use. Of the allocated memory 76.73 GiB is \n",
       "allocated by PyTorch, and 981.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 1.09 GiB. GPU 1 has a total capacty of 79.11 GiB \n",
       "of which 762.88 MiB is free. Process 1489750 has 78.35 GiB memory in use. Of the allocated memory 76.73 GiB is \n",
       "allocated by PyTorch, and 981.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e183aab14f4dd4ad86c81c7f828284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:13:52 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 632.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 130.88 MiB is free. Process 1489750 has 78.96 GiB memory in use. Of the allocated memory 77.19 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 632.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 130.88 MiB is free. Process 1489750 has 78.96 GiB memory in use. Of the allocated memory 77.40 GiB is \n",
       "allocated by PyTorch, and 927.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 632.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 130.88 MiB is free. Process 1489750 has 78.96 GiB memory in use. Of the allocated memory 77.40 GiB is \n",
       "allocated by PyTorch, and 927.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d7b795b4cf44e1da03330974175c213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70ad3ae8be384a679cb503bc0cd3a15d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58fd502fa3c84262a1b1688acd637c35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:14:11 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 786.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 280.88 MiB is free. Process 1489750 has 78.82 GiB memory in use. Of the allocated memory 77.36 GiB is allocated by PyTorch, and 824.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 786.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 280.88 MiB is free. Process 1489750 has 78.82 GiB memory in use. Of the allocated memory 77.59 GiB is \n",
       "allocated by PyTorch, and 582.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 786.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 280.88 MiB is free. Process 1489750 has 78.82 GiB memory in use. Of the allocated memory 77.59 GiB is \n",
       "allocated by PyTorch, and 582.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fb875e66e7d416dbd4bdfd9a1a23544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:14:13 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 854.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 182.88 MiB is free. Process 1489750 has 78.91 GiB memory in use. Of the allocated memory 77.40 GiB is allocated by PyTorch, and 876.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 854.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 182.88 MiB is free. Process 1489750 has 78.91 GiB memory in use. Of the allocated memory 77.65 GiB is \n",
       "allocated by PyTorch, and 622.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 854.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 182.88 MiB is free. Process 1489750 has 78.91 GiB memory in use. Of the allocated memory 77.65 GiB is \n",
       "allocated by PyTorch, and 622.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87f1da2cfb3b43c5ae0c65d3f609d5e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41e28fbfc0e843bb9b67ac64409cd221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f5cb0359574a9abd25fdc564d4b825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95906d40c9174d14aafcf3c2916e3fcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bdaf2faa8f84b08819363ce4b0de988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:14:49 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 812.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 166.88 MiB is free. Process 1489750 has 78.93 GiB memory in use. Of the allocated memory 77.45 GiB is allocated by PyTorch, and 845.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 812.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 166.88 MiB is free. Process 1489750 has 78.93 GiB memory in use. Of the allocated memory 77.69 GiB is \n",
       "allocated by PyTorch, and 598.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 812.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 166.88 MiB is free. Process 1489750 has 78.93 GiB memory in use. Of the allocated memory 77.69 GiB is \n",
       "allocated by PyTorch, and 598.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0ae1f16f43641b29258c9cd4eac0e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faaebd42781c43d39689ec83cb02a8f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15627caf78c641ce80b45278f72106e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c7f2937e15401dae61117fb28f5772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b780b3c24dc41e7b54446fb3650e395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "555aaa543c5749a081eac23193bf74f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52891a53f54345aeaf8d2c83ac695759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "246f7f4eb6e54563a57bcb25989d0f94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e90edbfc48784c89a89798735a1376b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2306f1ce6b14af1880fd1c1d6ca9d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:16:04 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 886.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 114.88 MiB is free. Process 1489750 has 78.98 GiB memory in use. Of the allocated memory 77.42 GiB is allocated by PyTorch, and 924.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 886.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 114.88 MiB is free. Process 1489750 has 78.98 GiB memory in use. Of the allocated memory 77.67 GiB is \n",
       "allocated by PyTorch, and 665.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 886.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 114.88 MiB is free. Process 1489750 has 78.98 GiB memory in use. Of the allocated memory 77.67 GiB is \n",
       "allocated by PyTorch, and 665.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c419fff30674cc794daa771660ec00c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2904602d9db4c229f2b4769c057017e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcc8931ac6a84bfc96e1aece25f11ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02685ad4588e431390b50d16de8b895d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e13cade256140b48ec769eede2d53c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e24022aba764a50b86b00e853331866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3661e21a22fb44679e46feebf66626a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a958c669919d4dd6a755f187e693ba24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da3a107500f344d18f1bf04c21c525d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbe2525c1e1549b299a3fbbb43a7bff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "743b90348c26419dbce1232f20a60797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c8f1e37ef844175ae9c18b2d4b45f15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0db63c5cb6c48878f9070a0a10f7cdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:17:39 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 796.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 230.88 MiB is free. Process 1489750 has 78.87 GiB memory in use. Of the allocated memory 77.37 GiB is allocated by PyTorch, and 861.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 796.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 230.88 MiB is free. Process 1489750 has 78.87 GiB memory in use. Of the allocated memory 77.61 GiB is \n",
       "allocated by PyTorch, and 617.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 796.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 230.88 MiB is free. Process 1489750 has 78.87 GiB memory in use. Of the allocated memory 77.61 GiB is \n",
       "allocated by PyTorch, and 617.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e6126572b29410e9cb1b79f0d85cd4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2556801458474ff98f486451dce64126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25715921110445dca0cc3c253f30f7f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7245ed62e1c440ba3d8b0975a173391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67fd8bea1fd5478891cd45d42c8c8d4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2532506e0f044a5e8c3c74744d04da36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92cd8896dc6b453084ccb6e4adf7b819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7b6a67ea05a4092a7ab75d7f4b88fce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cd99ee321154e62aafe5bbb75923af9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d79a52d792447f19097e3b93175dc96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "791d869303134ce282ef399a05178c14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e57901261d6417fb2c5dd897f93cfde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70ca5a122a484dbc9e758d2a2e55ca18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9935c8dfe464e55963a1219d090c583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "632ce75f40b24a0dafcce1712203dabd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2f6d1819691424faefa86f24740619b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:19:42 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 870.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 300.88 MiB is free. Process 1489750 has 78.80 GiB memory in use. Of the allocated memory 77.28 GiB is allocated by PyTorch, and 884.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 870.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 300.88 MiB is free. Process 1489750 has 78.80 GiB memory in use. Of the allocated memory 77.53 GiB is \n",
       "allocated by PyTorch, and 627.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 870.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 300.88 MiB is free. Process 1489750 has 78.80 GiB memory in use. Of the allocated memory 77.53 GiB is \n",
       "allocated by PyTorch, and 627.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae4052b43b3b4d4dbf04a6bbfbc07f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:19:44 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 650.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 300.88 MiB is free. Process 1489750 has 78.80 GiB memory in use. Of the allocated memory 77.22 GiB is allocated by PyTorch, and 947.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 650.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 300.88 MiB is free. Process 1489750 has 78.80 GiB memory in use. Of the allocated memory 77.43 GiB is \n",
       "allocated by PyTorch, and 728.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 650.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 300.88 MiB is free. Process 1489750 has 78.80 GiB memory in use. Of the allocated memory 77.43 GiB is \n",
       "allocated by PyTorch, and 728.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cba3dc3a48744fdcbc819b572738d5c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7319b38cb564fd781903b62e6186456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e17eb06c26140b6b6d3c2a7e0375477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "194402a960a44054825c530aa070aea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a0fb0b9b234a4cb660661b6d7158c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b09ccd6a117c403e927bc14ef11792b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "317f4c4933ba47b3a134545e46a01c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b4ba2288934e08a279d7d1f62cd1b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:20:52 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 990.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 938.88 MiB is free. Process 1489750 has 78.18 GiB memory in use. Of the allocated memory 77.39 GiB is allocated by PyTorch, and 130.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 990.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 938.88 MiB is free. Process 1489750 has 78.18 GiB memory in use. Of the allocated memory 76.70 GiB is \n",
       "allocated by PyTorch, and 843.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 990.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 938.88 MiB is free. Process 1489750 has 78.18 GiB memory in use. Of the allocated memory 76.70 GiB is \n",
       "allocated by PyTorch, and 843.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92e9256306db4a42b4102d7ebaf5cf5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3389f1e5b3cf480aa89b0436af116286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40ca5d325649427f9c301633b4e590ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "797212883550402487dad6ff004b08cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ec6bcd8e62444fdb8dcdfc4cf965cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "008438276a0d49ac8f736b34b42c3df9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7724875a3c8748eca0cc486632a4020d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9732aa62f4e401fa29f229d046b0603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1f0f8aabe34fb0a3980f67540a901a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:22:00 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 1016.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 920.88 MiB is free. Process 1489750 has 78.19 GiB memory in use. Of the allocated memory 77.42 GiB is allocated by PyTorch, and 117.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 1016.00 MiB. GPU 1 has a total capacty of 79.11 \n",
       "GiB of which 920.88 MiB is free. Process 1489750 has 78.19 GiB memory in use. Of the allocated memory 76.70 GiB is \n",
       "allocated by PyTorch, and 852.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 1016.00 MiB. GPU 1 has a total capacty of 79.11 \n",
       "GiB of which 920.88 MiB is free. Process 1489750 has 78.19 GiB memory in use. Of the allocated memory 76.70 GiB is \n",
       "allocated by PyTorch, and 852.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42b5439e6754456aafb33cf6ac735948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:22:02 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 554.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 366.88 MiB is free. Process 1489750 has 78.73 GiB memory in use. Of the allocated memory 77.07 GiB is allocated by PyTorch, and 1.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 554.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 366.88 MiB is free. Process 1489750 has 78.73 GiB memory in use. Of the allocated memory 77.27 GiB is \n",
       "allocated by PyTorch, and 829.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 554.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 366.88 MiB is free. Process 1489750 has 78.73 GiB memory in use. Of the allocated memory 77.27 GiB is \n",
       "allocated by PyTorch, and 829.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62654653a1794190b621b384a3720eae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:22:04 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 790.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 130.88 MiB is free. Process 1489750 has 78.96 GiB memory in use. Of the allocated memory 77.30 GiB is allocated by PyTorch, and 1.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 790.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 130.88 MiB is free. Process 1489750 has 78.96 GiB memory in use. Of the allocated memory 77.54 GiB is \n",
       "allocated by PyTorch, and 786.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 790.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 130.88 MiB is free. Process 1489750 has 78.96 GiB memory in use. Of the allocated memory 77.54 GiB is \n",
       "allocated by PyTorch, and 786.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84cf0cb55074ee89b6a5845b6384d0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:22:06 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 696.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 130.88 MiB is free. Process 1489750 has 78.96 GiB memory in use. Of the allocated memory 77.28 GiB is allocated by PyTorch, and 1.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 696.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 130.88 MiB is free. Process 1489750 has 78.96 GiB memory in use. Of the allocated memory 77.50 GiB is \n",
       "allocated by PyTorch, and 830.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 696.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 130.88 MiB is free. Process 1489750 has 78.96 GiB memory in use. Of the allocated memory 77.50 GiB is \n",
       "allocated by PyTorch, and 830.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceac3e8b13a1424d863bd73c5a9ad77c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d6f0cb6496461e96f5afa7dd846bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:22:19 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 774.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 268.88 MiB is free. Process 1489750 has 78.83 GiB memory in use. Of the allocated memory 77.38 GiB is allocated by PyTorch, and 810.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 774.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 268.88 MiB is free. Process 1489750 has 78.83 GiB memory in use. Of the allocated memory 77.62 GiB is \n",
       "allocated by PyTorch, and 570.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 774.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 268.88 MiB is free. Process 1489750 has 78.83 GiB memory in use. Of the allocated memory 77.62 GiB is \n",
       "allocated by PyTorch, and 570.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a56612821bf84e12951ca42a0392e7b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90d66a60272b4918813ba8aeffc94645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a83c5f5ff3bf48989dd0da9c4c46788e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6483a4e8f62f459dbaf5fb6783f73877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e0ab91e6fba4fb2a48e663547a3f883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "857bdd88478a4d5aa03cd224e3f7ebd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5204b42acac2450da77ff989cfaf4ede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baa2efbc7ba442af8052b238352323d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a4ea4304b94471aa4c7ea12dd27dabe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1bbc057df9c4f5389b39d4a63333c45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "674f6a974bf34833af5552e27409c94d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a0f2326d0984b9a911dec02a8193319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11b954d23b8c41999fec111e019dc951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b16a4e17854948b19831cc8d26f78e16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10bb0d5ccea1495c84a3e0bff6e9c593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90698d2ebd8944f49204b1008b3f1eaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:24:26 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 898.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 236.88 MiB is free. Process 1489750 has 78.86 GiB memory in use. Of the allocated memory 77.31 GiB is allocated by PyTorch, and 917.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 898.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 236.88 MiB is free. Process 1489750 has 78.86 GiB memory in use. Of the allocated memory 77.56 GiB is \n",
       "allocated by PyTorch, and 656.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 898.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 236.88 MiB is free. Process 1489750 has 78.86 GiB memory in use. Of the allocated memory 77.56 GiB is \n",
       "allocated by PyTorch, and 656.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f3c2dc8fafb473d8a1dc4ad14a792e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f3fce0e2e74e2aa457a0d92c56df70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:24:37 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 712.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 422.88 MiB is free. Process 1489750 has 78.68 GiB memory in use. Of the allocated memory 77.10 GiB is allocated by PyTorch, and 949.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 712.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 422.88 MiB is free. Process 1489750 has 78.68 GiB memory in use. Of the allocated memory 77.32 GiB is \n",
       "allocated by PyTorch, and 720.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 712.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 422.88 MiB is free. Process 1489750 has 78.68 GiB memory in use. Of the allocated memory 77.32 GiB is \n",
       "allocated by PyTorch, and 720.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f27440bc124d4e41a83265e4d3fb21db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:24:39 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 736.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 398.88 MiB is free. Process 1489750 has 78.70 GiB memory in use. Of the allocated memory 77.12 GiB is allocated by PyTorch, and 944.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 736.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 398.88 MiB is free. Process 1489750 has 78.70 GiB memory in use. Of the allocated memory 77.35 GiB is \n",
       "allocated by PyTorch, and 710.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 736.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 398.88 MiB is free. Process 1489750 has 78.70 GiB memory in use. Of the allocated memory 77.35 GiB is \n",
       "allocated by PyTorch, and 710.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28d9ac9b3ce446a1b3d9d949484fdb12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:24:41 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 652.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 398.88 MiB is free. Process 1489750 has 78.70 GiB memory in use. Of the allocated memory 77.10 GiB is allocated by PyTorch, and 974.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 652.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 398.88 MiB is free. Process 1489750 has 78.70 GiB memory in use. Of the allocated memory 77.31 GiB is \n",
       "allocated by PyTorch, and 756.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 652.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 398.88 MiB is free. Process 1489750 has 78.70 GiB memory in use. Of the allocated memory 77.31 GiB is \n",
       "allocated by PyTorch, and 756.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52e21b4fade04e269d84ef5f5e9e5704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:24:42 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 934.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 386.88 MiB is free. Process 1489750 has 78.71 GiB memory in use. Of the allocated memory 77.34 GiB is allocated by PyTorch, and 740.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 934.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 386.88 MiB is free. Process 1489750 has 78.71 GiB memory in use. Of the allocated memory 77.60 GiB is \n",
       "allocated by PyTorch, and 474.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 934.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 386.88 MiB is free. Process 1489750 has 78.71 GiB memory in use. Of the allocated memory 77.60 GiB is \n",
       "allocated by PyTorch, and 474.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b35a250d794c38bd4ecc6b209519b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "232d564ef5074e6392bb956794fb4b4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:24:53 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 672.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 648.88 MiB is free. Process 1489750 has 78.46 GiB memory in use. Of the allocated memory 77.05 GiB is allocated by PyTorch, and 771.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 672.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 648.88 MiB is free. Process 1489750 has 78.46 GiB memory in use. Of the allocated memory 77.27 GiB is \n",
       "allocated by PyTorch, and 548.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 672.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 648.88 MiB is free. Process 1489750 has 78.46 GiB memory in use. Of the allocated memory 77.27 GiB is \n",
       "allocated by PyTorch, and 548.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be2b599c1468462d9f6eae7a618f3dd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bfaba455c73443f9d165ee0f7a1596e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:25:03 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 850.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 590.88 MiB is free. Process 1489750 has 78.52 GiB memory in use. Of the allocated memory 77.23 GiB is allocated by PyTorch, and 640.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 850.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 590.88 MiB is free. Process 1489750 has 78.52 GiB memory in use. Of the allocated memory 77.48 GiB is \n",
       "allocated by PyTorch, and 387.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 850.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 590.88 MiB is free. Process 1489750 has 78.52 GiB memory in use. Of the allocated memory 77.48 GiB is \n",
       "allocated by PyTorch, and 387.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae60cdd047d450894109651129f9af9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:25:05 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 636.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 590.88 MiB is free. Process 1489750 has 78.52 GiB memory in use. Of the allocated memory 76.99 GiB is allocated by PyTorch, and 892.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 636.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 590.88 MiB is free. Process 1489750 has 78.52 GiB memory in use. Of the allocated memory 77.20 GiB is \n",
       "allocated by PyTorch, and 676.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 636.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 590.88 MiB is free. Process 1489750 has 78.52 GiB memory in use. Of the allocated memory 77.20 GiB is \n",
       "allocated by PyTorch, and 676.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14c6fb8f63484631a663a54e331bec27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efc039d4237740cb84298ef7c3fc7b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b335424e157a42b7ae4e97890359b4ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f2284065bcd4c97a35ad3c506b16e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f00d89bbeafb4b4eb092a749cef01f79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e8e21fb526a4ab0998d6d70fbc5c860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52841302896a444080e6279a00855790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:25:58 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 912.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 772.88 MiB is free. Process 1489750 has 78.34 GiB memory in use. Of the allocated memory 77.31 GiB is allocated by PyTorch, and 384.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 912.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 772.88 MiB is free. Process 1489750 has 78.34 GiB memory in use. Of the allocated memory 77.56 GiB is \n",
       "allocated by PyTorch, and 121.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 912.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 772.88 MiB is free. Process 1489750 has 78.34 GiB memory in use. Of the allocated memory 77.56 GiB is \n",
       "allocated by PyTorch, and 121.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61f99ae9f76643f295117afba5b05a6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca22e58a2a5f46efb5e815ebe6fe154d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "027969e0b6b64034b87379bae0b5373c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "877373c8d0694a08b96c3f78bc0cba0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcdeb12be877489181529212acc0e139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24d3365b91104f9a99f891c66b4cc44a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4d400f05c1342f7903a8b96ed9327ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daaa3ffac17d423bb4c39cbdff07583d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a4cea641fc647a79afc84f02de815e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:27:10 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 844.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 310.88 MiB is free. Process 1489750 has 78.79 GiB memory in use. Of the allocated memory 77.29 GiB is allocated by PyTorch, and 864.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 844.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 310.88 MiB is free. Process 1489750 has 78.79 GiB memory in use. Of the allocated memory 77.53 GiB is \n",
       "allocated by PyTorch, and 612.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 844.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 310.88 MiB is free. Process 1489750 has 78.79 GiB memory in use. Of the allocated memory 77.53 GiB is \n",
       "allocated by PyTorch, and 612.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "182e54065c8d4c80ab11642aa72e2e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddbe1e6d63644b9b81631374f4ed80e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:27:20 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 586.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 568.88 MiB is free. Process 1489750 has 78.54 GiB memory in use. Of the allocated memory 77.03 GiB is allocated by PyTorch, and 870.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 586.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 568.88 MiB is free. Process 1489750 has 78.54 GiB memory in use. Of the allocated memory 77.23 GiB is \n",
       "allocated by PyTorch, and 664.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 586.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 568.88 MiB is free. Process 1489750 has 78.54 GiB memory in use. Of the allocated memory 77.23 GiB is \n",
       "allocated by PyTorch, and 664.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49d6f4c45c6e4b98b2d323c8ae70415c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d7741d03354548815004d3037692d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba6ebed6470c4d18acb3b4c9f6639879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:27:37 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 802.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 394.88 MiB is free. Process 1489750 has 78.71 GiB memory in use. Of the allocated memory 77.24 GiB is allocated by PyTorch, and 830.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 802.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 394.88 MiB is free. Process 1489750 has 78.71 GiB memory in use. Of the allocated memory 77.48 GiB is \n",
       "allocated by PyTorch, and 585.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 802.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 394.88 MiB is free. Process 1489750 has 78.71 GiB memory in use. Of the allocated memory 77.48 GiB is \n",
       "allocated by PyTorch, and 585.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce11a7112499408f884a85670f9fd30b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:27:39 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 732.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 394.88 MiB is free. Process 1489750 has 78.71 GiB memory in use. Of the allocated memory 77.21 GiB is allocated by PyTorch, and 859.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 732.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 394.88 MiB is free. Process 1489750 has 78.71 GiB memory in use. Of the allocated memory 77.44 GiB is \n",
       "allocated by PyTorch, and 625.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 732.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 394.88 MiB is free. Process 1489750 has 78.71 GiB memory in use. Of the allocated memory 77.44 GiB is \n",
       "allocated by PyTorch, and 625.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd7e406973754b27b1b4b36928e6f74e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:27:41 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 714.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 394.88 MiB is free. Process 1489750 has 78.71 GiB memory in use. Of the allocated memory 77.24 GiB is allocated by PyTorch, and 825.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 714.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 394.88 MiB is free. Process 1489750 has 78.71 GiB memory in use. Of the allocated memory 77.47 GiB is \n",
       "allocated by PyTorch, and 595.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 714.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 394.88 MiB is free. Process 1489750 has 78.71 GiB memory in use. Of the allocated memory 77.47 GiB is \n",
       "allocated by PyTorch, and 595.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d49ea485fe43b5aed025b96fe49fbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:27:42 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 798.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 394.88 MiB is free. Process 1489750 has 78.71 GiB memory in use. Of the allocated memory 77.23 GiB is allocated by PyTorch, and 837.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 798.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 394.88 MiB is free. Process 1489750 has 78.71 GiB memory in use. Of the allocated memory 77.47 GiB is \n",
       "allocated by PyTorch, and 592.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 798.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 394.88 MiB is free. Process 1489750 has 78.71 GiB memory in use. Of the allocated memory 77.47 GiB is \n",
       "allocated by PyTorch, and 592.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc8f7691ecfc4883acfa33843a56f653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:27:44 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 728.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 394.88 MiB is free. Process 1489750 has 78.71 GiB memory in use. Of the allocated memory 77.21 GiB is allocated by PyTorch, and 862.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 728.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 394.88 MiB is free. Process 1489750 has 78.71 GiB memory in use. Of the allocated memory 77.44 GiB is \n",
       "allocated by PyTorch, and 630.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 728.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 394.88 MiB is free. Process 1489750 has 78.71 GiB memory in use. Of the allocated memory 77.44 GiB is \n",
       "allocated by PyTorch, and 630.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5791434730d40349becf2d2cd6197fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:27:47 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 640.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 394.88 MiB is free. Process 1489750 has 78.71 GiB memory in use. Of the allocated memory 77.22 GiB is allocated by PyTorch, and 850.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 640.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 394.88 MiB is free. Process 1489750 has 78.71 GiB memory in use. Of the allocated memory 77.43 GiB is \n",
       "allocated by PyTorch, and 634.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 640.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 394.88 MiB is free. Process 1489750 has 78.71 GiB memory in use. Of the allocated memory 77.43 GiB is \n",
       "allocated by PyTorch, and 634.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d963347b00544d02882549ee985409ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:27:48 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 854.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 290.88 MiB is free. Process 1489750 has 78.81 GiB memory in use. Of the allocated memory 77.30 GiB is allocated by PyTorch, and 873.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 854.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 290.88 MiB is free. Process 1489750 has 78.81 GiB memory in use. Of the allocated memory 77.55 GiB is \n",
       "allocated by PyTorch, and 619.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 854.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 290.88 MiB is free. Process 1489750 has 78.81 GiB memory in use. Of the allocated memory 77.55 GiB is \n",
       "allocated by PyTorch, and 619.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d8bf3eee464684a2eac6c166042320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1758d9fa1364489ab452f084f6cd2050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:28:03 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 594.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 550.88 MiB is free. Process 1489750 has 78.55 GiB memory in use. Of the allocated memory 77.04 GiB is allocated by PyTorch, and 876.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 594.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 550.88 MiB is free. Process 1489750 has 78.55 GiB memory in use. Of the allocated memory 77.25 GiB is \n",
       "allocated by PyTorch, and 668.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 594.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 550.88 MiB is free. Process 1489750 has 78.55 GiB memory in use. Of the allocated memory 77.25 GiB is \n",
       "allocated by PyTorch, and 668.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ffcd6bb89634c999a3ffe89fc20af17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:28:04 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 858.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 282.88 MiB is free. Process 1489750 has 78.82 GiB memory in use. Of the allocated memory 77.31 GiB is allocated by PyTorch, and 874.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 858.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 282.88 MiB is free. Process 1489750 has 78.82 GiB memory in use. Of the allocated memory 77.55 GiB is \n",
       "allocated by PyTorch, and 619.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 858.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 282.88 MiB is free. Process 1489750 has 78.82 GiB memory in use. Of the allocated memory 77.55 GiB is \n",
       "allocated by PyTorch, and 619.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7943fa00ef84e8f8ea7cc086ae87e11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e7b9e70db4343ac90169fafaa04c0ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbc5330d1605455b996145067469af0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d80ca7888f2b4115b0eedda9dd6ad4b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2b3370932dd42fbbc2e486ada854328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d58bfdd6cd7544938deee779455d7da7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:28:49 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 850.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 330.88 MiB is free. Process 1489750 has 78.77 GiB memory in use. Of the allocated memory 77.26 GiB is allocated by PyTorch, and 876.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 850.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 330.88 MiB is free. Process 1489750 has 78.77 GiB memory in use. Of the allocated memory 77.51 GiB is \n",
       "allocated by PyTorch, and 622.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 850.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 330.88 MiB is free. Process 1489750 has 78.77 GiB memory in use. Of the allocated memory 77.51 GiB is \n",
       "allocated by PyTorch, and 622.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c429a1df9ab04eee8c1c411b92fc2a58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b7f36c2c90488aba263c39b5cfd8e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9974fb8dfa644d1a9bac470172c07004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa48e7589dee4740a89518f8e7a8df5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb2cabdb1654457798ca830af18e5d5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce4d4830304e4d039f0be2a6cbaa2b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "2024-01-03 00:29:34 autolabel.models.hf_pipeline ERROR: Error while generating output via HF Pipeline: CUDA out of memory. Tried to allocate 882.00 MiB. GPU 1 has a total capacty of 79.11 GiB of which 46.88 MiB is free. Process 1489750 has 79.05 GiB memory in use. Of the allocated memory 77.52 GiB is allocated by PyTorch, and 894.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error generating from LLM: CUDA out of memory. Tried to allocate 882.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 46.88 MiB is free. Process 1489750 has 79.05 GiB memory in use. Of the allocated memory 77.77 GiB is \n",
       "allocated by PyTorch, and 636.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error generating from LLM: CUDA out of memory. Tried to allocate 882.00 MiB. GPU 1 has a total capacty of 79.11 GiB\n",
       "of which 46.88 MiB is free. Process 1489750 has 79.05 GiB memory in use. Of the allocated memory 77.77 GiB is \n",
       "allocated by PyTorch, and 636.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is \n",
       "large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cf8d365550a44dbb83c6a6a4a8827d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e00dcddf9be4aa7a108ceea9c2d2a2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e92c3c79ac7c4113af38f83b1c1ff504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cc2915f83b34803a86ca790601a0c42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e2d52ea62394f6f81df3b544834d96e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df8cb064fe254429a20b02c5378d9425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41e7e0f188c44cf6a803242b9d8ceb05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa0f2d21b5045e5a9627d0f12ceba51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed60b0aa38a42118d467ca19ec80004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16f9ea620c7844569cd6c533540b8cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "199df62876a94bad8d9a5644e78d05d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7ccea31d626494fa6bedd7f69769c03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fca97ab4e864a28ad467e57f9dd95ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee2df2ac5d74c2f9b851fefa45fafb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d2d0735de2341a1af18f81ef1f83a44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d470e9a140d043a9aa1e3a5761e056c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34b93c921a947fe9f9d38d818730f5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff0fa73d424648d284234fba61eb9166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the \n",
       "pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# now, do the actual labeling\n",
    "ds = agent.run(ds, max_items=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0a4bc7-b62e-4ad3-96a5-9e04fe53c24b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
