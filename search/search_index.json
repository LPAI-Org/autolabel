{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>Autolabel is a Python library to label, clean and enrich datasets with Large Language Models (LLMs).</p>"},{"location":"#new-access-refuelllm-through-autolabel","title":"\ud83c\udf1f (New!) Access RefuelLLM through Autolabel","text":"<p>You can access RefuelLLM, our recently announced LLM purpose built for data labeling, through Autolabel (Read more about it in this blog post). Refuel LLM is a Llama-v2-13b base model, instruction tuned on over 2500 unique (5.24B tokens) labeling tasks spanning categories such as classification, entity resolution, matching, reading comprehension and information extraction. You can experiment with the model in the playground here.</p> <p></p> <p>You can request access to Refuel LLM here. Read the docs about using RefuelLLM in autolabel here.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Autolabel data for NLP tasks such as classification, question-answering and named entity-recognition, entity matching and more.</li> <li>Seamlessly use commercial and open source LLMs from providers such as OpenAI, Anthropic, HuggingFace, Google and more.</li> <li>Leverage research-proven LLM techniques to boost label quality, such as few-shot learning and chain-of-thought prompting.</li> <li>Confidence estimation and explanations out of the box for every single output label</li> <li>Caching and state management to minimize costs and experimentation time</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>You can get started with Autolabel by simpling bringing the dataset you want to label, picking your favorite LLM and writing a few lines of code.</p> <ul> <li>Installation and your first labeling task: Steps to install Autolabel and run sentiment analysis for movie reviews using OpenAI's <code>gpt-3.5-turbo</code>.</li> <li>Classification tutorial: A deeper dive into how Autolabel can be used to detect toxic comments at 95%+ accuracy.</li> <li>Command Line Interface: Learn how to use Autolabel's CLI to intuitively create configs from the command line.</li> <li>Here are more examples with sample notebooks that show how Autolabel can be used for different NLP tasks.</li> </ul>"},{"location":"#resources","title":"Resources","text":"<ul> <li>Discord: Join our Discord community for conversations on LLMs, Autolabel and so much more!</li> <li>Github: Create an issue to report any bugs or give us a star on Github.</li> <li>Contribute: Share your feedback or add new features, and help us improve Autolabel!</li> </ul>"},{"location":"concepts/concepts/","title":"Modules","text":"<p>On this page, we will talk about the different pages that exist in Autolabel. We will first discuss the overview of a module and then go into the different subheadings, expanding and giving some examples for each.</p>"},{"location":"concepts/concepts/#prompts","title":"Prompts","text":"<p>Writing prompts is a crucial aspect of training language models for specific tasks. In this tutorial, we will explore the five essential parts of a prompt: the prefix prompt, task prompt, output prompt, seed examples, and current example. Understanding and constructing these components effectively can help guide the model's behavior and generate accurate and contextually appropriate responses. Let's delve into each part in detail.</p>"},{"location":"concepts/concepts/#prefix-prompt","title":"Prefix Prompt","text":"<p>The prefix prompt is the initial line of the prompt, which sets the domain and provides task-independent information to the model. It helps the model understand the specific area or expertise it should embody while generating responses. For example, if the prefix prompt indicates a medical domain, the model will focus on generating responses that align with medical knowledge and terminology. Example: [Medical] In this prompt, the model should provide expert advice on diagnosing and treating common ailments.</p>"},{"location":"concepts/concepts/#task-prompt","title":"Task Prompt","text":"<p>The task prompt explains the objective or task the model needs to accomplish. It describes the specific instructions or guidelines for completing the task. This section is crucial for clearly conveying the desired output from the model. Example: You are a medical expert. Given a patient's symptoms and medical history, provide a diagnosis and recommend appropriate treatment options.</p>"},{"location":"concepts/concepts/#output-prompt","title":"Output Prompt","text":"<p>The output prompt informs the model about the expected answer format or structure. It defines the specific format in which the model should provide the answer. This step ensures consistency and enables easier processing of the model's responses. Example: Provide the diagnosis and treatment recommendations in JSON format, with the following keys: \"diagnosis\" and \"treatment.\" The value for each key should be a string representing the diagnosis and treatment, respectively.</p>"},{"location":"concepts/concepts/#seed-examples","title":"Seed Examples","text":"<p>Seed examples play a vital role in training the model by providing real-world examples from the task distribution. These examples help the model grasp the nature of the task, understand the expected outputs, and align its behavior accordingly. It is crucial to provide meaningful and diverse seed examples to facilitate accurate responses. Example: Seed Examples:  </p> <p>Patient: Fever, sore throat, and fatigue. Medical History: None. Diagnosis: \"Common cold\" Treatment: \"Rest, plenty of fluids, and over-the-counter cold medication.\" Patient: Persistent cough, shortness of breath, and wheezing. Medical History: Asthma. Diagnosis: \"Asthma exacerbation\" Treatment: \"Inhaled bronchodilators and corticosteroids as prescribed.\"</p>"},{"location":"concepts/concepts/#current-example","title":"Current Example","text":"<p>The current example is the specific instance for which you seek the model's response. It provides the exact answer or label you want the model to assign to this particular example. Example: Current Example: Patient: Severe headache, visual disturbances, and nausea. Medical History: None. Desired Diagnosis: \"Migraine\" Desired Treatment: \"Prescribed pain-relief medication and lifestyle modifications.\"  </p>"},{"location":"concepts/concepts/#configs","title":"Configs","text":"<p>There are 3 modules required by every labeling run - 1. A task 2. An LLM 3. A dataset</p> <p>All 3 of these modules can be instantiated with configs. A config can be passed in as a dictionary or as the path to a json file. The config consists of different keys and the following section will list out each key along with the property of the module that it affects.</p>"},{"location":"concepts/concepts/#config","title":"Config","text":"<p>The Config class is used to parse, validate, and store information about the labeling task being performed.</p> <p>             Bases: <code>BaseConfig</code></p> <p>Class to parse and store configs passed to Autolabel agent.</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>class AutolabelConfig(BaseConfig):\n    \"\"\"Class to parse and store configs passed to Autolabel agent.\"\"\"\n\n    # Top-level config keys\n    TASK_NAME_KEY = \"task_name\"\n    TASK_TYPE_KEY = \"task_type\"\n    DATASET_CONFIG_KEY = \"dataset\"\n    MODEL_CONFIG_KEY = \"model\"\n    EMBEDDING_CONFIG_KEY = \"embedding\"\n    PROMPT_CONFIG_KEY = \"prompt\"\n    DATASET_GENERATION_CONFIG_KEY = \"dataset_generation\"\n    CHUNKING_CONFIG_KEY = \"chunking\"\n\n    # Dataset config keys (config[\"dataset\"][&lt;key&gt;])\n    LABEL_COLUMN_KEY = \"label_column\"\n    LABEL_SEPARATOR_KEY = \"label_separator\"\n    EXPLANATION_COLUMN_KEY = \"explanation_column\"\n    IMAGE_COLUMN_KEY = \"image_url_column\"\n    TEXT_COLUMN_KEY = \"text_column\"\n    INPUT_COLUMNS_KEY = \"input_columns\"\n    DELIMITER_KEY = \"delimiter\"\n    DISABLE_QUOTING = \"disable_quoting\"\n\n    # Model config keys (config[\"model\"][&lt;key&gt;])\n    PROVIDER_KEY = \"provider\"\n    MODEL_NAME_KEY = \"name\"\n    MODEL_PARAMS_KEY = \"params\"\n    COMPUTE_CONFIDENCE_KEY = \"compute_confidence\"\n    LOGIT_BIAS_KEY = \"logit_bias\"\n\n    # Embedding config keys (config[\"embedding\"][&lt;key&gt;])\n    EMBEDDING_PROVIDER_KEY = \"provider\"\n    EMBEDDING_MODEL_NAME_KEY = \"model\"\n\n    # Prompt config keys (config[\"prompt\"][&lt;key&gt;])\n    TASK_GUIDELINE_KEY = \"task_guidelines\"\n    VALID_LABELS_KEY = \"labels\"\n    FEW_SHOT_EXAMPLE_SET_KEY = \"few_shot_examples\"\n    FEW_SHOT_SELECTION_ALGORITHM_KEY = \"few_shot_selection\"\n    FEW_SHOT_NUM_KEY = \"few_shot_num\"\n    VECTOR_STORE_PARAMS_KEY = \"vector_store_params\"\n    EXAMPLE_TEMPLATE_KEY = \"example_template\"\n    OUTPUT_GUIDELINE_KEY = \"output_guidelines\"\n    OUTPUT_FORMAT_KEY = \"output_format\"\n    CHAIN_OF_THOUGHT_KEY = \"chain_of_thought\"\n    LABEL_SELECTION_KEY = \"label_selection\"\n    LABEL_SELECTION_COUNT_KEY = \"label_selection_count\"\n    LABEL_SELECTION_THRESHOLD = \"label_selection_threshold\"\n    ATTRIBUTES_KEY = \"attributes\"\n    TRANSFORM_KEY = \"transforms\"\n\n    # Dataset generation config keys (config[\"dataset_generation\"][&lt;key&gt;])\n    DATASET_GENERATION_GUIDELINES_KEY = \"guidelines\"\n    DATASET_GENERATION_NUM_ROWS_KEY = \"num_rows\"\n\n    # Chunking config keys (config[\"chunking\"][&lt;key&gt;])\n    CONFIDENCE_CHUNK_COLUMN_KEY = \"confidence_chunk_column\"\n    CONFIDENCE_CHUNK_SIZE_KEY = \"confidence_chunk_size\"\n    CONFIDENCE_MERGE_FUNCTION_KEY = \"confidence_merge_function\"\n\n    def __init__(self, config: Union[str, Dict], validate: bool = True) -&gt; None:\n        super().__init__(config, validate=validate)\n\n    def _validate(self) -&gt; bool:\n        \"\"\"Returns true if the config settings are valid\"\"\"\n        from autolabel.configs.schema import schema\n\n        validate(\n            instance=self.config,\n            schema=schema,\n        )\n        return True\n\n    @cached_property\n    def _dataset_config(self) -&gt; Dict:\n        \"\"\"Returns information about the dataset being used for labeling (e.g. label_column, text_column, delimiter)\"\"\"\n        return self.config.get(self.DATASET_CONFIG_KEY, {})\n\n    @cached_property\n    def _model_config(self) -&gt; Dict:\n        \"\"\"Returns information about the model being used for labeling (e.g. provider name, model name, parameters)\"\"\"\n        return self.config[self.MODEL_CONFIG_KEY]\n\n    @cached_property\n    def _embedding_config(self) -&gt; Dict:\n        \"\"\"Returns information about the model being used for computing embeddings (e.g. provider name, model name)\"\"\"\n        return self.config.get(self.EMBEDDING_CONFIG_KEY, {})\n\n    @cached_property\n    def _prompt_config(self) -&gt; Dict:\n        \"\"\"Returns information about the prompt we are passing to the model (e.g. task guidelines, examples, output formatting)\"\"\"\n        return self.config[self.PROMPT_CONFIG_KEY]\n\n    @cached_property\n    def _dataset_generation_config(self) -&gt; Dict:\n        \"\"\"Returns information about the prompt for synthetic dataset generation\"\"\"\n        return self.config.get(self.DATASET_GENERATION_CONFIG_KEY, {})\n\n    @cached_property\n    def _chunking_config(self) -&gt; Dict:\n        \"\"\"Returns information about the chunking config\"\"\"\n        return self.config.get(self.CHUNKING_CONFIG_KEY, {})\n\n    # project and task definition config\n    def task_name(self) -&gt; str:\n        return self.config[self.TASK_NAME_KEY]\n\n    def task_type(self) -&gt; str:\n        \"\"\"Returns the type of task we have configured the labeler to perform (e.g. Classification, Question Answering)\"\"\"\n        return self.config[self.TASK_TYPE_KEY]\n\n    # Dataset config\n    def label_column(self) -&gt; str:\n        \"\"\"Returns the name of the column containing labels for the dataset. Used for comparing accuracy of autolabel results vs ground truth\"\"\"\n        return self._dataset_config.get(self.LABEL_COLUMN_KEY, None)\n\n    def label_separator(self) -&gt; str:\n        \"\"\"Returns the token used to seperate multiple labels in the dataset. Defaults to a semicolon ';'\"\"\"\n        return self._dataset_config.get(self.LABEL_SEPARATOR_KEY, \";\")\n\n    def text_column(self) -&gt; str:\n        \"\"\"Returns the name of the column containing text data we intend to label\"\"\"\n        return self._dataset_config.get(self.TEXT_COLUMN_KEY, None)\n\n    def input_columns(self) -&gt; List[str]:\n        \"\"\"Returns the names of the input columns from the dataset that are used in the prompt\"\"\"\n        return self._dataset_config.get(self.INPUT_COLUMNS_KEY, [])\n\n    def explanation_column(self) -&gt; str:\n        \"\"\"Returns the name of the column containing an explanation as to why the data is labeled a certain way\"\"\"\n        return self._dataset_config.get(self.EXPLANATION_COLUMN_KEY, None)\n\n    def image_column(self) -&gt; str:\n        \"\"\"Returns the name of the column containing an image url for the given item\"\"\"\n        return self._dataset_config.get(self.IMAGE_COLUMN_KEY, None)\n\n    def delimiter(self) -&gt; str:\n        \"\"\"Returns the token used to seperate cells in the dataset. Defaults to a comma ','\"\"\"\n        return self._dataset_config.get(self.DELIMITER_KEY, \",\")\n\n    def disable_quoting(self) -&gt; bool:\n        \"\"\"Returns true if quoting is disabled. Defaults to false\"\"\"\n        return self._dataset_config.get(self.DISABLE_QUOTING, False)\n\n    # Model config\n    def provider(self) -&gt; str:\n        \"\"\"Returns the name of the entity that provides the currently configured model (e.g. OpenAI, Anthropic, Refuel)\"\"\"\n        return self._model_config[self.PROVIDER_KEY]\n\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the name of the model being used for labeling (e.g. gpt-4, claude-v1)\"\"\"\n        return self._model_config[self.MODEL_NAME_KEY]\n\n    def model_params(self) -&gt; Dict:\n        \"\"\"Returns a dict of configured settings for the model (e.g. hyperparameters)\"\"\"\n        return self._model_config.get(self.MODEL_PARAMS_KEY, {})\n\n    def confidence(self) -&gt; bool:\n        \"\"\"Returns true if the model is able to return a confidence score along with its predictions\"\"\"\n        return self._model_config.get(self.COMPUTE_CONFIDENCE_KEY, False)\n\n    def logit_bias(self) -&gt; float:\n        \"\"\"Returns the logit bias for the labels specified in the config\"\"\"\n        return self._model_config.get(self.LOGIT_BIAS_KEY, 0.0)\n\n    # Embedding config\n    def embedding_provider(self) -&gt; str:\n        \"\"\"Returns the name of the entity that provides the model used for computing embeddings\"\"\"\n        return self._embedding_config.get(self.EMBEDDING_PROVIDER_KEY, self.provider())\n\n    def embedding_model_name(self) -&gt; str:\n        \"\"\"Returns the name of the model being used for computing embeddings (e.g. sentence-transformers/all-mpnet-base-v2)\"\"\"\n        return self._embedding_config.get(self.EMBEDDING_MODEL_NAME_KEY, None)\n\n    # Prompt config\n    def task_guidelines(self) -&gt; str:\n        return self._prompt_config.get(self.TASK_GUIDELINE_KEY, \"\")\n\n    def labels_list(self) -&gt; List[str]:\n        \"\"\"Returns a list of valid labels\"\"\"\n        if isinstance(self._prompt_config.get(self.VALID_LABELS_KEY, []), List):\n            return self._prompt_config.get(self.VALID_LABELS_KEY, [])\n        else:\n            return list(self._prompt_config.get(self.VALID_LABELS_KEY, {}).keys())\n\n    def label_descriptions(self) -&gt; Dict[str, str]:\n        \"\"\"Returns a dict of label descriptions\"\"\"\n        if isinstance(self._prompt_config.get(self.VALID_LABELS_KEY, []), List):\n            return {}\n        else:\n            return self._prompt_config.get(self.VALID_LABELS_KEY, {})\n\n    def few_shot_example_set(self) -&gt; Union[str, List]:\n        \"\"\"Returns examples of how data should be labeled, used to guide context to the model about the task it is performing\"\"\"\n        return self._prompt_config.get(self.FEW_SHOT_EXAMPLE_SET_KEY, [])\n\n    def few_shot_algorithm(self) -&gt; str:\n        \"\"\"Returns which algorithm is being used to construct the set of examples being given to the model about the labeling task\"\"\"\n        return self._prompt_config.get(self.FEW_SHOT_SELECTION_ALGORITHM_KEY, None)\n\n    def few_shot_num_examples(self) -&gt; int:\n        \"\"\"Returns how many examples should be given to the model in its instruction prompt\"\"\"\n        return self._prompt_config.get(self.FEW_SHOT_NUM_KEY, 0)\n\n    def vector_store_params(self) -&gt; Dict:\n        \"\"\"Returns any parameters to be passed to the vector store\"\"\"\n        return self._prompt_config.get(self.VECTOR_STORE_PARAMS_KEY, {})\n\n    def example_template(self) -&gt; str:\n        \"\"\"Returns a string containing a template for how examples will be formatted in the prompt\"\"\"\n        example_template = self._prompt_config.get(self.EXAMPLE_TEMPLATE_KEY, None)\n        if not example_template:\n            raise ValueError(\"An example template needs to be specified in the config.\")\n        return example_template\n\n    def output_format(self) -&gt; str:\n        return self._prompt_config.get(self.OUTPUT_FORMAT_KEY, None)\n\n    def output_guidelines(self) -&gt; str:\n        return self._prompt_config.get(self.OUTPUT_GUIDELINE_KEY, None)\n\n    def chain_of_thought(self) -&gt; bool:\n        \"\"\"Returns true if the model is able to perform chain of thought reasoning.\"\"\"\n        return self._prompt_config.get(self.CHAIN_OF_THOUGHT_KEY, False)\n\n    def label_selection(self) -&gt; bool:\n        \"\"\"Returns true if label selection is enabled. Label selection is the process of\n        narrowing down the list of possible labels by similarity to a given input. Useful for\n        classification tasks with a large number of possible classes.\"\"\"\n        return self._prompt_config.get(self.LABEL_SELECTION_KEY, False)\n\n    def max_selected_labels(self) -&gt; int:\n        \"\"\"Returns the number of labels to select in LabelSelector\"\"\"\n        k = self._prompt_config.get(self.LABEL_SELECTION_COUNT_KEY, 10)\n        if k &lt; 1:\n            return len(self.labels_list())\n        return k\n\n    def label_selection_threshold(self) -&gt; float:\n        \"\"\"Returns the threshold for label selection in LabelSelector\n        If the similarity score ratio with the top Score is above this threshold,\n        the label is selected.\"\"\"\n        return self._prompt_config.get(self.LABEL_SELECTION_THRESHOLD, 0.0)\n\n    def attributes(self) -&gt; List[Dict]:\n        \"\"\"Returns a list of attributes to extract from the text.\"\"\"\n        return self._prompt_config.get(self.ATTRIBUTES_KEY, [])\n\n    def transforms(self) -&gt; List[Dict]:\n        \"\"\"Returns a list of transforms to apply to the data before sending to the model.\"\"\"\n        return self.config.get(self.TRANSFORM_KEY, [])\n\n    def dataset_generation_guidelines(self) -&gt; str:\n        \"\"\"Returns a string containing guidelines for how to generate a synthetic dataset\"\"\"\n        return self._dataset_generation_config.get(\n            self.DATASET_GENERATION_GUIDELINES_KEY, \"\"\n        )\n\n    def dataset_generation_num_rows(self) -&gt; int:\n        \"\"\"Returns the number of rows to generate for the synthetic dataset\"\"\"\n        return self._dataset_generation_config.get(\n            self.DATASET_GENERATION_NUM_ROWS_KEY, 1\n        )\n\n    def confidence_chunk_column(self) -&gt; str:\n        \"\"\"Returns the column name to use for confidence chunking\"\"\"\n        return self._chunking_config.get(self.CONFIDENCE_CHUNK_COLUMN_KEY)\n\n    def confidence_chunk_size(self) -&gt; int:\n        \"\"\"Returns the chunk size for confidence chunking\"\"\"\n        return self._chunking_config.get(self.CONFIDENCE_CHUNK_SIZE_KEY, 3400)\n\n    def confidence_merge_function(self) -&gt; str:\n        \"\"\"Returns the function to use when merging confidence scores\"\"\"\n        return self._chunking_config.get(self.CONFIDENCE_MERGE_FUNCTION_KEY, \"max\")\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.attributes","title":"<code>attributes()</code>","text":"<p>Returns a list of attributes to extract from the text.</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def attributes(self) -&gt; List[Dict]:\n    \"\"\"Returns a list of attributes to extract from the text.\"\"\"\n    return self._prompt_config.get(self.ATTRIBUTES_KEY, [])\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.chain_of_thought","title":"<code>chain_of_thought()</code>","text":"<p>Returns true if the model is able to perform chain of thought reasoning.</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def chain_of_thought(self) -&gt; bool:\n    \"\"\"Returns true if the model is able to perform chain of thought reasoning.\"\"\"\n    return self._prompt_config.get(self.CHAIN_OF_THOUGHT_KEY, False)\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.confidence","title":"<code>confidence()</code>","text":"<p>Returns true if the model is able to return a confidence score along with its predictions</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def confidence(self) -&gt; bool:\n    \"\"\"Returns true if the model is able to return a confidence score along with its predictions\"\"\"\n    return self._model_config.get(self.COMPUTE_CONFIDENCE_KEY, False)\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.confidence_chunk_column","title":"<code>confidence_chunk_column()</code>","text":"<p>Returns the column name to use for confidence chunking</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def confidence_chunk_column(self) -&gt; str:\n    \"\"\"Returns the column name to use for confidence chunking\"\"\"\n    return self._chunking_config.get(self.CONFIDENCE_CHUNK_COLUMN_KEY)\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.confidence_chunk_size","title":"<code>confidence_chunk_size()</code>","text":"<p>Returns the chunk size for confidence chunking</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def confidence_chunk_size(self) -&gt; int:\n    \"\"\"Returns the chunk size for confidence chunking\"\"\"\n    return self._chunking_config.get(self.CONFIDENCE_CHUNK_SIZE_KEY, 3400)\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.confidence_merge_function","title":"<code>confidence_merge_function()</code>","text":"<p>Returns the function to use when merging confidence scores</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def confidence_merge_function(self) -&gt; str:\n    \"\"\"Returns the function to use when merging confidence scores\"\"\"\n    return self._chunking_config.get(self.CONFIDENCE_MERGE_FUNCTION_KEY, \"max\")\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.dataset_generation_guidelines","title":"<code>dataset_generation_guidelines()</code>","text":"<p>Returns a string containing guidelines for how to generate a synthetic dataset</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def dataset_generation_guidelines(self) -&gt; str:\n    \"\"\"Returns a string containing guidelines for how to generate a synthetic dataset\"\"\"\n    return self._dataset_generation_config.get(\n        self.DATASET_GENERATION_GUIDELINES_KEY, \"\"\n    )\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.dataset_generation_num_rows","title":"<code>dataset_generation_num_rows()</code>","text":"<p>Returns the number of rows to generate for the synthetic dataset</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def dataset_generation_num_rows(self) -&gt; int:\n    \"\"\"Returns the number of rows to generate for the synthetic dataset\"\"\"\n    return self._dataset_generation_config.get(\n        self.DATASET_GENERATION_NUM_ROWS_KEY, 1\n    )\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.delimiter","title":"<code>delimiter()</code>","text":"<p>Returns the token used to seperate cells in the dataset. Defaults to a comma ','</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def delimiter(self) -&gt; str:\n    \"\"\"Returns the token used to seperate cells in the dataset. Defaults to a comma ','\"\"\"\n    return self._dataset_config.get(self.DELIMITER_KEY, \",\")\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.disable_quoting","title":"<code>disable_quoting()</code>","text":"<p>Returns true if quoting is disabled. Defaults to false</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def disable_quoting(self) -&gt; bool:\n    \"\"\"Returns true if quoting is disabled. Defaults to false\"\"\"\n    return self._dataset_config.get(self.DISABLE_QUOTING, False)\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.embedding_model_name","title":"<code>embedding_model_name()</code>","text":"<p>Returns the name of the model being used for computing embeddings (e.g. sentence-transformers/all-mpnet-base-v2)</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def embedding_model_name(self) -&gt; str:\n    \"\"\"Returns the name of the model being used for computing embeddings (e.g. sentence-transformers/all-mpnet-base-v2)\"\"\"\n    return self._embedding_config.get(self.EMBEDDING_MODEL_NAME_KEY, None)\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.embedding_provider","title":"<code>embedding_provider()</code>","text":"<p>Returns the name of the entity that provides the model used for computing embeddings</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def embedding_provider(self) -&gt; str:\n    \"\"\"Returns the name of the entity that provides the model used for computing embeddings\"\"\"\n    return self._embedding_config.get(self.EMBEDDING_PROVIDER_KEY, self.provider())\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.example_template","title":"<code>example_template()</code>","text":"<p>Returns a string containing a template for how examples will be formatted in the prompt</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def example_template(self) -&gt; str:\n    \"\"\"Returns a string containing a template for how examples will be formatted in the prompt\"\"\"\n    example_template = self._prompt_config.get(self.EXAMPLE_TEMPLATE_KEY, None)\n    if not example_template:\n        raise ValueError(\"An example template needs to be specified in the config.\")\n    return example_template\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.explanation_column","title":"<code>explanation_column()</code>","text":"<p>Returns the name of the column containing an explanation as to why the data is labeled a certain way</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def explanation_column(self) -&gt; str:\n    \"\"\"Returns the name of the column containing an explanation as to why the data is labeled a certain way\"\"\"\n    return self._dataset_config.get(self.EXPLANATION_COLUMN_KEY, None)\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.few_shot_algorithm","title":"<code>few_shot_algorithm()</code>","text":"<p>Returns which algorithm is being used to construct the set of examples being given to the model about the labeling task</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def few_shot_algorithm(self) -&gt; str:\n    \"\"\"Returns which algorithm is being used to construct the set of examples being given to the model about the labeling task\"\"\"\n    return self._prompt_config.get(self.FEW_SHOT_SELECTION_ALGORITHM_KEY, None)\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.few_shot_example_set","title":"<code>few_shot_example_set()</code>","text":"<p>Returns examples of how data should be labeled, used to guide context to the model about the task it is performing</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def few_shot_example_set(self) -&gt; Union[str, List]:\n    \"\"\"Returns examples of how data should be labeled, used to guide context to the model about the task it is performing\"\"\"\n    return self._prompt_config.get(self.FEW_SHOT_EXAMPLE_SET_KEY, [])\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.few_shot_num_examples","title":"<code>few_shot_num_examples()</code>","text":"<p>Returns how many examples should be given to the model in its instruction prompt</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def few_shot_num_examples(self) -&gt; int:\n    \"\"\"Returns how many examples should be given to the model in its instruction prompt\"\"\"\n    return self._prompt_config.get(self.FEW_SHOT_NUM_KEY, 0)\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.image_column","title":"<code>image_column()</code>","text":"<p>Returns the name of the column containing an image url for the given item</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def image_column(self) -&gt; str:\n    \"\"\"Returns the name of the column containing an image url for the given item\"\"\"\n    return self._dataset_config.get(self.IMAGE_COLUMN_KEY, None)\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.input_columns","title":"<code>input_columns()</code>","text":"<p>Returns the names of the input columns from the dataset that are used in the prompt</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def input_columns(self) -&gt; List[str]:\n    \"\"\"Returns the names of the input columns from the dataset that are used in the prompt\"\"\"\n    return self._dataset_config.get(self.INPUT_COLUMNS_KEY, [])\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.label_column","title":"<code>label_column()</code>","text":"<p>Returns the name of the column containing labels for the dataset. Used for comparing accuracy of autolabel results vs ground truth</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def label_column(self) -&gt; str:\n    \"\"\"Returns the name of the column containing labels for the dataset. Used for comparing accuracy of autolabel results vs ground truth\"\"\"\n    return self._dataset_config.get(self.LABEL_COLUMN_KEY, None)\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.label_descriptions","title":"<code>label_descriptions()</code>","text":"<p>Returns a dict of label descriptions</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def label_descriptions(self) -&gt; Dict[str, str]:\n    \"\"\"Returns a dict of label descriptions\"\"\"\n    if isinstance(self._prompt_config.get(self.VALID_LABELS_KEY, []), List):\n        return {}\n    else:\n        return self._prompt_config.get(self.VALID_LABELS_KEY, {})\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.label_selection","title":"<code>label_selection()</code>","text":"<p>Returns true if label selection is enabled. Label selection is the process of narrowing down the list of possible labels by similarity to a given input. Useful for classification tasks with a large number of possible classes.</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def label_selection(self) -&gt; bool:\n    \"\"\"Returns true if label selection is enabled. Label selection is the process of\n    narrowing down the list of possible labels by similarity to a given input. Useful for\n    classification tasks with a large number of possible classes.\"\"\"\n    return self._prompt_config.get(self.LABEL_SELECTION_KEY, False)\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.label_selection_threshold","title":"<code>label_selection_threshold()</code>","text":"<p>Returns the threshold for label selection in LabelSelector If the similarity score ratio with the top Score is above this threshold, the label is selected.</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def label_selection_threshold(self) -&gt; float:\n    \"\"\"Returns the threshold for label selection in LabelSelector\n    If the similarity score ratio with the top Score is above this threshold,\n    the label is selected.\"\"\"\n    return self._prompt_config.get(self.LABEL_SELECTION_THRESHOLD, 0.0)\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.label_separator","title":"<code>label_separator()</code>","text":"<p>Returns the token used to seperate multiple labels in the dataset. Defaults to a semicolon ';'</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def label_separator(self) -&gt; str:\n    \"\"\"Returns the token used to seperate multiple labels in the dataset. Defaults to a semicolon ';'\"\"\"\n    return self._dataset_config.get(self.LABEL_SEPARATOR_KEY, \";\")\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.labels_list","title":"<code>labels_list()</code>","text":"<p>Returns a list of valid labels</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def labels_list(self) -&gt; List[str]:\n    \"\"\"Returns a list of valid labels\"\"\"\n    if isinstance(self._prompt_config.get(self.VALID_LABELS_KEY, []), List):\n        return self._prompt_config.get(self.VALID_LABELS_KEY, [])\n    else:\n        return list(self._prompt_config.get(self.VALID_LABELS_KEY, {}).keys())\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.logit_bias","title":"<code>logit_bias()</code>","text":"<p>Returns the logit bias for the labels specified in the config</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def logit_bias(self) -&gt; float:\n    \"\"\"Returns the logit bias for the labels specified in the config\"\"\"\n    return self._model_config.get(self.LOGIT_BIAS_KEY, 0.0)\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.max_selected_labels","title":"<code>max_selected_labels()</code>","text":"<p>Returns the number of labels to select in LabelSelector</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def max_selected_labels(self) -&gt; int:\n    \"\"\"Returns the number of labels to select in LabelSelector\"\"\"\n    k = self._prompt_config.get(self.LABEL_SELECTION_COUNT_KEY, 10)\n    if k &lt; 1:\n        return len(self.labels_list())\n    return k\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.model_name","title":"<code>model_name()</code>","text":"<p>Returns the name of the model being used for labeling (e.g. gpt-4, claude-v1)</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def model_name(self) -&gt; str:\n    \"\"\"Returns the name of the model being used for labeling (e.g. gpt-4, claude-v1)\"\"\"\n    return self._model_config[self.MODEL_NAME_KEY]\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.model_params","title":"<code>model_params()</code>","text":"<p>Returns a dict of configured settings for the model (e.g. hyperparameters)</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def model_params(self) -&gt; Dict:\n    \"\"\"Returns a dict of configured settings for the model (e.g. hyperparameters)\"\"\"\n    return self._model_config.get(self.MODEL_PARAMS_KEY, {})\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.provider","title":"<code>provider()</code>","text":"<p>Returns the name of the entity that provides the currently configured model (e.g. OpenAI, Anthropic, Refuel)</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def provider(self) -&gt; str:\n    \"\"\"Returns the name of the entity that provides the currently configured model (e.g. OpenAI, Anthropic, Refuel)\"\"\"\n    return self._model_config[self.PROVIDER_KEY]\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.task_type","title":"<code>task_type()</code>","text":"<p>Returns the type of task we have configured the labeler to perform (e.g. Classification, Question Answering)</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def task_type(self) -&gt; str:\n    \"\"\"Returns the type of task we have configured the labeler to perform (e.g. Classification, Question Answering)\"\"\"\n    return self.config[self.TASK_TYPE_KEY]\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.text_column","title":"<code>text_column()</code>","text":"<p>Returns the name of the column containing text data we intend to label</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def text_column(self) -&gt; str:\n    \"\"\"Returns the name of the column containing text data we intend to label\"\"\"\n    return self._dataset_config.get(self.TEXT_COLUMN_KEY, None)\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.transforms","title":"<code>transforms()</code>","text":"<p>Returns a list of transforms to apply to the data before sending to the model.</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def transforms(self) -&gt; List[Dict]:\n    \"\"\"Returns a list of transforms to apply to the data before sending to the model.\"\"\"\n    return self.config.get(self.TRANSFORM_KEY, [])\n</code></pre>"},{"location":"concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.vector_store_params","title":"<code>vector_store_params()</code>","text":"<p>Returns any parameters to be passed to the vector store</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def vector_store_params(self) -&gt; Dict:\n    \"\"\"Returns any parameters to be passed to the vector store\"\"\"\n    return self._prompt_config.get(self.VECTOR_STORE_PARAMS_KEY, {})\n</code></pre>"},{"location":"concepts/concepts/#tasks","title":"Tasks","text":""},{"location":"concepts/concepts/#classification","title":"Classification","text":""},{"location":"concepts/concepts/#question-answering","title":"Question Answering","text":""},{"location":"concepts/concepts/#entity-matching","title":"Entity matching","text":""},{"location":"concepts/concepts/#named-entity-recognition","title":"Named Entity Recognition","text":""},{"location":"guide/accuracy/chain-of-thought/","title":"Chain of Thought","text":"Chain of Thought Prompting (Wei et al) <p>LLMs find it hard to perform well on complex reasoning tasks. We can unlock the reasoning abilities of LLMs using chain of thought prompting. This involves asking the LLM to produce the reasoning before producing the answer (roughly analogous to \"show me your work\").</p> <p>Chain of thought makes LLMs more effective at reasoning tasks like mathematical word problems, commonsense reasoning questions and complex medical questions. It also provides a window into the thought process of the LLM, though some research points the link between the generated explanation and the final answer may be weak.</p>"},{"location":"guide/accuracy/chain-of-thought/#using-chain-of-thought-in-autolabel","title":"Using Chain Of Thought in Autolabel","text":"<p>Enabling chain-of-thought prompting for your task is straightforward with Autolabel. It works best when provided with a few seed examples with explanations. Thus enabling chain of thought requires a few things:</p> <ol> <li>Setting <code>chain_of_thought</code> flag in the labeling config.</li> <li>Providing explanations or generating explanations for your seed examples automatically by using an LLM</li> <li>Setting the <code>explanation_column</code> in the labeling config.</li> <li>Altering the task guidelines and <code>example_template</code> to tell the model to generate an explanation before generating the final answer.</li> </ol> <p>We will go through using chain of thought on a dataset where it shows improvement, like the SQuAD question answering dataset.</p> <p>Let's see a datapoint before there is any explanation added to it.</p> context question answer Private schools generally prefer to be called independent schools, because of their freedom to operate outside of government and local government control. Some of these are also known as public schools. Preparatory schools in the UK prepare pupils aged up to 13 years old to enter public schools. The name 'public school' is based on the fact that the schools were open to pupils from anywhere, and not merely to those from a certain locality, and of any religion or occupation. According to The Good Schools Guide approximately 9 per cent of children being educated in the UK are doing so at fee-paying schools at GSCE level and 13 per cent at A-level.[citation needed] Many independent schools are single-sex (though this is becoming less common). Fees range from under \u00a33,000 to \u00a321,000 and above per year for day pupils, rising to \u00a327,000+ per year for boarders. For details in Scotland, see 'Meeting the Cost'. At A-level, what percentage of British students attend fee-paying schools? 13 <p>Now we can manually write the explanation for this or a couple of seed examples easily. But this will be tiresome for &gt; 10 examples. LLMs come to the rescue yet again! We can just define the config and ask the agent to generate explanations as well!</p> <pre><code>config = {\n    \"task_name\": \"OpenbookQAWikipedia\",\n    \"task_type\": \"question_answering\",\n    \"dataset\": {\n        \"label_column\": \"answer\",\n        \"explanation_column\": \"explanation\",\n        \"delimiter\": \",\"\n    },\n    \"model\": {\n        \"provider\": \"openai\",\n        \"name\": \"gpt-3.5-turbo\",\n    },\n    \"prompt\": {\n        \"task_guidelines\": \"You are an expert at answering questions based on wikipedia articles. Your job is to answer the following questions using the context provided with the question. Use the context to answer the question - the answer is a continuous span of words from the context.\\n\",\n        \"output_guidelines\": \"Your answer will consist of an explanation, followed by the correct answer. The last line of the response should always be is JSON format with one key: {\\\"label\\\": \\\"the correct answer\\\"}.\\n If the question cannot be answered using the context and the context alone without any outside knowledge, the question is unanswerable. If the question is unanswerable, return the answer as {\\\"label\\\": \\\"unanswerable\\\"}\\n\",\n        \"few_shot_examples\": \"seed.csv\",\n        \"few_shot_selection\": \"semantic_similarity\",\n        \"few_shot_num\": 3,\n        \"example_template\": \"Context: {context}\\nQuestion: {question}\\nAnswer: Let's think step by step.\\n{explanation}\\n{answer}\",\n        \"chain_of_thought\": True\n    }\n}\n</code></pre> <p>Notice the changes that we have made to the config compared to the config without Chain-of-Thought here:</p> <ul> <li><code>chain_of_thought</code> flag - this tells labeling agent to expect an explanation for the answer, in the seed dataset as well as LLM generated responses.</li> <li><code>explanation_column</code> - this is the column where the explanation for the seed examples will reside.</li> <li><code>example_template</code> - Notice that the template contains contains the explanation column as well. This tells the config where the explanation should be put when using the seed examples. We use the <code>Let's think step by step</code> prompt to initiate the chain of thought in the model.</li> <li><code>output_guidelines</code> - We are explicitly prompting the LLM to first output an explanation, and then the final answer.</li> </ul> <p>Now, in order to generate explanations for the seed examples, in case they were not manually generated is,</p> <pre><code>from autolabel import LabelingAgent\nagent = LabelingAgent(config)\nagent.generate_explanations(\"path_to_seed_examples.csv\")\n</code></pre> <p>Once these explanations are generated, the dataset looks like</p> context question answer explanation Private schools generally prefer to be called independent schools, because of their freedom to operate outside of government and local government control. Some of these are also known as public schools. Preparatory schools in the UK prepare pupils aged up to 13 years old to enter public schools. The name 'public school' is based on the fact that the schools were open to pupils from anywhere, and not merely to those from a certain locality, and of any religion or occupation. According to The Good Schools Guide approximately 9 per cent of children being educated in the UK are doing so at fee-paying schools at GSCE level and 13 per cent at A-level.[citation needed] Many independent schools are single-sex (though this is becoming less common). Fees range from under \u00a33,000 to \u00a321,000 and above per year for day pupils, rising to \u00a327,000+ per year for boarders. For details in Scotland, see 'Meeting the Cost'. At A-level, what percentage of British students attend fee-paying schools? 13 Independent schools in the UK are private schools that charge fees.  These schools are also known as public schools. According to The Good Schools Guide, about 9% of children in the UK attend fee-paying schools at the GSCE level. At the A-level, which is a higher level of education, a higher percentage of students, 13%, attend fee-paying independent schools. Since 13% of students attend fee-paying schools at the A-level, and the question asks what percentage attend at the A-level specifically, So, the answer is 13. <p>Now to generate labels for this dataset, all we have to do is,</p> <pre><code>from autolabel import AutolabelDataset\nds = AutolabelDatset('data/squad_v2_test.csv', config = config)\nagent.plan(ds)\nagent.run(ds, max_items = 100)\n</code></pre> <p>Autolabel currently supports Chain-of-thought prompting for the following tasks:</p> <ol> <li>Classifcation (example)</li> <li>Entity Match</li> <li>Question Answering (example)</li> </ol> <p>Support for other tasks coming soon!</p>"},{"location":"guide/accuracy/confidence/","title":"Confidence","text":"ChatGPT summarizing a non-existent New York Times article even without access to the Internet <p>One of the biggest criticisms of using a LLMs so far has been hallucinations - LLMs can seem very confidence in their language even when they are completely incorrect. <code>autolabel</code> provides a confidence score for each LLM output that is correlated with the likelihood of that output being incorrect, i.e. if the confidence score is high, then it is more likely that the output is correct, and if confidence score is low, it is likely that the LLM has produced an incorrect output.</p>"},{"location":"guide/accuracy/confidence/#computing-confidence-scores","title":"Computing Confidence Scores","text":"<p>The <code>autolabel</code> library today relies on token level probabilities, also known as logprobs, to compute confidence scores. However, very few models today return token level probabilities alongside prediction. Out of all models supported by <code>autolabel</code> today, only the <code>text-davinci-003</code> model by <code>openai</code> can return logprobs. For all other models, Refuel has setup an in-house API to generate logprobs for a specific prediction given an input, regardless of the language model that was originally used to query for the prediction. For <code>text-davinci-003</code>, we use the logprobs returned by <code>openai</code>'s API instead of querying our in-house API.</p> <p>Generating confidence scores is simple - setting the key <code>compute_confidence</code> to <code>True</code> in the <code>model</code> dictionary of the config should initiate confidence score retrieval. Here is an example:</p> <pre><code>{\n    \"task_name\": \"PersonLocationOrgMiscNER\",\n    \"task_type\": \"named_entity_recognition\",\n    \"dataset\": {\n        \"label_column\": \"CategorizedLabels\",\n        \"text_column\": \"example\",\n        \"delimiter\": \"%\"\n    },\n    \"model\": {\n        \"provider\": \"anthropic\",\n        \"name\": \"claude-v1\",\n        \"compute_confidence\": True\n    },\n    \"prompt\": {\n        \"task_guidelines\": \"You are an expert at extracting entities from text.\",\n        \"labels\": [\n            \"Location\",\n            \"Organization\",\n            \"Person\",\n            \"Miscellaneous\"\n        ],\n        \"example_template\": \"Example: {example}\\nOutput: {CategorizedLabels}\",\n        \"few_shot_examples\": \"../examples/conll2003/seed.csv\",\n        \"few_shot_selection\": \"semantic_similarity\",\n        \"few_shot_num\": 5\n    }\n}\n</code></pre> <p>In the above example, by setting <code>compute_confidence</code> to True, <code>autolabel</code> will start calling Refuel's api to generate token level probabilities and compute confidence scores for each prediction. In order for this to run successfully, ensure that the following setup has been completed:</p> <p>Set the following environment variable: <pre><code>export REFUEL_API_KEY=&lt;your-refuel-key&gt;\n</code></pre> replacing <code>&lt;your-refuel-key&gt;</code> with your API key, which you can get from here</p>"},{"location":"guide/accuracy/confidence/#interpreting-scores","title":"Interpreting Scores","text":"<p>To see how confidence scores can be used to make a tradeoff between task performance and completion rate, let's take a look at the following example:</p> <p> </p> Library output when confidence is enabled <p><code>autolabel</code> outputs a table consisting of metrics at various confidence thresholds when <code>compute_confidence</code> is set to <code>True</code>. Specifically, this is the table we get when we label 100 examples from the CONLL-2003 dataset with semantic similarity enabled. The first row in the table corresponds to the overall performance: we were able to successfully label 98% of examples at an F1 score of 0.885. However, we can use this table to decide on a confidence threshold to accept predictions at and increase our metrics. For example, note that according the highlighed row, if we accept labels with confidence scores above ~2.207, we can boost our F1 score to 0.95 while reducing completion rate to 79%. </p>"},{"location":"guide/accuracy/few-shot/","title":"Few-shot Prompting","text":"<p>It has been shown that the specific seed examples used while constructing the prompt have an impact on the performance of the model. Seed examples are the labeled examples which are shown as demonstration to the LLM to help it understand the task better. Optimally selecting the seed examples can help boost performance and save on labeling costs by reducing the context size.</p> <p>We support the following few-shot example selection techniques:</p> <ol> <li>Fixed - The same set of seed examples are used for every input data point.</li> <li>Semantic_similarity - Embeddings are computed for all the examples in the seed set and a vector similarity search finds the few shot examples which are closest to the input datapoint. Closer datapoints from the seed set can give the model more context on how similar examples have been labeled, helping it improve performance.</li> <li>Max_marginal_relevance - Semantic similarity search is used to retrieve a set of candidate examples. Then, a diversity-driven selection strategy is used amongst these candidates to select a final subset of examples that have the most coverage of the initial pool of candidate examples.</li> <li>Label diversity - This strategy focuses on ensuring that the few-shot examples selected provide coverage across all the valid output labels.</li> <li>Label diversity with similarity - This strategy is a combination of (2) and (4) above - it samples a fixed number of examples per valid label, and within each label it selects the examples that are most similar to the input.</li> </ol> <p>Example:</p> <p></p> <p>Consider the following labeling runs for a classification task on the banking dataset. There are a total of 1998 items to be labeled and we assume a starting labeled seedset of 200 examples. Here is the config to label this dataset in zero-shot fashion:</p> <pre><code>config_zero_shot = {\n    \"task_name\": \"BankingComplaintsClassification\",\n    \"task_type\": \"classification\",\n    \"dataset\": {\n        \"label_column\": \"label\",\n        \"delimiter\": \",\"\n    },\n    \"model\": {\n        \"provider\": \"openai\",\n        \"name\": \"gpt-3.5-turbo\"\n    },\n    \"prompt\": {\n        \"task_guidelines\": \"You are an expert at understanding bank customers support complaints and queries.\\nYour job is to correctly classify the provided input example into one of the following categories.\\nCategories:\\n{labels}\",\n        \"output_guidelines\": \"You will answer with just the the correct output label and nothing else.\",\n        \"labels\": [\n            \"activate_my_card\",\n            \"age_limit\",\n            \"apple_pay_or_google_pay\",\n            \"atm_support\",\n            \"automatic_top_up\",\n            \"balance_not_updated_after_bank_transfer\",\n            \"balance_not_updated_after_cheque_or_cash_deposit\",\n            \"beneficiary_not_allowed\",\n            \"cancel_transfer\",\n            \"card_about_to_expire\",\n            \"card_acceptance\",\n            \"card_arrival\",\n            \"card_delivery_estimate\",\n            \"card_linking\",\n            \"card_not_working\",\n            \"card_payment_fee_charged\",\n            \"card_payment_not_recognised\",\n            \"card_payment_wrong_exchange_rate\",\n            \"card_swallowed\",\n            \"cash_withdrawal_charge\",\n            \"cash_withdrawal_not_recognised\",\n            \"change_pin\",\n            \"compromised_card\",\n            \"contactless_not_working\",\n            \"country_support\",\n            \"declined_card_payment\",\n            \"declined_cash_withdrawal\",\n            \"declined_transfer\",\n            \"direct_debit_payment_not_recognised\",\n            \"disposable_card_limits\",\n            \"edit_personal_details\",\n            \"exchange_charge\",\n            \"exchange_rate\",\n            \"exchange_via_app\",\n            \"extra_charge_on_statement\",\n            \"failed_transfer\",\n            \"fiat_currency_support\",\n            \"get_disposable_virtual_card\",\n            \"get_physical_card\",\n            \"getting_spare_card\",\n            \"getting_virtual_card\",\n            \"lost_or_stolen_card\",\n            \"lost_or_stolen_phone\",\n            \"order_physical_card\",\n            \"passcode_forgotten\",\n            \"pending_card_payment\",\n            \"pending_cash_withdrawal\",\n            \"pending_top_up\",\n            \"pending_transfer\",\n            \"pin_blocked\",\n            \"receiving_money\",\n            \"Refund_not_showing_up\",\n            \"request_refund\",\n            \"reverted_card_payment?\",\n            \"supported_cards_and_currencies\",\n            \"terminate_account\",\n            \"top_up_by_bank_transfer_charge\",\n            \"top_up_by_card_charge\",\n            \"top_up_by_cash_or_cheque\",\n            \"top_up_failed\",\n            \"top_up_limits\",\n            \"top_up_reverted\",\n            \"topping_up_by_card\",\n            \"transaction_charged_twice\",\n            \"transfer_fee_charged\",\n            \"transfer_into_account\",\n            \"transfer_not_received_by_recipient\",\n            \"transfer_timing\",\n            \"unable_to_verify_identity\",\n            \"verify_my_identity\",\n            \"verify_source_of_funds\",\n            \"verify_top_up\",\n            \"virtual_card_not_working\",\n            \"visa_or_mastercard\",\n            \"why_verify_identity\",\n            \"wrong_amount_of_cash_received\",\n            \"wrong_exchange_rate_for_cash_withdrawal\"\n        ],\n        \"example_template\": \"Input: {example}\\nOutput: {label}\"\n    }\n}\n</code></pre> <pre><code>from autolabel import LabelingAgent, AutolabelDataset\n\nagent = LabelingAgent(config=config_zero_shot)\nds = AutolabelDataset('../examples/banking/test.csv', config = config_zero_shot)\nlabeled_dataset = agent.run(ds)\n</code></pre> <p>This zero-shot task execution results in an accuracy of 70.19%.</p> <p>Iterating on this, we compare a fixed few-shot example selection strategy, which randomly chooses k examples from the labeled seedset and appends these same k examples to each prompt for the 1998 items to be labeled. In this case, we use k=10 seed examples per prompt. To use this selection strategy, we need to modify the config:</p> <pre><code>config_fixed_few_shot = {\n    \"task_name\": \"BankingComplaintsClassification\",\n    \"task_type\": \"classification\",\n    \"dataset\": {\n        \"label_column\": \"label\",\n        \"delimiter\": \",\"\n    },\n    \"model\": {\n        \"provider\": \"openai\",\n        \"name\": \"gpt-3.5-turbo\"\n    },\n    \"prompt\": {\n        ...\n        \"few_shot_examples\": \"../examples/banking/seed.csv\",\n        \"few_shot_selection\": \"fixed\",\n        \"few_shot_num\": 10,\n        \"example_template\": \"Input: {example}\\nOutput: {label}\"\n    }\n}\n</code></pre> <pre><code>agent = LabelingAgent(config=config_fixed_few_shot)\nds = AutolabelDataset('../examples/banking/test.csv', config = config_fixed_few_shot)\nlabeled_dataset = agent.run(ds)\n</code></pre> <p>This leads to an accuracy of 73.16%, an improvement of ~3% over the zero-shot baseline.</p> <p>Finally, we compare a semantic similarity example selection strategy, which computes a text embedding for each of the 200 labeled seedset examples. Then, for each of the 1998 items to be labeled, we compute a text embedding and find the k most similar examples from the labeled seedset and append those k examples to the prompt for the current example. This leads to custom examples used for each item to be labeled, with the idea being that more similar examples and their corresponding labels may assist the LLM in labeling. Here is the config change to use semantic similarity as the example selection strategy:</p> <pre><code>config_semantic_similarity = {\n    \"task_name\": \"BankingComplaintsClassification\",\n    \"task_type\": \"classification\",\n    \"dataset\": {\n        \"label_column\": \"label\",\n        \"delimiter\": \",\"\n    },\n    \"model\": {\n        \"provider\": \"openai\",\n        \"name\": \"gpt-3.5-turbo\"\n    },\n    \"prompt\": {\n        ...\n        \"few_shot_examples\": \"../examples/banking/seed.csv\",\n        \"few_shot_selection\": \"semantic_similarity\",\n        \"few_shot_num\": 10,\n        \"example_template\": \"Input: {example}\\nOutput: {label}\"\n    }\n}\n</code></pre> <pre><code>agent = LabelingAgent(config=config_semantic_similarity)\nds = AutolabelDataset('../examples/banking/test.csv', config = config_semantic_similarity)\nlabeled_dataset = agent.run(ds)\n</code></pre> <p>With semantic similarity example selection, we obtain a 79.02% accuracy, a significant increase of ~6% over the fixed-shot strategy.</p> <p>Finally, let's take a look at label diversity set of example selection techniques in action:</p> <pre><code>config_label_diversity_random = {\n    \"task_name\": \"ToxicCommentClassification\",\n    \"task_type\": \"classification\",\n    \"dataset\": {\n        \"label_column\": \"label\",\n        \"delimiter\": \",\"\n    },\n    \"model\": {\n        \"provider\": \"openai\",\n        \"name\": \"gpt-3.5-turbo\"\n    },\n    \"prompt\": {\n        ...\n        \"few_shot_examples\": \"../examples/civil_comments/seed.csv\",\n        \"few_shot_selection\": \"label_diversity_random\",\n        \"few_shot_num\": 5,\n        \"example_template\": \"Input: {example}\\nOutput: {label}\"\n    }\n}\n</code></pre> <pre><code>agent = LabelingAgent(config=config_label_diversity_random)\nds = AutolabelDataset('../examples/civil_comments/test.csv', config = config_label_diversity_random)\nlabeled_dataset = agent.run(ds, max_items=200)\n</code></pre> <pre><code>config_label_diversity_similarity = {\n    \"task_name\": \"ToxicCommentClassification\",\n    \"task_type\": \"classification\",\n    \"dataset\": {\n        \"label_column\": \"label\",\n        \"delimiter\": \",\"\n    },\n    \"model\": {\n        \"provider\": \"openai\",\n        \"name\": \"gpt-3.5-turbo\"\n    },\n    \"prompt\": {\n        ...\n        \"few_shot_examples\": \"../examples/civil_comments/seed.csv\",\n        \"few_shot_selection\": \"label_diversity_similarity\",\n        \"few_shot_num\": 5,\n        \"example_template\": \"Input: {example}\\nOutput: {label}\"\n    }\n}\n</code></pre> <pre><code>agent = LabelingAgent(config=config_label_diversity_similarity)\nds = AutolabelDataset('../examples/civil_comments/test.csv', config = config_label_diversity_similarity)\nlabeled_dataset = agent.run(ds, max_items=200)\n</code></pre> <p>For this run on the civil comments dataset, label diversity at random achieved 80% accuracy and label diversity with semantic similarity achieved 78% accuracy. For the same subset of data, the use of regular semantic similarity example selection obtained 72% accuracy, making for a significant improvement by using label diversity. </p> <p>Label diversity example selection strategies are likely best suited for labeling tasks with a small number of unique labels, which is the case for the civil comments dataset with only 2 labels. This is because equal representation of all the possible labels may be less likely to bias the LLM towards a particular label.</p> <p>By default, Autolabel uses OpenAI to compute text embeddings for few shot example selection strategies that require them (semantic similarity, max marginal relevance). However, Autolabel also supports alternative embedding model providers such as Google Vertex AI and Huggingface as outlined here.</p> <p>It is almost always advisable to use an example selection strategy over a zero-shot approach in your autolabeling workflows, but the choice of which example selection strategy to use is dependent upon the specific labeling task and dataset.</p>"},{"location":"guide/accuracy/prompting-better/","title":"Prompting Better","text":"<p>Like most LLM tasks, a critical part of improving LLM performance in autolabeling tasks is selecting a good prompt. Often, this entails finding a good balance between a descriptive set of instructions, while still remaining concise and clear. </p> <p>Consider the following example of refining a prompt used for a classification task on the civil-comments dataset. Each labeling run below included 500 examples and used the same LLM: gpt-3.5-turbo and used a fixed-shot example selection strategy with 4 seed examples.</p> <p></p> <p>First attempt: <pre><code>config = {\n    \"task_name\": \"ToxicCommentClassification\",\n    \"task_type\": \"classification\",\n    \"dataset\": {\n        \"label_column\": \"label\",\n        \"delimiter\": \",\"\n    },\n    \"model\": {\n        \"provider\": \"openai\",\n        \"name\": \"gpt-3.5-turbo\",\n        \"compute_confidence\": True\n    },\n    \"prompt\": {\n        \"task_guidelines\": \"You are an expert at identifying toxic comments and understanding if a comment is sexually explicit, obscene, toxic, insults a person, demographic or race. \\nYour job is to correctly label the provided input example into one of the following categories:\\n{labels}\",\n        \"labels\": [\n            \"toxic\",\n            \"not toxic\"\n        ],\n        \"few_shot_examples\": \"../examples/civil_comments/seed.csv\",\n        \"few_shot_selection\": \"fixed\",\n        \"few_shot_num\": 4,\n        \"example_template\": \"Input: {example}\\nOutput: {label}\"\n    }\n}\n</code></pre></p> <pre><code>from autolabel import LabelingAgent, AutolabelDataset\n\nagent = LabelingAgent(config=config)\ndataset = AutolabelDataset('../examples/civil_comments/test.csv', config=config)\nlabeled_dataset = agent.run(dataset, max_items = 100)\n</code></pre> <p>Accuracy: 68%</p> <p>This first basic prompt seems clear and concise, but only attains a baseline accuracy of 68%. We can analyze some of the errors the LLM is making to get a better idea of how to improve our prompt. </p> <pre><code>df[df['label'] != df['ToxicCommentClassification_llm_label']]\n</code></pre> <p>In doing so, we notice that a vast majority of the errors (97.2%) are misclassifications of civil comments as toxic by the LLM. For instance, one such example comment is:</p> <pre><code>'This is malfeasance by the Administrator and the Board. They are wasting our money!'\n</code></pre> <p>The presence of generally negative words such as \"malfeasance\" and \"wasting\" may be misleading the LLM. Our prompt may need to include details that guide the LLM to correctly identify cases where the vocabulary used could be mistaken as toxic, but the surrounding context suggests that the comment is actually civil.</p> <p>Adding nuance to the prompt:</p> <p>We can replace the prompt in the above config with the following updated guidelines and re-run the labeling task.</p> <pre><code>\"task_guidelines\": \"You are an expert at identifying toxic comments. You aim to act in a fair and balanced manner, where comments that provide fair criticism of something or someone are labelled 'not toxic'. Similarly, criticisms of policy and politicians are marked 'not toxic', unless the comment includes obscenities, racial slurs or sexually explicit material. Any comments that are sexually explicit, obscene, or insults a person, demographic or race are not allowed and labeled 'toxic'.\\nYour job is to correctly label the provided input example into one of the following categories:\\n{labels}\",\n</code></pre> <pre><code>from autolabel import LabelingAgent, AutolabelDataset\n\nagent = LabelingAgent(config=config)\ndataset = AutolabelDataset('../examples/civil_comments/test.csv', config=config)\nlabeled_dataset = agent.run(dataset, max_items = 100)\n</code></pre> <p>Accuracy: 74%</p> <p>In this second iteration, we added more detail to the prompt such as addressing the nuances between \"fair criticisms\" vs. toxic comments. These additional details lead to better performance, reaching 74% accuracy. From a similar analysis of the LLM errors, we see that the previous misclassification example, along with several other similar ones, has now been correctly labeled.</p> <p>Further improvements:</p> <p>After subsequently experimenting with a few different variations to this prompt, we do not see significant improvements in performance for this task. As a result, after sufficient iteration of the prompt, it is better to look for performance gains through other modifications to the task configuration. For example, comparing different LLMs can often lead to significant improvements. With the same final prompt above, the text-davinci-003 model achieved 88% accuracy, a 14% increase compared to gpt-turbo-3.5.</p>"},{"location":"guide/llms/benchmarks/","title":"Benchmarks","text":""},{"location":"guide/llms/benchmarks/#benchmarking-llms-for-data-labeling","title":"Benchmarking LLMs for data labeling","text":"<p>Key takeaways from our technical report:</p> <ul> <li>State of the art LLMs can label text datasets at the same or better quality compared to skilled human annotators, but ~20x faster and ~7x cheaper.</li> <li>For achieving the highest quality labels, GPT-4 is the best choice among out of the box LLMs (88.4% agreement with ground truth, compared to 86% for skilled human annotators). </li> <li>For achieving the best tradeoff between label quality and cost, GPT-3.5-turbo, PaLM-2 and open source models like FLAN-T5-XXL are compelling.</li> <li>Confidence based thresholding can be a very effective way to mitigate impact of hallucinations and ensure high label quality.</li> </ul>"},{"location":"guide/llms/embeddings/","title":"Embedding Models","text":"<p>Autolabel also supports various models to compute text embeddings that are used in some few shot example selection strategies such as semantic similarity and max marginal relevance. Like the LLMs that Autolabel supports, each embedding model belongs to a provider. Currently the library supports embedding models from 3 providers: OpenAI, Google Vertex AI, and Huggingface. By default, if no embedding config is present in the labeling config but a few shot strategy that requires text embeddings is enabled, Autolabel defaults to use OpenAI embeddings and an OpenAI API key will be required. </p> <p>Details on how to set up the embedding config for each provider are below.</p>"},{"location":"guide/llms/embeddings/#openai","title":"OpenAI","text":"<p>To use models from OpenAI, you can set <code>provider</code> to <code>openai</code> under the <code>embedding</code> key in the labeling configuration. Then, the specific model that will be queried can be specified using the <code>model</code> key. The default embedding model, if none is provided, is <code>text-embedding-ada-002</code></p>"},{"location":"guide/llms/embeddings/#setup","title":"Setup","text":"<p>To use OpenAI models with Autolabel, make sure to first install the relevant packages by running: <pre><code>pip install 'refuel-autolabel[openai]'\n</code></pre> and also setting the following environment variable: <pre><code>export OPENAI_API_KEY=&lt;your-openai-key&gt;\n</code></pre> replacing <code>&lt;your-openai-key&gt;</code> with your API key, which you can get from here.</p>"},{"location":"guide/llms/embeddings/#example-usage","title":"Example usage","text":"<p>Here is an example of setting config to a dictionary that will use OpenAI's <code>text-embedding-ada-002</code> model for computing text embeddings. Specifically, note that in the dictionary provided by the <code>embedding</code> tag, <code>provider</code> is set to <code>openai</code> and <code>model</code> is not set so it will default to <code>text-embedding-ada-002</code>.</p> <pre><code>config = {\n    \"task_name\": \"OpenbookQAWikipedia\",\n    \"task_type\": \"question_answering\",\n    \"dataset\": {\n        \"label_column\": \"answer\",\n        \"delimiter\": \",\"\n    },\n    \"model\": {\n        \"provider\": \"openai\",\n        \"name\": \"gpt-3.5-turbo\",\n        \"params\": {}\n    },\n    \"embedding\": {\n        \"provider\": \"openai\"\n    },\n    \"prompt\": {\n        \"task_guidelines\": \"You are an expert at answering questions.\",\n        \"example_template\": \"Question: {question}\\nAnswer: {answer}\"\n    }\n}\n</code></pre>"},{"location":"guide/llms/embeddings/#hugging-face","title":"Hugging Face","text":"<p>To use models from Hugging Face, you can set <code>provider</code> to <code>huggingface_pipeline</code> when creating a labeling configuration. The specific model that will be queried can be specified using the <code>name</code> key. </p> <p>This will run the model locally on a GPU (if available). You can also specify  quantization strategy to load larger models in lower precision (and thus decreasing memory requirements).</p>"},{"location":"guide/llms/embeddings/#setup_1","title":"Setup","text":"<p>To use Hugging Face models with Autolabel, make sure to first install the relevant packages by running: <pre><code>pip install 'refuel-autolabel[huggingface]'\n</code></pre></p>"},{"location":"guide/llms/embeddings/#example-usage_1","title":"Example usage","text":"<p>Here is an example of setting config to a dictionary that will use the <code>sentence-transformers/all-mpnet-base-v2</code> model for computing text embeddings. Specifically, note that in the dictionary provided by the <code>embedding</code> tag, <code>provider</code> is set to <code>huggingface_pipeline</code> and <code>model</code> is set to be <code>sentence-transformers/all-mpnet-base-v2</code>.</p> <pre><code>config = {\n    \"task_name\": \"OpenbookQAWikipedia\",\n    \"task_type\": \"question_answering\",\n    \"dataset\": {\n        \"label_column\": \"answer\",\n        \"delimiter\": \",\"\n    },\n    \"model\": {\n        \"provider\": \"huggingface_pipeline\",\n        \"name\": \"google/flan-t5-small\",\n        \"params\": {}\n    },\n    \"embedding\": {\n        \"provider\": \"huggingface_pipeline\",\n        \"model\": \"sentence-transformers/all-mpnet-base-v2\"\n    },\n    \"prompt\": {\n        \"task_guidelines\": \"You are an expert at answering questions.\",\n        \"example_template\": \"Question: {question}\\nAnswer: {answer}\"\n    }\n}\n</code></pre>"},{"location":"guide/llms/embeddings/#google-vertex-ai","title":"Google Vertex AI","text":"<p>To use models from Google, you can set the <code>provider</code> to <code>google</code> when creating a labeling configuration. The specific model that will be queried can be specified using the <code>model</code> key. </p>"},{"location":"guide/llms/embeddings/#setup_2","title":"Setup","text":"<p>To use Google models with Autolabel, make sure to first install the relevant packages by running: <pre><code>pip install 'refuel-autolabel[google]'\n</code></pre> and also setting up Google authentication locally.</p>"},{"location":"guide/llms/embeddings/#example-usage_2","title":"Example usage","text":"<p>Here is an example of setting config to a dictionary that will use google's <code>textembedding-gecko</code> model for computing text embeddings. Specifically, note that in the dictionary provided by the <code>embedding</code> tag, <code>provider</code> is set to <code>google</code> and <code>model</code> is set to be <code>textembedding-gecko</code>.</p> <pre><code>config = {\n    \"task_name\": \"OpenbookQAWikipedia\",\n    \"task_type\": \"question_answering\",\n    \"dataset\": {\n        \"label_column\": \"answer\",\n        \"delimiter\": \",\"\n    },\n    \"model\": {\n        \"provider\": \"google\",\n        \"name\": \"text-bison@001\",\n        \"params\": {}\n    },\n    \"embedding\": {\n        \"provider\": \"google\",\n        \"model\": \"textembedding-gecko\"\n    }\n    \"prompt\": {\n        \"task_guidelines\": \"You are an expert at answering questions.\",\n        \"example_template\": \"Question: {question}\\nAnswer: {answer}\"\n    }\n}\n</code></pre>"},{"location":"guide/llms/llms/","title":"Large Language Models (LLMs)","text":"<p>Autolabel supports multiple LLMs for labeling data. Some LLMs are available by calling an API with the appropriate API keys (OpenAI, Anthropic, etc.) while others can be run locally (such as the ones available on Hugging Face). The LLM used to label can be controlled using the <code>provider</code> and <code>name</code> keys in the dictionary specified under <code>model</code> in the input config.</p> <p>Each LLM belongs to an LLM provider -- which refers to the organization or open-source framework through which we are able to access the LLM. A full list of LLM providers and LLMs that are currently supported is provided towards the end of this page.</p> <p>Autolabel makes it easy to try out different LLMs for your task and this page will walk you through how to get started with each LLM provider and model. Separately, we've also benchmarked multiple LLMs across different datasets - you can read the full technical report here [link to blog post] or check out the latest benchmark results here.</p>"},{"location":"guide/llms/llms/#refuel","title":"Refuel","text":"<p>To use models hosted by Refuel, you can set <code>provider</code> to <code>refuel</code> when creating a labeling configuration. The specific model that will be queried can be specified using the <code>name</code> key. Autolabel currently supports two models:</p> <ul> <li><code>refuel-llm</code></li> <li><code>llama-13b-chat</code></li> </ul> <p>You can access RefuelLLM, our recently announced LLM purpose built for data labeling, through Autolabel (Read more about it in this blog post). Refuel LLM is a Llama-v2-13b base model, instruction tuned on over 2500 unique (5.24B tokens) labeling tasks spanning categories such as classification, entity resolution, matching, reading comprehension and information extraction. You can experiment with the model in the playground here.</p> <p></p> <p>You can request access to Refuel LLM here. Read the docs about using RefuelLLM in autolabel here.</p> <p>Llama-13b-chat is a 13 billion parameter model available on Huggingface. However, running such a huge model locally is a challenge, which is why we are currently hosting the model on our servers.</p>"},{"location":"guide/llms/llms/#setup","title":"Setup","text":"<p>To use Refuel models with Autolabel, make sure set the following environment variable:</p> <pre><code>export REFUEL_API_KEY=&lt;your-refuel-key&gt;\n</code></pre> <p>replacing <code>&lt;your-refuel-key&gt;</code> with your API key.</p>"},{"location":"guide/llms/llms/#getting-a-refuel-api-key","title":"Getting a Refuel API key","text":"<p>If you're interested in trying one of the LLMs hosted by Refuel, sign up for your Refuel API key by filling out the form here. We'll review your application and get back to you soon!</p>"},{"location":"guide/llms/llms/#example-usage","title":"Example usage","text":"<p>Here is an example of setting config to a dictionary that will use Refuel's <code>refuel-llm</code> model. Specifically, note that in the dictionary proivded by the <code>model</code> tag, <code>provider</code> is set to <code>refuel</code> and <code>name</code> is set to be <code>refuel-llm</code>.</p> <pre><code>config = {\n    \"task_name\": \"OpenbookQAWikipedia\",\n    \"task_type\": \"question_answering\",\n    \"dataset\": {\n        \"label_column\": \"answer\",\n        \"delimiter\": \",\"\n    },\n    \"model\": {\n        \"provider\": \"refuel\",\n        \"name\": \"refuel-llm\",\n        \"params\": {}\n    },\n    \"prompt\": {\n        \"task_guidelines\": \"You are an expert at answering questions.\",\n        \"example_template\": \"Question: {question}\\nAnswer: {answer}\"\n    }\n}\n</code></pre>"},{"location":"guide/llms/llms/#additional-parameters","title":"Additional parameters","text":"<p>A few parameters that can be passed in for <code>refuel</code> models to control the model behavior. For example:</p> <ul> <li><code>max_new_tokens</code> (int) - The maximum tokens to sample from the model</li> <li><code>temperature</code> (float) - A float b/w 0 and 1 which indicates the diversity you want in the output. 0 uses greedy sampling.</li> </ul> <p>These parameters can be passed in via the <code>params</code> dictionary under <code>model</code>. Here is an example:</p> <pre><code>\"model\": {\n    \"provider\": \"refuel\",\n    \"name\": \"refuel-llm\",\n    \"params\": {\n        \"max_new_tokens\": 512,\n        \"temperature\": 0.1,\n    }\n}\n</code></pre> <p><code>refuel</code> hosted LLMs support all the parameters that can be passed as a part of GenerationConfig while calling generate functions of Hugging Face LLMs.</p>"},{"location":"guide/llms/llms/#openai","title":"OpenAI","text":"<p>To use models from OpenAI, you can set <code>provider</code> to <code>openai</code> when creating a labeling configuration. The specific model that will be queried can be specified using the <code>name</code> key. Autolabel currently supports the following models from OpenAI:</p> <ul> <li><code>text-davinci-003</code></li> <li><code>gpt-3.5-turbo</code>, <code>gpt-3.5-turbo-0301</code> and <code>gpt-3.5-turbo-0613</code> (4,096 max tokens)</li> <li><code>gpt-3.5-turbo-16k</code> and <code>gpt-3.5-turbo-16k-0613</code> (16,384 max tokens)</li> <li><code>gpt-4</code>, <code>gpt-4-0314</code> and <code>gpt-4-0613</code> (8,192 max tokens)</li> <li><code>gpt-4-32k</code>, <code>gpt-4-32k-0314</code> and <code>gpt-4-32k-0613</code> (32,768 max tokens)</li> </ul> <p><code>gpt-4</code> set of models are the most capable (and most expensive) from OpenAI, while <code>gpt-3.5-turbo</code> set of models are cheap (but still quite capable). Detailed pricing for these models is available here.</p>"},{"location":"guide/llms/llms/#setup_1","title":"Setup","text":"<p>To use OpenAI models with Autolabel, make sure to first install the relevant packages by running:</p> <pre><code>pip install 'refuel-autolabel[openai]'\n</code></pre> <p>and also setting the following environment variable:</p> <pre><code>export OPENAI_API_KEY=&lt;your-openai-key&gt;\n</code></pre> <p>replacing <code>&lt;your-openai-key&gt;</code> with your API key, which you can get from here.</p>"},{"location":"guide/llms/llms/#example-usage_1","title":"Example usage","text":"<p>Here is an example of setting config to a dictionary that will use OpenAI's <code>gpt-3.5-turbo</code> model for labeling. Specifically, note that in the dictionary proivded by the <code>model</code> tag, <code>provider</code> is set to <code>openai</code> and <code>name</code> is set to be <code>gpt-3.5-turbo</code>. <code>name</code> can be switched to use any of the three models mentioned above.</p> <pre><code>config = {\n    \"task_name\": \"OpenbookQAWikipedia\",\n    \"task_type\": \"question_answering\",\n    \"dataset\": {\n        \"label_column\": \"answer\",\n        \"delimiter\": \",\"\n    },\n    \"model\": {\n        \"provider\": \"openai\",\n        \"name\": \"gpt-3.5-turbo\",\n        \"params\": {}\n    },\n    \"prompt\": {\n        \"task_guidelines\": \"You are an expert at answering questions.\",\n        \"example_template\": \"Question: {question}\\nAnswer: {answer}\"\n    }\n}\n</code></pre>"},{"location":"guide/llms/llms/#additional-parameters_1","title":"Additional parameters","text":"<p>A few parameters can be passed in alongside <code>openai</code> models to tweak their behavior:</p> <ul> <li><code>max_tokens</code> (int): The maximum tokens to sample from the model</li> <li><code>temperature</code> (float): A float between 0 and 2 which indicates the diversity you want in the output. 0 uses greedy sampling (picks the most likely outcome).</li> </ul> <p>These parameters can be passed in via the <code>params</code> dictionary under <code>model</code>. Here is an example:</p> <pre><code>\"model\": {\n    \"provider\": \"openai\",\n    \"name\": \"gpt-3.5-turbo\",\n    \"params\": {\n        \"max_tokens\": 512,\n        \"temperature\": 0.1\n    }\n}\n</code></pre>"},{"location":"guide/llms/llms/#anthropic","title":"Anthropic","text":"<p>To use models from Anthropic, you can set the <code>provider</code> to <code>anthropic</code> when creating a labeling configuration. The specific model that will be queried can be specified using the <code>name</code> key. Autolabel currently supports the following models from Anthropic:</p> <ul> <li><code>claude-instant-v1</code></li> <li><code>claude-v1</code></li> </ul> <p><code>claude-v1</code> is a state-of-the-art high-performance model, while <code>claude-instant-v1</code> is a lighter, less expensive, and much faster option. <code>claude-instant-v1</code> is ~6.7 times cheaper than <code>claude-v1</code>, at $1.63/1 million tokens. On the other hand <code>claude-v1</code> costs $11.02/1 million tokens.</p>"},{"location":"guide/llms/llms/#setup_2","title":"Setup","text":"<p>To use Anthropic models with Autolabel, make sure to first install the relevant packages by running:</p> <pre><code>pip install 'refuel-autolabel[anthropic]'\n</code></pre> <p>and also setting the following environment variable:</p> <pre><code>export ANTHROPIC_API_KEY=&lt;your-anthropic-key&gt;\n</code></pre> <p>replacing <code>&lt;your-anthropic-key&gt;</code> with your API key, which you can get from here.</p>"},{"location":"guide/llms/llms/#example-usage_2","title":"Example usage","text":"<p>Here is an example of setting config to a dictionary that will use anthropic's <code>claude-instant-v1</code> model for labeling. Specifically, note that in the dictionary proivded by the <code>model</code> tag, <code>provider</code> is set to <code>anthropic</code> and <code>name</code> is set to be <code>claude-instant-v1</code>. <code>name</code> can be switched to use any of the two models mentioned above.</p> <pre><code>config = {\n    \"task_name\": \"OpenbookQAWikipedia\",\n    \"task_type\": \"question_answering\",\n    \"dataset\": {\n        \"label_column\": \"answer\",\n        \"delimiter\": \",\"\n    },\n    \"model\": {\n        \"provider\": \"anthropic\",\n        \"name\": \"claude-instant-v1\",\n        \"params\": {}\n    },\n    \"prompt\": {\n        \"task_guidelines\": \"You are an expert at answering questions.\",\n        \"example_template\": \"Question: {question}\\nAnswer: {answer}\"\n    }\n}\n</code></pre>"},{"location":"guide/llms/llms/#additional-parameters_2","title":"Additional parameters","text":"<p>A few parameters that can be passed in for <code>anthropic</code> models to control the model behavior:</p> <ul> <li><code>max_tokens_to_sample</code> (int): The maximum tokens to sample from the model</li> <li><code>temperature</code> (float): A float between 0 and 2 which indicates the diversity you want in the output. 0 uses greedy sampling (picks the most likely outcome).</li> </ul> <p>These parameters can be passed in via the <code>params</code> dictionary under <code>model</code>. Here is an example:</p> <pre><code>\"model\": {\n    \"provider\": \"anthropic\",\n    \"name\": \"claude-instant-v1\",\n    \"params\": {\n        \"max_tokens_to_sample\": 512,\n        \"temperature\": 0.1\n    }\n}\n</code></pre>"},{"location":"guide/llms/llms/#hugging-face","title":"Hugging Face","text":"<p>To use models from Hugging Face, you can set <code>provider</code> to <code>huggingface_pipeline</code> when creating a labeling configuration. The specific model that will be queried can be specified using the <code>name</code> key. Autolabel currently supports all Sequence2Sequence and Causal Language Models on Hugging Face. All models available on Hugging Face can be found here. Ensure that the model you choose can be loaded using <code>AutoModelForSeq2SeqLM</code> or <code>AutoModelForCausalLM</code>. Here are a few examples:</p> <p>Sequence2Sequence Language Models:</p> <ul> <li><code>google/flan-t5-small</code> (all flan-t5-* models)</li> <li><code>google/pegasus-x-base</code></li> <li><code>microsoft/prophetnet-large-uncased</code></li> </ul> <p>Causal Language Models:</p> <ul> <li><code>gpt2</code></li> <li><code>openlm-research/open_llama_3b</code></li> <li><code>meta-llama/Llama-2-7b</code></li> </ul> <p>This will run the model locally on a GPU (if available). You can also specify quantization strategy to load larger models in lower precision (and thus decreasing memory requirements).</p>"},{"location":"guide/llms/llms/#setup_3","title":"Setup","text":"<p>To use Hugging Face models with Autolabel, make sure to first install the relevant packages by running:</p> <pre><code>pip install 'refuel-autolabel[huggingface]'\n</code></pre>"},{"location":"guide/llms/llms/#example-usage_3","title":"Example usage","text":"<p>Here is an example of setting config to a dictionary that will use <code>google/flan-t5-small</code> model for labeling via Hugging Face. Specifically, note that in the dictionary proivded by the <code>model</code> tag, <code>provider</code> is set to <code>huggingface_pipeline</code> and <code>name</code> is set to be <code>google/flan-t5-small</code>. <code>name</code> can be switched to use any model that satisfies the constraints above.</p> <pre><code>config = {\n    \"task_name\": \"OpenbookQAWikipedia\",\n    \"task_type\": \"question_answering\",\n    \"dataset\": {\n        \"label_column\": \"answer\",\n        \"delimiter\": \",\"\n    },\n    \"model\": {\n        \"provider\": \"huggingface_pipeline\",\n        \"name\": \"google/flan-t5-small\",\n        \"params\": {}\n    },\n    \"prompt\": {\n        \"task_guidelines\": \"You are an expert at answering questions.\",\n        \"example_template\": \"Question: {question}\\nAnswer: {answer}\"\n    }\n}\n</code></pre>"},{"location":"guide/llms/llms/#additional-parameters_3","title":"Additional parameters","text":"<p>A few parameters that can be passed in for <code>huggingface_pipeline</code> models to control the model behavior:</p> <ul> <li><code>max_new_tokens</code> (int) - The maximum tokens to sample from the model</li> <li><code>temperature</code> (float) - A float b/w 0 and 1 which indicates the diversity you want in the output. 0 uses greedy sampling.</li> <li><code>quantize</code> (int) - The model quantization to use. 32 bit by default, but we also support 16 bit and 8 bit support for models which have been hosted on Hugging Face.</li> </ul> <p>These parameters can be passed in via the <code>params</code> dictionary under <code>model</code>. Here is an example:</p> <pre><code>\"model\": {\n    \"provider\": \"huggingface_pipeline\",\n    \"name\": \"google/flan-t5-small\",\n    \"params\": {\n        \"max_new_tokens\": 512,\n        \"temperature\": 0.1,\n        \"quantize\": 8\n    }\n},\n</code></pre> <p>To use Llama 2, you can use the following model configuration:</p> <pre><code>\"model\": {\n    \"provider\": \"huggingface_pipeline\",\n    \"name\": \"meta-llama/Llama-2-7b\",\n}\n</code></pre>"},{"location":"guide/llms/llms/#google-palm","title":"Google PaLM","text":"<p>To use models from Google, you can set the <code>provider</code> to <code>google</code> when creating a labeling configuration. The specific model that will be queried can be specified using the <code>name</code> key. Autolabel currently supports the following models from Google:</p> <ul> <li><code>text-bison@001</code></li> <li><code>chat-bison@001</code></li> </ul> <p><code>text-bison@001</code> is often more suitable for labeling tasks due to its ability to follow natural language instructions. <code>chat-bison@001</code> is fine-tuned for multi-turn conversations. <code>text-bison@001</code> costs $0.001/1K characters and <code>chat-bison@001</code> costs half that at $0.0005/1K characters. Detailed pricing for these models is available here</p>"},{"location":"guide/llms/llms/#setup_4","title":"Setup","text":"<p>To use Google models with Autolabel, make sure to first install the relevant packages by running:</p> <pre><code>pip install 'refuel-autolabel[google]'\n</code></pre> <p>and also setting up Google authentication locally.</p>"},{"location":"guide/llms/llms/#example-usage_4","title":"Example usage","text":"<p>Here is an example of setting config to a dictionary that will use google's <code>text-bison@001</code> model for labeling. Specifically, note that in the dictionary provided by the <code>model</code> tag, <code>provider</code> is set to <code>google</code> and <code>name</code> is set to be <code>text-bison@001</code>. <code>name</code> can be switched to use any of the two models mentioned above.</p> <pre><code>config = {\n    \"task_name\": \"OpenbookQAWikipedia\",\n    \"task_type\": \"question_answering\",\n    \"dataset\": {\n        \"label_column\": \"answer\",\n        \"delimiter\": \",\"\n    },\n    \"model\": {\n        \"provider\": \"google\",\n        \"name\": \"text-bison@001\",\n        \"params\": {}\n    },\n    \"prompt\": {\n        \"task_guidelines\": \"You are an expert at answering questions.\",\n        \"example_template\": \"Question: {question}\\nAnswer: {answer}\"\n    }\n}\n</code></pre>"},{"location":"guide/llms/llms/#additional-parameters_4","title":"Additional parameters","text":"<p>A few parameters can be passed in alongside <code>google</code> models to tweak their behavior:</p> <ul> <li><code>max_output_tokens</code> (int): Maximum number of tokens that can be generated in the response.</li> <li><code>temperature</code> (float): A float between 0 and 1 which indicates the diversity you want in the output. 0 uses greedy sampling (picks the most likely outcome).</li> </ul> <p>These parameters can be passed in via the <code>params</code> dictionary under <code>model</code>. Here is an example:</p> <pre><code>\"model\": {\n    \"provider\": \"google\",\n    \"name\": \"text-bison@001\",\n    \"params\": {\n        \"max_output_tokens\": 512,\n        \"temperature\": 0.1\n    }\n}\n</code></pre>"},{"location":"guide/llms/llms/#model-behavior","title":"Model behavior","text":"<p><code>chat-bison@001</code> always responds in a \"chatty\" manner (example below), often returning more than just the requested label. This can cause problems on certain labeling tasks.</p>"},{"location":"guide/llms/llms/#content-moderation","title":"Content moderation","text":"<p>Both Google LLMs seem to have much stricter content moderation rules than the other supported models. This can cause certain labeling jobs to completely fail as shown in our technical report [add link to technical report]. Consider a different model if your dataset has content that is likely to trigger Google's built-in content moderation.</p>"},{"location":"guide/llms/llms/#cohere","title":"Cohere","text":"<p>To use models from Cohere, you can set the <code>provider</code> to <code>cohere</code> when creating a labeling configuration. The specific model that will be queried can be specified using the <code>name</code> key. Autolabel currently supports the following models from Cohere:</p> <ul> <li><code>command</code> (4096 max tokens)</li> <li><code>command-light</code> (4096 max tokens)</li> <li><code>base</code> (2048 max tokens)</li> <li><code>base-light</code> (2048 max tokens)</li> </ul> <p><code>command</code> is an instruction-following conversational model that performs language tasks with high quality, while <code>command-light</code> is an almost as capable, but much faster option. <code>base</code> is a model that performs generative language tasks, while <code>base-light</code> much faster but a little less capable. All models cost the same at $15/1 million tokens. Detailed pricing for these models is available here.</p>"},{"location":"guide/llms/llms/#setup_5","title":"Setup","text":"<p>To use Cohere models with Autolabel, make sure to first install the relevant packages by running:</p> <pre><code>pip install 'refuel-autolabel[cohere]'\n</code></pre> <p>and also setting the following environment variable:</p> <pre><code>export COHERE_API_KEY=&lt;your-cohere-key&gt;\n</code></pre> <p>replacing <code>&lt;your-cohere-key&gt;</code> with your API key, which you can get from here.</p>"},{"location":"guide/llms/llms/#example-usage_5","title":"Example usage","text":"<p>Here is an example of setting config to a dictionary that will use cohere's <code>command</code> model for labeling. Specifically, note that in the dictionary proivded by the <code>model</code> tag, <code>provider</code> is set to <code>cohere</code> and <code>name</code> is set to be <code>command</code>. <code>name</code> can be switched to use any of the four models mentioned above.</p> <pre><code>config = {\n    \"task_name\": \"OpenbookQAWikipedia\",\n    \"task_type\": \"question_answering\",\n    \"dataset\": {\n        \"label_column\": \"answer\",\n        \"delimiter\": \",\"\n    },\n    \"model\": {\n        \"provider\": \"cohere\",\n        \"name\": \"command\",\n        \"params\": {}\n    },\n    \"prompt\": {\n        \"task_guidelines\": \"You are an expert at answering questions.\",\n        \"example_template\": \"Question: {question}\\nAnswer: {answer}\"\n    }\n}\n</code></pre>"},{"location":"guide/llms/llms/#additional-parameters_5","title":"Additional parameters","text":"<p>A few parameters that can be passed in for <code>cohere</code> models to control the model behavior:</p> <ul> <li><code>max_tokens</code> (int): The maximum number of tokens to predict per generation</li> <li><code>temperature</code> (float): The degree of randomness in generations from 0.0 to 5.0, lower is less random.</li> </ul> <p>These parameters can be passed in via the <code>params</code> dictionary under <code>model</code>. Here is an example:</p> <pre><code>\"model\": {\n    \"provider\": \"cohere\",\n    \"name\": \"command\",\n    \"params\": {\n        \"max_tokens\": 512,\n        \"temperature\": 0.1\n    }\n}\n</code></pre>"},{"location":"guide/llms/llms/#provider-list","title":"Provider List","text":"<p>The table lists out all the provider, model combinations that Autolabel supports today:</p> Provider Name openai text-davinci-003 openai gpt-3.5-turbo models openai gpt-4 models anthropic claude-v1 anthropic claude-instant-v1 huggingface_pipeline seq2seq models and causalLM models refuel flan-t5-xxl google text-bison@001 google chat-bison@001 cohere command cohere command-light cohere base cohere base-light"},{"location":"guide/overview/getting-started/","title":"Getting Started with Autolabel","text":"<p>This page will walk you through your very first labeling task using Refuel Autolabel. Specifically, it'll go over:</p> <ul> <li>Installation</li> <li>Overview of a dataset to label</li> <li>Labeling the dataset using Autolabel</li> </ul>"},{"location":"guide/overview/getting-started/#installation","title":"Installation","text":"<p>Autolabel is available on PyPI and can be installed by running:</p> <pre><code>pip install 'refuel-autolabel[openai]'\n</code></pre> <p>Separate from the Autolabel library, you'll also need to install an integration with your favorite LLM provider. In the example below, we'll be using OpenAI, so you'll need to install the OpenAI SDK and set your API key as an environment variable:</p> <pre><code>export OPENAI_API_KEY=\"&lt;your-openai-key&gt;\"\n</code></pre> <p>To use a different LLM provider, follow the documentation here.</p>"},{"location":"guide/overview/getting-started/#goal-sentiment-analysis-on-a-movie-review-dataset","title":"Goal: Sentiment Analysis on a Movie Review Dataset","text":"<p>Let's say we wanted to run sentiment analysis on a dataset of movie reviews. We want to train our own ML model, but first, we need to label some data for training.</p> <p>Now, we could label a few hundred examples by hand which would take us a few hours. Instead, let's use Autolabel to get a clean, labeled dataset in a few minutes.</p> <p>A dataset<sup>1</sup> containing 200 unlabeled movie reviews is available here, and a couple of examples (with labels) are shown below:</p> text label I was very excited about seeing this film, anticipating a visual excursus on the relation of artistic beauty and nature, containing the kinds of wisdom the likes of \"Rivers and Tides.\" However, that's not what I received. Instead, I get a fairly uninspired film about how human industry is bad for nature. Which is clearly a quite unorthodox claim.The photographer seems conflicted about the aesthetic qualities of his images and the supposed \"ethical\" duty he has to the workers occasionally peopling the images, along the periphery. And frankly, the images were not generally that impressive. And according to this \"artist,\" scale is the basis for what makes something beautiful.In all respects, a stupid film. For people who'd like to feel better about their environmental consciousness ... but not for any one who would like to think about the complexities of the issues surrounding it. negative I loved this movie. I knew it would be chocked full of camp and silliness like the original series. I found it very heart warming to see Adam West, Burt Ward, Frank Gorshin, and Julie Newmar all back together once again. Anyone who loved the Batman series from the 60's should have enjoyed Return to the Batcave. You could tell the actors had a lot of fun making this film, especially Adam West. And I'll bet he would have gladly jumped back into his Batman costume had the script required him to do so. I told a number of friends about this movie who chose not to view it... now they wished they had. I have all of the original 120 episodes on VHS. Now this movie will join my collection. Thank You for the reunion Adam and Burt. positive <p>Our goal is to label the full 200 examples using Autolabel.</p>"},{"location":"guide/overview/getting-started/#labeling-with-autolabel","title":"Labeling with AutoLabel","text":"<p>Autolabel provides a simple 3-step process for labeling data:</p> <ul> <li>Specify the configuration of your labeling task as a JSON</li> <li>Preview the labeling task against your dataset</li> <li>Label your data!</li> </ul>"},{"location":"guide/overview/getting-started/#specify-the-labeling-task-via-configuration","title":"Specify the labeling task via configuration","text":"<p>First, create a JSON file that specifies:</p> <ul> <li>Task: <code>task_name</code> is <code>MovieSentimentReview</code> and the <code>task_type</code> is <code>classification</code></li> <li>LLM: Choice of LLM provider and model - here we are using <code>gpt-3.5-turbo</code> from OpenAI</li> <li>Instructions: These are the labeling guidelines provided to the LLM for labeling</li> </ul> <pre><code>config = {\n    \"task_name\": \"MovieSentimentReview\",\n    \"task_type\": \"classification\",\n    \"dataset\": {\n        \"label_column\": \"label\"\n    },\n    \"model\": {\n        \"provider\": \"openai\",\n        \"name\": \"gpt-3.5-turbo\"\n    },\n    \"prompt\": {\n        \"task_guidelines\": \"You are an expert at analyzing the sentiment of movie reviews. Your job is to classify the provided movie review into one of the following labels: {labels}\",\n        \"labels\": [\n            \"positive\",\n            \"negative\",\n            \"neutral\"\n        ],\n        \"few_shot_examples\": [\n            {\n                \"example\": \"I got a fairly uninspired stupid film about how human industry is bad for nature.\",\n                \"label\": \"negative\"\n            },\n            {\n                \"example\": \"I loved this movie. I found it very heart warming to see Adam West, Burt Ward, Frank Gorshin, and Julie Newmar together again.\",\n                \"label\": \"positive\"\n            },\n            {\n                \"example\": \"This movie will be played next week at the Chinese theater.\",\n                \"label\": \"neutral\"\n            }\n        ],\n        \"example_template\": \"Example: {example}\\nLabel: {label}\"\n    }\n}\n</code></pre> <p>To create a custom configuration, you can use the CLI or write your own.</p>"},{"location":"guide/overview/getting-started/#preview-the-labeling-against-your-dataset","title":"Preview the labeling against your dataset","text":"<p>First import <code>autolabel</code>, create a <code>LabelingAgent</code> object and then run the <code>plan</code> command against the dataset (available here) and can be downloaded through the <code>autolabel.get_data</code> function):</p> <pre><code>from autolabel import LabelingAgent, AutolabelDataset, get_data\nget_data('movie_reviews')\n\nagent = LabelingAgent(config)\nds = AutolabelDataset('data/movie_reviews/test.csv', config = config)\nagent.plan(ds)\n</code></pre> <p>This produces:</p> <pre><code>Computing embeddings... \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100/100 0:00:00 0:00:00\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Total Estimated Cost     \u2502 $0.538  \u2502\n\u2502 Number of Examples       \u2502 200     \u2502\n\u2502 Average cost per example \u2502 0.00269 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Prompt Example \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nYou are an expert at analyzing the sentiment of moview reviews. Your job is to classify the provided movie review as positive or negative.\n\nYou will return the answer with just one element: \"the correct label\"\n\nNow I want you to label the following example:\nInput: I was very excited about seeing this film, anticipating a visual excursus on the relation of artistic beauty and nature, containing the kinds of wisdom the likes of \"Rivers and Tides.\" However, that's not what I received. Instead, I get a fairly uninspired film about how human industry is bad for nature. Which is clearly a quite unorthodox claim.&lt;br /&gt;&lt;br /&gt;The photographer seems conflicted about the aesthetic qualities of his images and the supposed \"ethical\" duty he has to the workers occasionally peopling the images, along the periphery. And frankly, the images were not generally that impressive. And according to this \"artist,\" scale is the basis for what makes something beautiful.&lt;br /&gt;&lt;br /&gt;In all respects, a stupid film. For people who'd like to feel better about their environmental consciousness ... but not for any one who would like to think about the complexities of the issues surrounding it.\nOutput:\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</code></pre> <p>This shows you:</p> <ul> <li>Number of examples to be labeled in the dataset: <code>200</code></li> <li>Estimated cost of running this labeling task: <code>&lt;$1</code></li> <li>Exact prompt being sent to the LLM</li> </ul> <p>Having previewed the labeling, we are ready to start labeling.</p>"},{"location":"guide/overview/getting-started/#label-your-dataset","title":"Label your dataset","text":"<p>Now, you can use the <code>run</code> command to label:</p> <pre><code>ds = AutolabelDataset('docs/assets/movie_reviews.csv', config = config)\nds = agent.run(ds)\n</code></pre> <p>This takes just a few minutes to run, and returns the labeled data as an Autolabel Dataset. We can explore this by running:</p> <pre><code>ds.df.head()\n&gt;\n                                                text  ... MovieSentimentReview_llm_label\n0  I was very excited about seeing this film, ant...  ...                       negative\n1  Serum is about a crazy doctor that finds a ser...  ...                       negative\n2  This movie was so very badly written. The char...  ...                       negative\n3  Hmmmm, want a little romance with your mystery...  ...                       negative\n4  I loved this movie. I knew it would be chocked...  ...                       positive\n\n[5 rows x 4 columns]\n</code></pre> <p>At this point, we have a labeled dataset ready, and we can begin training our ML models.</p>"},{"location":"guide/overview/getting-started/#using-hugging-face-datasets","title":"Using Hugging Face datasets","text":"<p>If you want to use a Hugging Face dataset directly, you can pass it into <code>agent.plan</code> and <code>agent.run</code> as you would a file path or <code>pandas.DataFrame</code>.</p> <pre><code>dataset = load_dataset(DATASET_NAME)\nagent = LabelingAgent(config)\n\nagent.plan(test_dataset)\nagent.run(test_dataset)\n</code></pre>"},{"location":"guide/overview/getting-started/#summary","title":"Summary","text":"<p>In this simple walkthrough, we have installed <code>autolabel</code>, gone over an example dataset to label (sentiment analysis for moview reviews) and used <code>autolabel</code> to label this dataset in just a few minutes.</p> <p>We hope that this gives you a glimpse of what you can do with Refuel. There are many other labeling tasks available within Autolabel, and if you have any questions, join our community here or open an issue on Github.</p> <ol> <li> <p>Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). Learning Word Vectors for Sentiment Analysis. The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011).\u00a0\u21a9</p> </li> </ol>"},{"location":"guide/overview/tutorial-classification/","title":"Tutorial - Toxic comment classification","text":"<p>This is a detailed tutorial that walks you through many features of the Autolabel library while solving a problem faced by many companies - labeling toxic comments for content moderation. We will be using OpenAI's <code>gpt-3.5-turbo</code> for the data labeling, and Refuel's LLM for confidence estimation.</p> <p>If you want to run this code as you follow along, check out this Colab notebook: </p>"},{"location":"guide/overview/tutorial-classification/#autolabel-installation","title":"Autolabel installation","text":"<p>Since we'll be using OpenAI along with Autolabel, we can install all necessary libraries by simply running: <pre><code>pip install 'refuel-autolabel[openai]'\n</code></pre></p> <p>Now, we can set our OpenAI key as an environment variable to get started. You can always use an LLM of your choice - see more optioons and installation instructions here. </p>"},{"location":"guide/overview/tutorial-classification/#download-and-review-dataset","title":"Download and review dataset","text":"<p>We'll be using a dataset called Civil Comments, which is available through Autolabel. You can download it locally, by simply running: <pre><code>from autolabel import get_data\n\nget_data('civil_comments')\n</code></pre></p> <p>The output is: <pre><code>Downloading seed example dataset to \"seed.csv\"...\n100% [..............................................................................] 65757 / 65757\nDownloading test dataset to \"test.csv\"...\n100% [............................................................................] 610663 / 610663\n</code></pre></p> <p>This results in two files being downloaded locally:</p> <ul> <li><code>seed.csv</code>: small dataset with labels that we'll rely on as helpful examples.</li> <li><code>test.csv</code>: larger dataset that we are trying to label.</li> </ul> <p>A few examples are shown below:</p> label examples <code>toxic</code> \"The ignorance and bigotry comes from your post!\" <code>not toxic</code> \"This is malfeasance by the Administrator and the Board. They are wasting our money!\""},{"location":"guide/overview/tutorial-classification/#start-the-labeling-process","title":"Start the labeling process","text":"<p>Labeling with Autolabel is a 3-step process:</p> <ul> <li>First, we specify a labeling configuration (see <code>config</code> object below) and create a <code>LabelingAgent</code></li> <li>Next, we do a dry-run on our dataset using the LLM specified in <code>config</code> by running <code>agent.plan</code></li> <li>Finally, we run the labeling with <code>agent.run</code></li> </ul>"},{"location":"guide/overview/tutorial-classification/#experiment-1-try-simple-labeling-guidelines","title":"Experiment #1: Try simple labeling guidelines","text":"<p>Define the configuration file below: <pre><code>config = {\n    \"task_name\": \"ToxicCommentClassification\",\n    \"task_type\": \"classification\", # classification task\n    \"dataset\": {\n        \"label_column\": \"label\",\n    },\n    \"model\": {\n        \"provider\": \"openai\",\n        \"name\": \"gpt-3.5-turbo\" # the model we want to use\n    },\n    \"prompt\": {\n        # very simple instructions for the LLM\n        \"task_guidelines\": \"Does the provided comment contain 'toxic' language? Say toxic or not toxic.\",\n        \"labels\": [ # list of labels to choose from\n            \"toxic\",\n            \"not toxic\"\n        ],\n        \"example_template\": \"Input: {example}\\nOutput: {label}\"\n    }\n}\n</code></pre> To create a custom configuration, you can use the CLI or write your own.</p> <p>Now, we do the dry-run with <code>agent.plan</code>: <pre><code>from autolabel import LabelingAgent, AutolabelDataset\n\nagent = LabelingAgent(config)\nds = AutolabelDataset('docs/civil_comments/test.csv', config = config)\nagent.plan(ds)\n</code></pre></p> <p>Output: <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Total Estimated Cost     \u2502 $4.4442 \u2502\n\u2502 Number of Examples       \u2502 2000    \u2502\n\u2502 Average cost per example \u2502 $0.0022 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Prompt Example \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nDoes the provided comment contain 'toxic' language? Say toxic or not toxic.\n\nYou will return the answer with just one element: \"the correct label\"\n\nNow I want you to label the following example:\nInput: [ Integrity means that you pay your debts.]. Does this apply to President Trump too?\nOutput: \n</code></pre></p> <p>Finally, we run the data labeling: <pre><code>ds = agent.run(ds, max_items=100)\n</code></pre></p> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 support \u2503 threshold \u2503 accuracy \u2503 completion_rate \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 100     \u2502 -inf      \u2502 0.54     \u2502 1.0             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>54% accuracy is not very good! Let's see if we can improve this further!</p>"},{"location":"guide/overview/tutorial-classification/#experiment-2-few-shot-prompting-to-provide-helpful-examples","title":"Experiment #2: Few-shot prompting to provide helpful examples","text":"<p>Similar to how human labelers find it helpful to use relevant examples when making a decision, LLM performance for labeling also goes up when choosing helpful examples in the prompt. For this next experiment, we will pick a few helpful examples from <code>seed.csv</code>. More information on few-shot prompting can be found here.</p> <p>We take the previous config, and just update the following fields: <pre><code>{\n    \"task_name\": \"ToxicCommentClassification\",\n    \"task_type\": \"classification\",\n    \"dataset\": {\n        \"label_column\": \"label\",\n        \"delimiter\": \",\"\n    },\n    \"model\": {\n        \"provider\": \"openai\",\n        \"name\": \"gpt-3.5-turbo\",\n    },\n    \"prompt\": {\n        \"task_guidelines\":  \"Does the provided comment contain 'toxic' language? Say toxic or not toxic.\",\n        \"labels\": [\n            \"toxic\",\n            \"not toxic\"\n        ],\n        \"few_shot_examples\": [\n            {\n                \"example\": \"It's ridiculous that these guys are being called 'protesters'. Being armed is a threat of violence, which makes them terrorists.\",\n                \"label\": \"toxic\"\n            },\n            {\n                \"example\": \"This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!\",\n                \"label\": \"not toxic\"\n            },\n            {\n                \"example\": \"This bitch is nuts. Who would read a book by a woman\",\n                \"label\": \"toxic\"\n            },\n            {\n                \"example\": \"It was a great show. Not a combo I'd of expected to be good together but it was.\",\n                \"label\": \"not toxic\"\n            }\n        ],\n        \"few_shot_selection\": \"fixed\",\n        \"few_shot_num\": 4,\n        \"example_template\": \"Input: {example}\\nOutput: {label}\"\n    }\n}\n</code></pre></p> <p>That's it! We are now ready to create a <code>LabelingAgent</code> and run the same <code>agent.plan</code> and <code>agent.run</code> commands.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Total Estimated Cost     \u2502 $4.9442 \u2502\n\u2502 Number of Examples       \u2502 2000    \u2502\n\u2502 Average cost per example \u2502 $0.0025 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Prompt Example \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nDoes the provided comment contain 'toxic' language? Say toxic or not toxic.\n\nYou will return the answer with just one element: \"the correct label\"\n\nSome examples with their output answers are provided below:\n\nInput: It's ridiculous that these guys are being called 'protesters'. Being armed is a threat of violence, which makes them terrorists.\nOutput: toxic\n\nInput: This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!\nOutput: not toxic\n\nInput: This bitch is nuts. Who would read a book by a woman\nOutput: toxic\n\nInput: It was a great show. Not a combo I'd of expected to be good together but it was.\nOutput: not toxic\n\nNow I want you to label the following example:\nInput: [ Integrity means that you pay your debts.] Does this apply to President Trump too?\nOutput:\n</code></pre> <p>With additional examples, the cost has gone up slightly. Now, we run the labeling with:</p> <pre><code>labels, df, metrics = agent.run(ds, max_items=100)\n</code></pre> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 support \u2503 threshold \u2503 accuracy \u2503 completion_rate \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 100     \u2502 -inf      \u2502 0.68     \u2502 1.0             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Nice! We improved performance from 54% to 68% by providing a few examples to the LLM.</p>"},{"location":"guide/overview/tutorial-classification/#experiment-3-improving-task-guidelines-after-reviewing-errors-prompt-engineering","title":"Experiment #3: Improving task guidelines after reviewing errors (prompt engineering)","text":"<p>Typically, you can improve the accuracy by reviewing mistakes and updating the task guidelines (see another example here). You can review some of the mistakes from the previous run by looking at the output Pandas DataFrame produced called <code>df</code>: <pre><code>df[df['label'] != df['ToxicCommentClassification_llm_label']].head(10)\n</code></pre></p> <p>Let's say we update our task guidelines to be more explicit about how should the LLM make the decision about whether a comment is toxic or not:</p> <pre><code>{\n    \"task_name\": \"ToxicCommentClassification\",\n    \"task_type\": \"classification\",\n    \"dataset\": {\n        \"label_column\": \"label\",\n        \"delimiter\": \",\"\n    },\n    \"model\": {\n        \"provider\": \"openai\",\n        \"name\": \"gpt-3.5-turbo\",\n    },\n    \"prompt\": {\n        \"task_guidelines\": \"You are an expert at identifying toxic comments. You aim to act in a fair and balanced manner, where comments that provide fair criticism of something or someone are labelled 'not toxic'. Similarly, criticisms of policy and politicians are marked 'not toxic', unless the comment includes obscenities, racial slurs or sexually explicit material. Any comments that are sexually explicit, obscene, or insults a person, demographic or race are not allowed and labeled 'toxic'. \\nYour job is to correctly label the provided input example into one of the following categories:\\n{labels}\",\n        \"labels\": [\n            \"toxic\",\n            \"not toxic\"\n        ],\n        \"few_shot_examples\": [\n            {\n                \"example\": \"It's ridiculous that these guys are being called 'protesters'. Being armed is a threat of violence, which makes them terrorists.\",\n                \"label\": \"toxic\"\n            },\n            {\n                \"example\": \"This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!\",\n                \"label\": \"not toxic\"\n            },\n            {\n                \"example\": \"This bitch is nuts. Who would read a book by a woman\",\n                \"label\": \"toxic\"\n            },\n            {\n                \"example\": \"It was a great show. Not a combo I'd of expected to be good together but it was.\",\n                \"label\": \"not toxic\"\n            }\n        ],\n        \"few_shot_selection\": \"fixed\",\n        \"few_shot_num\": 4,\n        \"example_template\": \"Input: {example}\\nOutput: {label}\"\n    }\n}\n</code></pre> <p>Now, when we run <code>agent.run</code>, we get the following results:</p> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 support \u2503 threshold \u2503 accuracy \u2503 completion_rate \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 100     \u2502 -inf      \u2502 0.78     \u2502 1.0             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>We now hit an accuracy of 78%, which is very promising! If we spend more time improving the guidelines or choosing different examples, we can push accuracy even further.</p>"},{"location":"guide/overview/tutorial-classification/#experiment-4-experimenting-with-llms","title":"Experiment #4: Experimenting with LLMs","text":"<p>We've iterated a fair bit on prompts, and few-shot examples. Let's evaluate a few different LLMs provided by the library out of the box. For example, we observe that we can boost performance even further by using <code>text-davinci-003</code>: </p> <pre><code>{\n    \"task_name\": \"ToxicCommentClassification\",\n    \"task_type\": \"classification\",\n    \"dataset\": {\n        \"label_column\": \"label\",\n        \"delimiter\": \",\"\n    },\n    \"model\": {\n        \"provider\": \"openai\",\n        \"name\": \"text-davinci-003\",\n    },\n    \"prompt\": {\n        \"task_guidelines\": \"You are an expert at identifying toxic comments. You aim to act in a fair and balanced manner, where comments that provide fair criticism of something or someone are labelled 'not toxic'. Similarly, criticisms of policy and politicians are marked 'not toxic', unless the comment includes obscenities, racial slurs or sexually explicit material. Any comments that are sexually explicit, obscene, or insults a person, demographic or race are not allowed and labeled 'toxic'. \\nYour job is to correctly label the provided input example into one of the following categories:\\n{labels}\",\n        \"labels\": [\n            \"toxic\",\n            \"not toxic\"\n        ],\n        \"few_shot_examples\": [\n            {\n                \"example\": \"It's ridiculous that these guys are being called 'protesters'. Being armed is a threat of violence, which makes them terrorists.\",\n                \"label\": \"toxic\"\n            },\n            {\n                \"example\": \"This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!\",\n                \"label\": \"not toxic\"\n            },\n            {\n                \"example\": \"This bitch is nuts. Who would read a book by a woman\",\n                \"label\": \"toxic\"\n            },\n            {\n                \"example\": \"It was a great show. Not a combo I'd of expected to be good together but it was.\",\n                \"label\": \"not toxic\"\n            }\n        ],\n        \"few_shot_selection\": \"fixed\",\n        \"few_shot_num\": 4,\n        \"example_template\": \"Input: {example}\\nOutput: {label}\"\n    }\n}\n</code></pre> <p>While the per token API price for this model is higher, we're able to boost the accuracy to 88%!</p> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 support \u2503 threshold \u2503 accuracy \u2503 completion_rate \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 100     \u2502 -inf      \u2502 0.88     \u2502 1.0             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guide/overview/tutorial-classification/#experiment-5-using-confidence-scores","title":"Experiment #5: Using confidence scores","text":"<p>Refuel provides LLMs that can compute confidence scores for every label, if the LLM you've chosen doesn't provide token-level log probabilities. This is helpful, because you can calibrate a confidence threshold for your labeling task, and then route less confident labels to humans, while you still get the benefits of auto-labeling for the confident examples. Let's see how this works. </p> <p>First, set your Refuel API key as an environment variable (and if you don't have this key yet, sign up here). <pre><code>os.environ['REFUEL_API_KEY'] = '&lt;your-api-key&gt;'\n</code></pre></p> <p>Now, update your configuration: <pre><code>config[\"model\"][\"compute_confidence\"] = True\n</code></pre></p> <p>Finally, let's run <code>agent.run</code> as before - this produces the table below: <pre><code>Metric: auroc: 0.8858\nActual Cost: 0.0376\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 support \u2503 threshold \u2503 accuracy \u2503 completion_rate \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 100     \u2502 -inf      \u2502 0.78     \u2502 1.0             \u2502\n\u2502 1       \u2502 0.9988    \u2502 1.0      \u2502 0.01            \u2502\n\u2502 12      \u2502 0.9957    \u2502 1.0      \u2502 0.12            \u2502\n\u2502 13      \u2502 0.9949    \u2502 0.9231   \u2502 0.13            \u2502\n\u2502 54      \u2502 0.9128    \u2502 0.9815   \u2502 0.54            \u2502\n\u2502 55      \u2502 0.9107    \u2502 0.9636   \u2502 0.55            \u2502\n\u2502 63      \u2502 0.6682    \u2502 0.9683   \u2502 0.63            \u2502\n\u2502 66      \u2502 0.6674    \u2502 0.9242   \u2502 0.66            \u2502\n\u2502 67      \u2502 0.6673    \u2502 0.9254   \u2502 0.67            \u2502\n\u2502 69      \u2502 0.6671    \u2502 0.8986   \u2502 0.69            \u2502\n\u2502 71      \u2502 0.6667    \u2502 0.9014   \u2502 0.71            \u2502\n\u2502 72      \u2502 0.6667    \u2502 0.8889   \u2502 0.72            \u2502\n\u2502 78      \u2502 0.4819    \u2502 0.8974   \u2502 0.78            \u2502\n\u2502 79      \u2502 0.4774    \u2502 0.8861   \u2502 0.79            \u2502\n\u2502 87      \u2502 0.4423    \u2502 0.8966   \u2502 0.87            \u2502\n\u2502 100     \u2502 0.0402    \u2502 0.78     \u2502 1.0             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <p>The rows in this table show labeling performance at different confidence thresholds, and set an autolabeling confidence threshold at the desired accuracy. For instance, from the table above we can set the confidence threshold at 0.6682 which allows us to label at 96% accuracy with a completion rate of 63%.</p> <p>If you want to run this code as you follow along, check out this Colab notebook: </p>"},{"location":"guide/overview/tutorial-classification/#final-thoughts","title":"Final thoughts","text":"<p>Hopefully, this tutorial was helpful in understanding how Autolabel can help you label datasets quickly and at high quality. A Jupyter notebook for this tutorial can be found here.</p> <p>You can find more example notebooks here, including for tasks such as question answering, named entity recognition, etc. </p> <p>Drop us a message in our Discord if you want to chat with us, or go to Github to report any issues!</p>"},{"location":"guide/reliability/llm-output-caching/","title":"LLM Output Caching","text":"<p>To help reduce time and cost when iterating the prompt for better labeling accuracy, we cache the calls made to the LLM.</p>"},{"location":"guide/reliability/llm-output-caching/#cache-entry","title":"Cache Entry","text":"<p>A cache entry has the following attributes:</p> <ul> <li><code>Model Name</code></li> <li><code>Prompt</code></li> <li><code>Model Params</code></li> </ul> <p>This means that anytime there are changes to either the language model or the prompt, the model will be called for producing label. Also, changes to the model parameters like the <code>max_tokens</code> or <code>temperature</code> could affect the label output and therefore modifying such parameters result in new calls to the model instead of using cached calls.</p>"},{"location":"guide/reliability/llm-output-caching/#caching-storage","title":"Caching Storage","text":"<p>The cached entries are stored in a SQLite database. We will be adding support for In Memory cache and Redis cache in future.</p>"},{"location":"guide/reliability/llm-output-caching/#disable-caching","title":"Disable Caching","text":"<p>The cache is enabled by default and if you wish to disable it, you can set <code>cache=False</code> when initializing the LabelingAgent.</p> <pre><code>from autolabel import LabelingAgent\n\nagent = LabelingAgent(config='examples/configs/civil_comments.json', cache=False)\n</code></pre>"},{"location":"guide/reliability/state-management/","title":"State Management","text":"<p>Labeling a large dataset can take some time and if you're running the task on a Jupyter notebook and your machine decides to sleep during the time, it could be really frustrating. (we've been there! ).</p> <p>Therefore, we periodically save the progress of the labeling task in a SQLite database, so if the task is interrupted, you can resume it from where you left off.</p>"},{"location":"guide/reliability/state-management/#task-run-state","title":"Task Run State","text":"<p>When a labeling task is triggered, a task run entry gets initialized inside the database. We maintain the dataset index till where the labels have been computed. After every small chunk (size 5) of data gets labeled, the dataset index gets updated and the labels are persisted.</p> <p>In case the labeling process get interrupted/terminated and you trigger the task with the same parameters again, the library first checks for a previous instance of the same task.</p> <p>If there was an incomplete task present, you would be prompted with details of the previous run and asked to resume the task. If you choose to resume the previous task, it gets loaded into the memory and resumed from previous state otherwise the previous entry gets deleted.</p>"},{"location":"guide/reliability/state-management/#deep-dive","title":"Deep Dive","text":"<p>You'd likely never have to interact with the database directly but in case you wish to look at the state of the database, you can do that using any CLI or GUI that supports SQL. The database is saved in the same directory from where you run the LabelingAgent notebook and is named <code>.autolabel.db</code>.</p> <p>We have the following tables:</p> <ul> <li><code>datasets</code>: Stores the dataset file information</li> <li><code>tasks</code>: Stores the labeling task attributes</li> <li><code>task_runs</code>: Stores the current state of a labeling task run</li> <li><code>annotations</code>: Stores the LLM annotation corresponding to the task run</li> <li><code>generation_cache</code>: Cache for the LLM calls</li> </ul>"},{"location":"guide/resources/CLI/","title":"CLI","text":"<p>The Autolabel CLI was created to make the config file creation process easier. It is a simple command line interface that will ask you a series of questions and then generate a config file for you. To use it, simply run the following command:</p> <pre><code>autolabel config\n</code></pre>"},{"location":"guide/resources/CLI/#walkthrough-creating-a-config-for-civil-comments","title":"Walkthrough: Creating a Config for Civil Comments","text":"<ol> <li> The first step is to run the <code>autolabel</code> command with the <code>config</code> argument:  <pre><code>autolabel config\n</code></pre> </li> <li> The program will prompt you to enter the task name.  <pre><code>Enter the task name: ToxicCommentClassification\n</code></pre> </li> <li> Next, you need to choose the task type from the provided options.:  <pre><code>Choose a task type\n&gt; classification\n  named_entity_recognition\n  question_answering\n  entity_matching\n  multilabel_classification\n</code></pre> </li> <li> Now, the program will ask for dataset configuration details. You need to specify the delimiter used in your dataset, the label column name, and an optional explanation column name:  <pre><code>Dataset Configuration\nEnter the delimiter (,):\nEnter the label column name: label\nEnter the explanation column name (optional):\n</code></pre> Anything surrounded by parenthesis at the end of a prompt will be used as the default value if you don't input anything. Make sure to change this if it does not line up with your task. </li> <li> The program will then ask for model configuration. You will need to specify the model provider from the options. Next, enter the model name, optional model parameters, whether the model should compute confidence, and the strength of the logit bias:  <pre><code>Model Configuration\nEnter the model provider\n&gt; openai\n  anthropic\n  huggingface_pipeline\n  refuel\n  google\n  cohere\nEnter the model name: gpt-3.5-turbo\nEnter a model parameter name (or leave blank for none):\nShould the model compute confidence? [y/n] (n):\nWhat is the strength of logit bias? (0.0): 100\n</code></pre> </li> <li> Next, you will configure the task prompt. First, enter the task guidelines. In the task guidelines, <code>{num_labels}</code> and <code>{labels}</code> will be replaced by the number of labels and the labels list respectively. Next, specify the labels. Then, write the example template with placeholders for the column names you want to use in the prompt. You can also add an output guideline and format if needed. Lastly, you can choose whether to use a chain of thought:  <pre><code>Prompt Configuration\nEnter the task guidelines (Your job is to correctly label the provided input example into one of the following {num_labels} categories.\nCategories:\n{labels}\n):\nEnter a valid label (or leave blank for none): toxic\nEnter a valid label (or leave blank to finish): not toxic\nEnter a valid label (or leave blank to finish):\nEnter the example template: Example: {example}\\nLabel: {label}\nEnter the value for example (or leave blank for none):\nEnter the output guideline (optional):\nEnter the output format (optional):\nShould the prompt use a chain of thought? [y/n] (n):\n</code></pre> </li> <li> The program will then display the configuration that you have provided as a python dictionary:  <pre><code>{\n    'task_name': 'ToxicCommentClassification',\n    'task_type': 'classification',\n    'dataset': {'delimiter': ',', 'label_column': 'label'},\n    'model': {'provider': 'openai', 'name': 'gpt-3.5-turbo', 'compute_confidence': False, 'logit_bias': 100.0},\n    'prompt': {\n        'task_guidelines': 'Your job is to correctly label the provided input example into one of the following {num_labels} categories.\\nCategories:\\n{labels}\\n',\n        'labels': ['toxic', 'not toxic'],\n        'example_template': 'Example: {example}\\nLabel: {label}',\n        'chain_of_thought': False\n    }\n}\n</code></pre> </li> <li> Finally, the program will write the configuration to a file named \"{your_task_name}_config.json\".  <pre><code>Writing config to ToxicCommentClassification_config.json\n</code></pre> </li> </ol> <p>That's it! You have successfully created a config for a task using the CLI program. The generated configuration file can now be used for any labeling runs with autolabel!</p>"},{"location":"guide/resources/CLI/#providing-a-seed-file","title":"Providing a seed file","text":"<p>You can provide a seed file to the CLI to help it generate the config file. Providing a seed file to the CLI allows it to automatically provide drop-down menus for column name inputs, detect labels that are already present in the seed file, and fill the few shot examples by row number in the seed file. To do this, simply run the following command:</p> <pre><code>autolabel config &lt;path-to-seed-file&gt;\n</code></pre> <p>For example, if you have a file called <code>seed.csv</code> in the current directory, you would run the following command:</p> <pre><code>autolabel config seed.csv\n</code></pre> <p>Here's an example of what the prompt configuration section would look like with a seed file:</p> <pre><code>Detected 2 unique labels in seed dataset. Use these labels? [y/n]: y\nEnter the example template: Example: {example}\\nLabel: {label}\nUse seed.csv as few shot example dataset? [y/n]: n\nEnter the value for example or row number (or leave blank for none): 3\n{'example': \"When all else fails, change the subject to Hillary's emails.\", 'label': 'not toxic'}\nEnter the value for example or row number (or leave blank to finish): 7\n{\n    'example': 'He may like the internal forum, but the reality is he has affirmed traditional doctrine and practices. While he does like the internal forum he has not changed anything.',\n    'label': 'not toxic'\n}\nEnter the value for example or row number (or leave blank to finish): 24\n{'example': '........... said the blind dumb and deaf lemming.', 'label': 'toxic'}\nEnter the value for example or row number (or leave blank to finish): 64\n{\n    'example': 'Do you have a citation for that statement or did you just make it up yourself? BTW, this thread is about the unhealthy liar the Democrats have\nnominated.',\n    'label': 'not toxic'\n}\nEnter the value for example or row number (or leave blank to finish):\nEnter the few shot selection algorithm\n&gt; fixed\n  semantic_similarity\n  max_marginal_relevance\n  label_diversity_random\n  label_diversity_similarity\nEnter the number of few shot examples to use (4):\n</code></pre> <p>As you can see, the CLI automatically detected the labels in the seed file and used them to generate the labels list. It also automatically filled the few shot examples with the examples from the seed file after letting the user choose the rows to use.</p>"},{"location":"guide/resources/CLI/#specifying-model-parameters","title":"Specifying Model Parameters","text":"<p>To specify model parameters, you can simply enter the parameter name and value when prompted. For example, if you wanted to specify the <code>temperature</code> parameter for the <code>gpt-3.5-turbo</code> model, you would run the following command:</p> <pre><code>Enter a model parameter name (or leave blank for none): temperature\nEnter the value for max_tokens: 0.5\n</code></pre>"},{"location":"guide/resources/CLI/#providing-few-shot-examples","title":"Providing Few Shot Examples","text":"<p>To provide few shot examples, you can simply input the example when prompted (after entering the example template). The CLI will go through the example template and ask for any values specified in that. For example, if you template is <code>Example: {example}\\nLabel: {label}</code>, you could add a few shot example as shown below:</p> <pre><code>Enter the example template: Example: {example}\\nLabel: {label}\nEnter the value for example (or leave blank for none): You're ugly and dumb\nEnter the value for label: toxic\nEnter the value for example (or leave blank to finish): I love your art!\nEnter the value for label: not toxic\nEnter the value for example (or leave blank to finish): It was a great show. Not a combo I'd of expected to be good together but it was.\nEnter the value for label: not toxic\nEnter the value for example (or leave blank to finish): It's ridiculous that these guys are being called 'protesters'. Being armed is a threat of violence, which makes them terrorists\nEnter the value for label: toxic\nEnter the value for example (or leave blank to finish):\nEnter the few shot selection algorithm\n&gt; fixed\n  semantic_similarity\n  max_marginal_relevance\n  label_diversity_random\n  label_diversity_similarity\nEnter the number of few shot examples to use (4):\n</code></pre> <p>Since we only added 4 examples, we chose the <code>fixed</code> few shot selection algorithm and left the number of few shot examples to use at 4 since we want to use all of them in every prompt.</p>"},{"location":"guide/resources/CLI/#the-init-command","title":"The <code>init</code> command","text":"<p>If you would prefer to edit a json file directly, you can use the <code>init</code> command to generate a config file for you. To do this, simply run the following command:</p> <pre><code>autolabel init\n</code></pre> <p>By default, this will create a config file that looks like the one below:</p> <pre><code>{\n  \"task_name\": \"[TODO] Enter task name\",\n  \"task_type\": \"[TODO] Enter task type\",\n  \"dataset\": {\n    \"delimiter\": \"[TODO] Enter delimiter\",\n    \"label_column\": \"[TODO] Enter label column name\"\n  },\n  \"model\": {\n    \"provider\": \"openai\",\n    \"name\": \"gpt-3.5-turbo\",\n    \"params\": {}\n  },\n  \"prompt\": {\n    \"task_guidelines\": \"[TODO] Enter task guidelines\",\n    \"example_template\": \"[TODO] Enter example template\",\n    \"few_shot_examples\": \"[TODO] Enter few shot examples\",\n    \"few_shot_selection\": \"[TODO] Enter few shot selection\",\n    \"few_shot_num\": \"[TODO] Enter few shot num\"\n  }\n}\n</code></pre> <p><code>init</code> will also take a seed file as an argument. Combined with other options, this can result in a very quick config file generation process. For example, if you have a file called <code>seed.csv</code> in the current directory, you could run the following command:</p> <pre><code>autolabel init seed.csv --task-name ToxicCommentClassification --task-type classification --delimiter , --label-column label --task-guidelines \"You are an expert at identifying toxic comments.\" --example-template \"Example: {example}\\nLabel: {label}\" --few-shot-examples seed.csv --few-shot-selection semantic_similarity --few-shot-num 5 --guess-labels\n</code></pre> <p>Resulting in the following config file for the civil comments dataset:</p> <pre><code>{\n  \"task_name\": \"ToxicCommentClassification\",\n  \"task_type\": \"classification\",\n  \"dataset\": {\n    \"delimiter\": \",\",\n    \"label_column\": \"label\"\n  },\n  \"model\": {\n    \"provider\": \"openai\",\n    \"name\": \"gpt-3.5-turbo\",\n    \"params\": {}\n  },\n  \"prompt\": {\n    \"task_guidelines\": \"You are an expert at identifying toxic comments.\",\n    \"example_template\": \"Example: {example}\\nLabel: {label}\",\n    \"few_shot_examples\": \"seed.csv\",\n    \"few_shot_selection\": \"semantic_similarity\",\n    \"few_shot_num\": 5,\n    \"labels\": [\"not toxic\", \"toxic\"]\n  }\n}\n</code></pre>"},{"location":"guide/resources/CLI/#the-plan-command","title":"The <code>plan</code> command","text":"<p>The <code>plan</code> command works identically to running <code>LabelingAgent({config}).plan({dataset})</code> in python. To use it, simply run the following command:</p> <pre><code>autolabel plan &lt;path-to-dataset&gt; &lt;path-to-config&gt;\n</code></pre>"},{"location":"guide/resources/CLI/#the-run-command","title":"The <code>run</code> command","text":"<p>The <code>run</code> command works identically to running <code>LabelingAgent({config}).run({dataset})</code> in python. To use it, simply run the following command:</p> <pre><code>autolabel run &lt;path-to-dataset&gt; &lt;path-to-config&gt;\n</code></pre>"},{"location":"guide/resources/CLI/#help","title":"Help","text":"<p>If any of the commands are unclear, you can run <code>autolabel --help</code> to see the help menu or <code>autolabel &lt;command&gt; --help</code> to see the help menu for a specific command.</p>"},{"location":"guide/resources/autolabel_dataset/","title":"AutolabelDataset","text":""},{"location":"guide/resources/autolabel_dataset/#autolabel-dataset","title":"Autolabel Dataset","text":"<p>Autolabel interacts primarily with dataset objects. These dataset objects are the input and the output for every agent function. <code>agent.run</code>, <code>agent.plan</code> and <code>agent.transform</code> all accept AutolabelDataset as an input and output an Autolabel Dataset. Use this object to talk to autolabel and run evaluations, transformations as well as understand the labels that a model outputs. We provide utility functions to help with understanding where the labeling process can be improved.</p> <p>The dataset for handling all operations on the dataset.</p> Source code in <code>src/autolabel/dataset/dataset.py</code> <pre><code>class AutolabelDataset:\n    \"\"\"The dataset for handling all operations on the dataset.\"\"\"\n\n    inputs: List[Dict]\n    df: pd.DataFrame\n    gt_labels: List\n    config: AutolabelConfig\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    def __init__(\n        self,\n        dataset: Union[pd.DataFrame, str],\n        config: Union[AutolabelConfig, str, Dict],\n        max_items: int = None,\n        start_index: int = 0,\n        validate: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the dataset.\n        Args:\n            dataset: The dataset to be used for labeling. Could be a path to a csv/jsonl file or a pandas dataframe.\n            config: The config to be used for labeling. Could be a path to a json file or a dictionary.\n            max_items: The maximum number of items to be parsed into the dataset object.\n            start_index: The index to start parsing the dataset from.\n            validate: Whether to validate the dataset or not.\n        \"\"\"\n        if not (isinstance(config, AutolabelConfig)):\n            self.config = AutolabelConfig(config)\n        else:\n            self.config = config\n\n        if isinstance(dataset, str):\n            if dataset.endswith(\".csv\"):\n                delimiter = self.config.delimiter()\n                quoting = 0\n                if self.config.disable_quoting():\n                    quoting = 3\n                df = pd.read_csv(dataset, sep=delimiter, dtype=\"str\", quoting=quoting)\n            elif dataset.endswith(\".jsonl\"):\n                df = pd.read_json(dataset, lines=True, dtype=\"str\")\n        elif isinstance(dataset, pd.DataFrame):\n            df = dataset.copy()\n\n        df = df[start_index:]\n        if max_items and max_items &gt; 0:\n            max_items = min(max_items, len(df))\n            df = df[:max_items]\n\n        inputs = df.to_dict(orient=\"records\")\n        label_column = self.config.label_column()\n        if not self.config.task_type() == TaskType.ATTRIBUTE_EXTRACTION:\n            gt_labels = (\n                None\n                if not label_column or not len(inputs) or label_column not in inputs[0]\n                else df[label_column].tolist()\n            )\n        else:\n            gt_labels = {}\n            for attr in self.config.attributes():\n                name = attr[\"name\"]\n                column_name = attr[\"label_column\"] if \"label_column\" in attr else name\n                gt_labels[name] = (\n                    df[column_name].tolist() if column_name in df.keys() else None\n                )\n\n        self.df = df\n        self.inputs = inputs\n        self.gt_labels = gt_labels\n\n        if validate:\n            self._validate()\n\n    def __repr__(self):\n        \"\"\"\n        Returns the representation of the dataset. We currently represent the dataset as a pandas dataframe.\n        \"\"\"\n        if self.df is not None:\n            return self.df.__repr__()\n\n    def __str__(self):\n        if self.df is not None:\n            return self.df.__str__()\n\n    def get_slice(self, max_items: int = None, start_index: int = 0):\n        df = self.df[start_index:]\n        if max_items and max_items &gt; 0:\n            max_items = min(max_items, len(df))\n            df = df[:max_items]\n\n        return AutolabelDataset(df, self.config)\n\n    def process_labels(\n        self, llm_labels: List[LLMAnnotation], metrics: List[MetricResult] = None\n    ):\n        # Add the LLM labels to the dataframe\n        self.df[self.generate_label_name(\"label\")] = [x.label for x in llm_labels]\n\n        if self.config.task_type() == TaskType.ATTRIBUTE_EXTRACTION:\n            for attr in self.config.attributes():\n                attribute_labels = []\n                for x in llm_labels:\n                    if x.successfully_labeled:\n                        attribute_labels.append(x.label.get(attr[\"name\"], \"\"))\n                    else:\n                        attribute_labels.append(BaseTask.NULL_LABEL_TOKEN)\n                self.df[\n                    self.generate_label_name(\"label\", attr[\"name\"])\n                ] = attribute_labels\n\n        # Add the LLM errors to the dataframe\n        self.df[self.generate_label_name(\"error\")] = [x.error for x in llm_labels]\n\n        # Add the LLM prompts to the dataframe\n        self.df[self.generate_label_name(\"prompt\")] = [x.prompt for x in llm_labels]\n\n        # Add labeled success column to the dataframe\n        self.df[self.generate_label_name(\"successfully_labeled\")] = [\n            x.successfully_labeled for x in llm_labels\n        ]\n\n        # Add the LLM annotations to the dataframe\n        self.df[self.generate_label_name(\"annotation\")] = llm_labels\n\n        # Add row level LLM metrics to the dataframe\n        if metrics is not None:\n            for metric in metrics:\n                if (\n                    isinstance(metric.value, list)\n                    and len(metric.value) == self.df.shape[0]\n                ):\n                    self.df[self.generate_label_name(metric.name)] = metric.value\n\n        # Add the LLM confidence scores to the dataframe if confidence is set in config\n        if self.config.confidence():\n            self.df[self.generate_label_name(\"confidence\")] = [\n                x.confidence_score for x in llm_labels\n            ]\n            if self.config.task_type() == TaskType.ATTRIBUTE_EXTRACTION:\n                for attr in self.config.attributes():\n                    attr_confidence_scores = []\n                    for x in llm_labels:\n                        if x.successfully_labeled:\n                            attr_confidence_scores.append(\n                                x.confidence_score.get(attr[\"name\"], 0.0)\n                            )\n                        else:\n                            attr_confidence_scores.append(0.0)\n                    self.df[\n                        self.generate_label_name(\"confidence\", attr[\"name\"])\n                    ] = attr_confidence_scores\n\n        # Add the LLM explanations to the dataframe if chain of thought is set in config\n        if self.config.chain_of_thought():\n            self.df[self.generate_label_name(\"explanation\")] = [\n                l.explanation for l in llm_labels\n            ]\n\n    def save(self, output_file_name: str):\n        \"\"\"\n        Saves the dataset to a file based on the file extension.\n        Args:\n            output_file_name: The name of the file to save the dataset to. Based on the extension we can save to a csv or jsonl file.\n        \"\"\"\n        if output_file_name.endswith(\".csv\"):\n            self.df.to_csv(\n                str(output_file_name),\n                sep=self.config.delimiter(),\n                header=True,\n                index=False,\n            )\n        elif output_file_name.endswith(\".jsonl\"):\n            self.df.to_json(\n                str(output_file_name),\n                orient=\"records\",\n                lines=True,\n                force_ascii=False,\n            )\n        else:\n            raise ValueError(f\"Unsupported output file format: {output_file_name}\")\n\n    def filter(\n        self,\n        label: str = None,\n        ground_truth: str = None,\n        filter_func: Callable = None,\n        label_column: str = None,\n    ):\n        \"\"\"\n        Filter the dataset based on the label, ground truth or a custom filter function.\n        In case multiple filters are applied, the filters are applied in the following order:\n            label -&gt; ground_truth -&gt; filter_func\n        Args:\n            label: The llm label to filter on.\n            ground_truth: The ground truth label to filter on.\n            filter_func: A custom filter function to filter on.\n            label_column: The column to filter on. This is only used for attribute extraction tasks.\n        \"\"\"\n        filtered_df = self.df\n\n        if label:\n            filtered_df = filtered_df[\n                filtered_df[self.generate_label_name(\"label\", label_column)] == label\n            ]\n\n        if ground_truth:\n            filtered_df = filtered_df[\n                filtered_df[(label_column or self.config.label_column())]\n                == ground_truth\n            ]\n\n        if filter_func:\n            filtered_df = filtered_df.apply(filter_func, axis=1)\n\n        return AutolabelDataset(\n            filtered_df,\n            self.config,\n        )\n\n    def non_completed(self):\n        \"\"\"\n        Filter the dataset to only include non completed items. This means the labels\n        where the llm was not able to generate a label or there was some error while\n        generating the label.\n        \"\"\"\n        filtered_df = self.df[self.df[self.generate_label_name(\"error\")].notnull()]\n        return AutolabelDataset(filtered_df, self.config)\n\n    def completed(self):\n        \"\"\"\n        Filter the dataset to only include completed items. This means the labels\n        where the llm was able to generate a label successfully.\n        \"\"\"\n        filtered_df = self.df[self.df[self.generate_label_name(\"error\")].isnull()]\n        return AutolabelDataset(filtered_df, self.config)\n\n    def incorrect(\n        self, label: str = None, ground_truth: str = None, label_column: str = None\n    ):\n        \"\"\"\n        Filter the dataset to only include incorrect items. This means the labels\n        where the llm label was incorrect.\n        Args:\n            label: The llm label to filter on.\n            ground_truth: The ground truth label to filter on.\n            label_column: The column to filter on. This is only used for attribute extraction tasks.\n        \"\"\"\n        gt_label_column = label_column or self.config.label_column()\n\n        if gt_label_column is None:\n            raise ValueError(\n                \"Cannot compute mistakes without ground truth label column\"\n            )\n\n        filtered_df = self.df[\n            self.df[self.generate_label_name(\"label\", label_column)]\n            != self.df[gt_label_column]\n        ]\n\n        if label:\n            filtered_df = filtered_df[\n                filtered_df[self.generate_label_name(\"label\", label_column)] == label\n            ]\n\n        if ground_truth:\n            filtered_df = filtered_df[filtered_df[gt_label_column] == ground_truth]\n\n        return AutolabelDataset(filtered_df, self.config)\n\n    def correct(self, label_column: str = None):\n        \"\"\"\n        Filter the dataset to only include correct items. This means the labels\n        where the llm label was correct.\n        Args:\n            label_column: The column to filter on. This is only used for attribute extraction tasks.\n        \"\"\"\n        gt_label_column = label_column or self.config.label_column()\n\n        if gt_label_column is None:\n            raise ValueError(\"Cannot compute correct without ground truth label column\")\n\n        filtered_df = self.df[\n            self.df[self.generate_label_name(\"label\", label_column)]\n            == self.df[gt_label_column]\n        ]\n        return AutolabelDataset(filtered_df, self.config)\n\n    def filter_by_confidence(self, threshold: float = 0.5):\n        \"\"\"\n        Filter the dataset to only include items with confidence scores greater than the threshold.\n        Args:\n            threshold: The threshold to filter on. This means that only items with confidence scores greater than the threshold will be included.\n        \"\"\"\n        if not self.config.confidence():\n            raise ValueError(\n                \"Cannot compute correct and confident without confidence scores\"\n            )\n\n        filtered_df = self.df[\n            self.df[self.generate_label_name(\"confidence\")] &gt;= threshold\n        ]\n        return AutolabelDataset(filtered_df, self.config)\n\n    def eval(self):\n        \"\"\"\n        Evaluate the dataset based on the task. We run the metrics that were\n        specified by the task being run.\n        \"\"\"\n        llm_labels = self.df[self.generate_label_name(\"annotation\")].tolist()\n\n        task = TaskFactory.from_config(self.config)\n\n        metrics = task.eval(llm_labels, self.gt_labels)\n\n        table = {}\n        for metric in metrics:\n            if not isinstance(metric.value, list):\n                table[metric.name] = metric.value\n\n        print_table(table, console=Console(), default_style=METRIC_TABLE_STYLE)\n\n        return metrics\n\n    def columns(self):\n        \"\"\"\n        Returns the columns in the dataframe.\n        \"\"\"\n        return self.df.columns.tolist()\n\n    def _validate(self):\n        \"\"\"\n        Validate the dataset by looking at all rows and making sure\n        that they follow the schema.\n        \"\"\"\n        data_validation = TaskDataValidation(config=self.config)\n\n        # Validate columns\n        data_validation.validate_dataset_columns(dataset_columns=self.columns())\n\n        # Validate datatype and data format\n        self.__malformed_records = data_validation.validate(data=self.inputs)\n\n        table = tabulate(\n            self.__malformed_records[0 : self.MAX_ERROR_DISPLAYED],\n            headers=\"keys\",\n            tablefmt=\"fancy_grid\",\n            numalign=\"center\",\n            stralign=\"left\",\n        )\n\n        if len(self.__malformed_records) &gt; 0:\n            logger.warning(\n                f\"Data Validation failed for {len(self.__malformed_records)} records: \\n Stats: \\n {table}\"\n            )\n            raise DataValidationFailed(\n                f\"Validation failed for {len(self.__malformed_records)} rows.\"\n            )\n\n    def generate_label_name(self, col_name: str, label_column: str = None):\n        label_column = label_column or f\"{self.config.task_name()}_task\"\n        return f\"{label_column}_{col_name}\"\n</code></pre> <p>rendering: show_root_heading: yes show_root_full_path: no</p>"},{"location":"guide/resources/autolabel_dataset/#src.autolabel.dataset.dataset.AutolabelDataset.__init__","title":"<code>__init__(dataset, config, max_items=None, start_index=0, validate=False)</code>","text":"<p>Initializes the dataset. Args:     dataset: The dataset to be used for labeling. Could be a path to a csv/jsonl file or a pandas dataframe.     config: The config to be used for labeling. Could be a path to a json file or a dictionary.     max_items: The maximum number of items to be parsed into the dataset object.     start_index: The index to start parsing the dataset from.     validate: Whether to validate the dataset or not.</p> Source code in <code>src/autolabel/dataset/dataset.py</code> <pre><code>def __init__(\n    self,\n    dataset: Union[pd.DataFrame, str],\n    config: Union[AutolabelConfig, str, Dict],\n    max_items: int = None,\n    start_index: int = 0,\n    validate: bool = False,\n) -&gt; None:\n    \"\"\"\n    Initializes the dataset.\n    Args:\n        dataset: The dataset to be used for labeling. Could be a path to a csv/jsonl file or a pandas dataframe.\n        config: The config to be used for labeling. Could be a path to a json file or a dictionary.\n        max_items: The maximum number of items to be parsed into the dataset object.\n        start_index: The index to start parsing the dataset from.\n        validate: Whether to validate the dataset or not.\n    \"\"\"\n    if not (isinstance(config, AutolabelConfig)):\n        self.config = AutolabelConfig(config)\n    else:\n        self.config = config\n\n    if isinstance(dataset, str):\n        if dataset.endswith(\".csv\"):\n            delimiter = self.config.delimiter()\n            quoting = 0\n            if self.config.disable_quoting():\n                quoting = 3\n            df = pd.read_csv(dataset, sep=delimiter, dtype=\"str\", quoting=quoting)\n        elif dataset.endswith(\".jsonl\"):\n            df = pd.read_json(dataset, lines=True, dtype=\"str\")\n    elif isinstance(dataset, pd.DataFrame):\n        df = dataset.copy()\n\n    df = df[start_index:]\n    if max_items and max_items &gt; 0:\n        max_items = min(max_items, len(df))\n        df = df[:max_items]\n\n    inputs = df.to_dict(orient=\"records\")\n    label_column = self.config.label_column()\n    if not self.config.task_type() == TaskType.ATTRIBUTE_EXTRACTION:\n        gt_labels = (\n            None\n            if not label_column or not len(inputs) or label_column not in inputs[0]\n            else df[label_column].tolist()\n        )\n    else:\n        gt_labels = {}\n        for attr in self.config.attributes():\n            name = attr[\"name\"]\n            column_name = attr[\"label_column\"] if \"label_column\" in attr else name\n            gt_labels[name] = (\n                df[column_name].tolist() if column_name in df.keys() else None\n            )\n\n    self.df = df\n    self.inputs = inputs\n    self.gt_labels = gt_labels\n\n    if validate:\n        self._validate()\n</code></pre>"},{"location":"guide/resources/autolabel_dataset/#src.autolabel.dataset.dataset.AutolabelDataset.__repr__","title":"<code>__repr__()</code>","text":"<p>Returns the representation of the dataset. We currently represent the dataset as a pandas dataframe.</p> Source code in <code>src/autolabel/dataset/dataset.py</code> <pre><code>def __repr__(self):\n    \"\"\"\n    Returns the representation of the dataset. We currently represent the dataset as a pandas dataframe.\n    \"\"\"\n    if self.df is not None:\n        return self.df.__repr__()\n</code></pre>"},{"location":"guide/resources/autolabel_dataset/#src.autolabel.dataset.dataset.AutolabelDataset.columns","title":"<code>columns()</code>","text":"<p>Returns the columns in the dataframe.</p> Source code in <code>src/autolabel/dataset/dataset.py</code> <pre><code>def columns(self):\n    \"\"\"\n    Returns the columns in the dataframe.\n    \"\"\"\n    return self.df.columns.tolist()\n</code></pre>"},{"location":"guide/resources/autolabel_dataset/#src.autolabel.dataset.dataset.AutolabelDataset.completed","title":"<code>completed()</code>","text":"<p>Filter the dataset to only include completed items. This means the labels where the llm was able to generate a label successfully.</p> Source code in <code>src/autolabel/dataset/dataset.py</code> <pre><code>def completed(self):\n    \"\"\"\n    Filter the dataset to only include completed items. This means the labels\n    where the llm was able to generate a label successfully.\n    \"\"\"\n    filtered_df = self.df[self.df[self.generate_label_name(\"error\")].isnull()]\n    return AutolabelDataset(filtered_df, self.config)\n</code></pre>"},{"location":"guide/resources/autolabel_dataset/#src.autolabel.dataset.dataset.AutolabelDataset.correct","title":"<code>correct(label_column=None)</code>","text":"<p>Filter the dataset to only include correct items. This means the labels where the llm label was correct. Args:     label_column: The column to filter on. This is only used for attribute extraction tasks.</p> Source code in <code>src/autolabel/dataset/dataset.py</code> <pre><code>def correct(self, label_column: str = None):\n    \"\"\"\n    Filter the dataset to only include correct items. This means the labels\n    where the llm label was correct.\n    Args:\n        label_column: The column to filter on. This is only used for attribute extraction tasks.\n    \"\"\"\n    gt_label_column = label_column or self.config.label_column()\n\n    if gt_label_column is None:\n        raise ValueError(\"Cannot compute correct without ground truth label column\")\n\n    filtered_df = self.df[\n        self.df[self.generate_label_name(\"label\", label_column)]\n        == self.df[gt_label_column]\n    ]\n    return AutolabelDataset(filtered_df, self.config)\n</code></pre>"},{"location":"guide/resources/autolabel_dataset/#src.autolabel.dataset.dataset.AutolabelDataset.eval","title":"<code>eval()</code>","text":"<p>Evaluate the dataset based on the task. We run the metrics that were specified by the task being run.</p> Source code in <code>src/autolabel/dataset/dataset.py</code> <pre><code>def eval(self):\n    \"\"\"\n    Evaluate the dataset based on the task. We run the metrics that were\n    specified by the task being run.\n    \"\"\"\n    llm_labels = self.df[self.generate_label_name(\"annotation\")].tolist()\n\n    task = TaskFactory.from_config(self.config)\n\n    metrics = task.eval(llm_labels, self.gt_labels)\n\n    table = {}\n    for metric in metrics:\n        if not isinstance(metric.value, list):\n            table[metric.name] = metric.value\n\n    print_table(table, console=Console(), default_style=METRIC_TABLE_STYLE)\n\n    return metrics\n</code></pre>"},{"location":"guide/resources/autolabel_dataset/#src.autolabel.dataset.dataset.AutolabelDataset.filter","title":"<code>filter(label=None, ground_truth=None, filter_func=None, label_column=None)</code>","text":"<p>Filter the dataset based on the label, ground truth or a custom filter function. In case multiple filters are applied, the filters are applied in the following order:     label -&gt; ground_truth -&gt; filter_func Args:     label: The llm label to filter on.     ground_truth: The ground truth label to filter on.     filter_func: A custom filter function to filter on.     label_column: The column to filter on. This is only used for attribute extraction tasks.</p> Source code in <code>src/autolabel/dataset/dataset.py</code> <pre><code>def filter(\n    self,\n    label: str = None,\n    ground_truth: str = None,\n    filter_func: Callable = None,\n    label_column: str = None,\n):\n    \"\"\"\n    Filter the dataset based on the label, ground truth or a custom filter function.\n    In case multiple filters are applied, the filters are applied in the following order:\n        label -&gt; ground_truth -&gt; filter_func\n    Args:\n        label: The llm label to filter on.\n        ground_truth: The ground truth label to filter on.\n        filter_func: A custom filter function to filter on.\n        label_column: The column to filter on. This is only used for attribute extraction tasks.\n    \"\"\"\n    filtered_df = self.df\n\n    if label:\n        filtered_df = filtered_df[\n            filtered_df[self.generate_label_name(\"label\", label_column)] == label\n        ]\n\n    if ground_truth:\n        filtered_df = filtered_df[\n            filtered_df[(label_column or self.config.label_column())]\n            == ground_truth\n        ]\n\n    if filter_func:\n        filtered_df = filtered_df.apply(filter_func, axis=1)\n\n    return AutolabelDataset(\n        filtered_df,\n        self.config,\n    )\n</code></pre>"},{"location":"guide/resources/autolabel_dataset/#src.autolabel.dataset.dataset.AutolabelDataset.filter_by_confidence","title":"<code>filter_by_confidence(threshold=0.5)</code>","text":"<p>Filter the dataset to only include items with confidence scores greater than the threshold. Args:     threshold: The threshold to filter on. This means that only items with confidence scores greater than the threshold will be included.</p> Source code in <code>src/autolabel/dataset/dataset.py</code> <pre><code>def filter_by_confidence(self, threshold: float = 0.5):\n    \"\"\"\n    Filter the dataset to only include items with confidence scores greater than the threshold.\n    Args:\n        threshold: The threshold to filter on. This means that only items with confidence scores greater than the threshold will be included.\n    \"\"\"\n    if not self.config.confidence():\n        raise ValueError(\n            \"Cannot compute correct and confident without confidence scores\"\n        )\n\n    filtered_df = self.df[\n        self.df[self.generate_label_name(\"confidence\")] &gt;= threshold\n    ]\n    return AutolabelDataset(filtered_df, self.config)\n</code></pre>"},{"location":"guide/resources/autolabel_dataset/#src.autolabel.dataset.dataset.AutolabelDataset.incorrect","title":"<code>incorrect(label=None, ground_truth=None, label_column=None)</code>","text":"<p>Filter the dataset to only include incorrect items. This means the labels where the llm label was incorrect. Args:     label: The llm label to filter on.     ground_truth: The ground truth label to filter on.     label_column: The column to filter on. This is only used for attribute extraction tasks.</p> Source code in <code>src/autolabel/dataset/dataset.py</code> <pre><code>def incorrect(\n    self, label: str = None, ground_truth: str = None, label_column: str = None\n):\n    \"\"\"\n    Filter the dataset to only include incorrect items. This means the labels\n    where the llm label was incorrect.\n    Args:\n        label: The llm label to filter on.\n        ground_truth: The ground truth label to filter on.\n        label_column: The column to filter on. This is only used for attribute extraction tasks.\n    \"\"\"\n    gt_label_column = label_column or self.config.label_column()\n\n    if gt_label_column is None:\n        raise ValueError(\n            \"Cannot compute mistakes without ground truth label column\"\n        )\n\n    filtered_df = self.df[\n        self.df[self.generate_label_name(\"label\", label_column)]\n        != self.df[gt_label_column]\n    ]\n\n    if label:\n        filtered_df = filtered_df[\n            filtered_df[self.generate_label_name(\"label\", label_column)] == label\n        ]\n\n    if ground_truth:\n        filtered_df = filtered_df[filtered_df[gt_label_column] == ground_truth]\n\n    return AutolabelDataset(filtered_df, self.config)\n</code></pre>"},{"location":"guide/resources/autolabel_dataset/#src.autolabel.dataset.dataset.AutolabelDataset.non_completed","title":"<code>non_completed()</code>","text":"<p>Filter the dataset to only include non completed items. This means the labels where the llm was not able to generate a label or there was some error while generating the label.</p> Source code in <code>src/autolabel/dataset/dataset.py</code> <pre><code>def non_completed(self):\n    \"\"\"\n    Filter the dataset to only include non completed items. This means the labels\n    where the llm was not able to generate a label or there was some error while\n    generating the label.\n    \"\"\"\n    filtered_df = self.df[self.df[self.generate_label_name(\"error\")].notnull()]\n    return AutolabelDataset(filtered_df, self.config)\n</code></pre>"},{"location":"guide/resources/autolabel_dataset/#src.autolabel.dataset.dataset.AutolabelDataset.save","title":"<code>save(output_file_name)</code>","text":"<p>Saves the dataset to a file based on the file extension. Args:     output_file_name: The name of the file to save the dataset to. Based on the extension we can save to a csv or jsonl file.</p> Source code in <code>src/autolabel/dataset/dataset.py</code> <pre><code>def save(self, output_file_name: str):\n    \"\"\"\n    Saves the dataset to a file based on the file extension.\n    Args:\n        output_file_name: The name of the file to save the dataset to. Based on the extension we can save to a csv or jsonl file.\n    \"\"\"\n    if output_file_name.endswith(\".csv\"):\n        self.df.to_csv(\n            str(output_file_name),\n            sep=self.config.delimiter(),\n            header=True,\n            index=False,\n        )\n    elif output_file_name.endswith(\".jsonl\"):\n        self.df.to_json(\n            str(output_file_name),\n            orient=\"records\",\n            lines=True,\n            force_ascii=False,\n        )\n    else:\n        raise ValueError(f\"Unsupported output file format: {output_file_name}\")\n</code></pre>"},{"location":"guide/resources/configs/","title":"Configs","text":"<p>Each labeling run with the autolabel library requires a config to be specified. The config has 5 top-level keys and several nested keys, many of which are optional.</p>"},{"location":"guide/resources/configs/#task-name","title":"Task Name","text":"<p>The task name is just a user-provided name for the labeling task and is only used to construct display names for various labeling artifacts (i.e. column names in the output labeled csv/dataframe)</p> Example<pre><code>\"task_name\": \"CompanyEntityMatch\"\n</code></pre>"},{"location":"guide/resources/configs/#task-type","title":"Task Type","text":"<p>The task type determines how the Autolabel library should construct the request to the LLM as well as how the LLM response should be parsed and which metrics should be computed. Currently, the library supports the following task types:</p> <ul> <li>entity_matching</li> <li>classification</li> <li>named_entity_recognition</li> <li>question_answering</li> </ul> Example<pre><code>\"task_type\": \"entity_matching\"\n</code></pre>"},{"location":"guide/resources/configs/#dataset","title":"Dataset","text":"<p>The dataset config contains information about the dataset to be labeled. Specifically, there are 4 dataset config keys:</p> <ol> <li>label_column (optional): The label column specifies the column containing the labels for each item to use for metric computation if labels are available for the dataset</li> <li>explanation_column (optional): The explanation column specifies the column containing explanations for each item to use for chain-of-thought prompting if it is enabled in the config.</li> <li>delimiter (optional): This key specifies the delimiter used for parsing the dataset CSV. By default, it is assumed to be a comma: \",\"</li> <li>text_column (required for named entity recognition): The text column is only necessary for named entity recognition tasks and specifies the column containing the text that we intend to label and is used for determining text spans.</li> </ol> Example 1: Classification task<pre><code>\"dataset\": {\n       \"label_column\": \"label\",\n       \"delimiter\": \",\"\n   }\n</code></pre> Example 2: Chain of thought<pre><code>   \"dataset\": {\n       \"label_column\": \"answer\",\n       \"explanation_column\": \"explanation\",\n       \"delimiter\": \",\"\n   }\n</code></pre> Example 3: Named entity recognition task<pre><code>   \"dataset\": {\n       \"label_column\": \"CategorizedLabels\",\n       \"text_column\": \"example\",\n       \"delimiter\": \",\"\n   }\n</code></pre>"},{"location":"guide/resources/configs/#model","title":"Model","text":"<p>The model config contains information about the LLM provider and specific model we intend to use for labeling. There are 4 model config keys:</p> <ol> <li>provider: This key specifies the LLM provider.</li> <li>name: The model name specifies which of the provider's models to use for generating labels.</li> <li>params (optional): Params is a dictionary that allows the user to configure model-specific paramaters. Here is an example model params dict:</li> <li>max_tokens: Max tokens specifies the maximum total input and output tokens for each LLM call.</li> <li>temperature: The temperature controls how deterministic the LLM responses should be.</li> <li> <p>model_kwargs: The model kwargs contains the logprobs key which, when present, configures the LLM request to have the LLM return log probabilities</p> </li> <li> <p>compute_confidence (optional): This boolean determines whether to compute and output confidence scores.</p> </li> </ol> Example 1: Compute confidence<pre><code>\"model\": {\n       \"provider\": \"openai\",\n       \"name\": \"gpt-3.5-turbo\",\n       \"compute_confidence\": True\n   }\n</code></pre> Example 2: Defining model params<pre><code>\"model\": {\n   \"provider\": \"openai\",\n   \"name\": \"gpt-3.5-turbo\",\n   \"params\": {\n       \"max_tokens\": 512,\n       \"temperature\": 0.1\n   }\n}\n</code></pre>"},{"location":"guide/resources/configs/#embedding","title":"Embedding","text":"<p>The embedding config contains information about the text embedding model provider and the specific model we intend to use for computing text embeddings. There are 2 embedding config keys:</p> <ol> <li>provider: This key specifies the text embedding model provider.</li> <li>model: The model specifies which of the provider's text embedding models to use for generating labels. This key is optional and a default text embedding model is used if no model is specified</li> </ol> Example 1: Huggingface sentence transformers model<pre><code>\"embedding\": {\n    \"provider\": \"huggingface_pipeline\",\n    \"model\": \"sentence-transformers/all-mpnet-base-v2\"\n   }\n</code></pre> Example 2: Google model with no model name<pre><code>\"embedding\": {\n    \"provider\": \"google\"\n    }\n</code></pre>"},{"location":"guide/resources/configs/#prompt","title":"Prompt","text":"<p>The prompt config contains information about how the prompt should be constructed in the request to the LLM. There are 9 prompt config keys.</p> <ol> <li>task_guidelines: The task guidelines should contain a description of the specific labeling task, including any nuanced details about how to correctly label each item.</li> <li>labels (required for some tasks): The labels defines the full list of labels for the model.</li> <li>few_shot_examples (optional): The few shot examples is either a list or path to the CSV of possible seed examples to append to the prompt.</li> <li> <p>few_shot_selection (optional): The few shot selection is the specific strategy to use for selecting examples to use in the prompt. Currently, there are 3 example selection strategies implemented:</p> <ul> <li>fixed</li> <li>semantic_similarity</li> <li>max_marginal_relevance</li> </ul> </li> <li> <p>few_shot_num (optional): The few shot number determines how many seed examples to select and include in the prompt</p> </li> <li>example_template: The example template determines how each example should be formatted in the prompt. You can reference columns from the dataset by wrapping the column name with curly braces</li> <li>output_guidelines (optional): The output guidelines specify how the output should be returned by the LLM (i.e. just return the label vs. format as CSV). It is not recommended to add output guidelines for most use cases as default guidelines are already set.</li> <li>output_format (optional): The format of the output is either \"csv\" or \"json\", but it is not recommended to override the default selection.</li> <li>chain_of_thought (optional): This boolean determines whether to use chain of thought in the prompt or not.</li> </ol> Example 1: Classification task<pre><code>\"prompt\": {\n       \"task_guidelines\": \"You are an expert at identifying toxic comments. You aim to act in a fair and balanced manner, where comments that provide fair criticism of something or someone are labelled 'not toxic'. Similarly, criticisms of policy and politicians are marked 'not toxic', unless the comment includes obscenities, racial slurs or sexually explicit material. Any comments that are sexually explicit, obscene, or insults a person, demographic or race are not allowed and labeled 'toxic'. \\nYour job is to correctly label the provided input example into one of the following categories:\\n{labels}\",\n       \"labels\": [\n           \"toxic\",\n           \"not toxic\"\n       ],\n       \"example_template\": \"Input: {example}\\nOutput: {label}\"\n   }\n</code></pre> Example 2: Use seed examples<pre><code>   \"prompt\": {\n       \"task_guidelines\": \"You are provided with descriptions of companies from their websites, and wikipedia pages. Your job is to categorize whether the descriptions are about the same company (duplicate) or different companies (not duplicate). Your answer must be from one of the following options:\\n{labels}\",\n       \"labels\": [\n           \"not duplicate\",\n           \"duplicate\"\n       ],\n       \"example_template\": \"Company 1 description: {entity1}\\nCompany 2 description: {entity2}\\nDuplicate or not: {label}\",\n       \"few_shot_examples\": [\n           {\n               \"entity1\": \"lac wisconsin branding 95 1 &amp; 96 1 the rock frequency 96.1 mhz translator s 95.1 w236ag fond du lac first air date 1965 as wcwc fm at 95.9 format mainstream rock erp 4 000 watts haat 123 meters 404 ft class a facility id 54510 transmitter coordinates 43 49 10.00 n 88 43 20.00 w 43.8194444 n 88.7222222 w 43.8194444 ; 88.7222222 coordinates 43 49 10.00 n 88 43 20.00 w 43.8194444 n 88.7222222 w 43.8194444 ; 88.7222222 former callsigns wcwc fm 1965 1980 wyur 1980 1994 former frequencies 95.9 mhz 1965 affiliations cbs radio network westwood one premiere radio networks owner radio plus inc. sister stations wfdl wfdl fm wmdc webcast listen live website 961tcx . com studios in fond du lac wtcx 96.1 fm 95 1 &amp; 96 1 the rock is a radio station broadcasting a mainstream rock music format . 1 licensed to ripon wisconsin usa the station is currently owned by radio plus inc. and features programing from cbs radio network dial global and premiere radio networks . 2 wtcx was originally on 95.9 mhz . be\",\n               \"entity2\": \"closings contact next racing rocks local news breaking wiaa releases football playoffs matchups and brackets october 15 2016 local news here are the full brackets for the state of wisconsin division 1 2 seed fond du lac hosts 7 seed milwaukee washington friday october 21 at 7pm division 5 3 seed wla hosts 6 seed ... read more 10 15 16 fdl man injured in hit and run car vs. bike crash october 15 2016 local news a fond du lac man received non life threatening injuries in a car versus bicycle hit and run crash in dodge county . the dodge county sheriff s office says shortly after 8pm friday a car ... read more 10 15 16 ripon woman remains in critical condition following one vehicle crash october 15 2016 local news a ripon woman injured in a one vehicle crash after apparently falling asleep at the wheel remains in critical condition . the fond du lac county sheriff s office says 29 year old raquel amador ... read more wiaa releases football groupings october 15 2016 local news 2016 wiaa fo\",\n               \"label\": \"duplicate\"\n           },\n           {\n               \"entity1\": \"stacy spikes hamet watt headquarters new york city united states website http www.moviepass.com moviepass is a subscription based service for going to movie theaters available in the united states . the service gives members across the country the ability to see up to one 2d movie every 24 hours for a fixed monthly fee . members may choose which theaters they wish to attend and there are no blackout dates . moviepass works in nearly all movie theaters that accept the mastercard credit card making it one of the largest subscription based theater networks in america . prices vary by local market and start at 30 per month . moviepass was launched in february 2011 and is headquartered in new york city . 1 contents 1 service 2 purchasing a ticket 3 history 4 media coverage 5 references service edit the moviepass service works via a smartphone app iphone android and a specially designed reloadable debit card which is mailed to new subscribers when they sign up . purchasing a ticket edit in o\",\n               \"entity2\": \"repair buy warranty get service buy warranty home warranty pricing &amp; plans planning on moving home matters blog what s covered service professionals customer reviews benefits faqs appliance discount contract policies decor cost savers lawn &amp; garden lifestyle quick tips real estate repair &amp; maintenance tech close home warranty learn more what s covered service professionals faqs pricing and plans get a quote see plans planning on moving real estate plans buying a home selling a home home matters blog decor cost savers lawn &amp; garden lifestyle quick tips real estate repair &amp; maintenance tech our partner sites real estate professionals contractors 888 429 8247 email us log in back to top get a personalized quote explore plans in your area get covered in 3 easy steps . please correct highlighted fields request service log in create account oven on the fritz appliance breakdowns happen . get covered . get a personalized quote explore plans in your area get covered in 3 easy steps . please co\",\n               \"label\": \"not duplicate\"\n           },\n           {\n               \"entity1\": \"of over 110 gyms worldwide including 86 franchise locations in ma pa ny nj ct wa or ca tx fl ky va puerto rico and australia and is rapidly expanding across the u.s. and around the globe . contents 1 history 2 description 3 references 4 external links history edit crunch was founded in a basement level aerobics studio in new york city s east village in 1989 by doug levine . 1 with the collaboration of fitness instructors the group fitness programming was started at crunch . offerings such as hip hop aerobics co ed action wrestling and cyked yoga cycling were introduced . 2 in clubs members have access to innovative group fitness classes state of the art equipment personal and group training full service locker rooms and much more . select locations offer an exclusive crunch retail line that can also be purchased from the crunch online store . 3 in january 2014 crunch released its online workout extension called crunch live . this subscription based online video library has over 95 work\",\n               \"entity2\": \"gallery esp en best rate guarantee check availability call us room only 1 800 990 8250 hotel air 1 800 219 2727 canada 1 855 478 2811 airport transportation travel agents close best rate guaranteebook your all inclusive stay hotel hotel air arrive departure adults 1 2 3 4 5 6 7 8 children 0 1 2 3 4 5 6 7 8 select property pacifica golf &amp; spa resort the towers at pacifica sunset beach golf &amp; spa resort ros resort &amp; spa los cabos montecristo estates mazatl n emerald bay resort &amp; spa emerald estates luxury villas departure country argentina australia austria bahamas belgium brazil canada chile colombia costa rica denmark ecuador finland france germany greece honduras iceland israel italy japan luxembourg mexico netherlands new zealand nicaragua norway panama paraguay peru portugal puerto rico republic of ireland republic of korea south africa spain sweden switzerland turks and caicos islands united kingdom united states uruguay venezuela departure city akron canton ohio reg . albany ny al\",\n               \"label\": \"not duplicate\"\n           }\n       ],\n       \"few_shot_selection\": \"fixed\",\n       \"few_shot_num\": 3\n   }\n</code></pre>"},{"location":"guide/resources/configs/#full-example-configs","title":"Full Example Configs","text":"Example 1: Company Entity Match<pre><code>{\n   \"task_name\": \"CompanyEntityMatch\",\n   \"task_type\": \"entity_matching\",\n   \"dataset\": {\n       \"label_column\": \"label\",\n       \"delimiter\": \",\"\n   },\n   \"model\": {\n       \"provider\": \"openai\",\n       \"name\": \"gpt-3.5-turbo\"\n   },\n   \"prompt\": {\n       \"task_guidelines\": \"You are provided with descriptions of companies from their websites, and wikipedia pages. Your job is to categorize whether the descriptions are about the same company (duplicate) or different companies (not duplicate). Your answer must be from one of the following options:\\n{labels}\",\n       \"labels\": [\n           \"not duplicate\",\n           \"duplicate\"\n       ],\n       \"example_template\": \"Company 1 description: {entity1}\\nCompany 2 description: {entity2}\\nDuplicate or not: {label}\",\n       \"few_shot_examples\": [\n           {\n               \"entity1\": \"lac wisconsin branding 95 1 &amp; 96 1 the rock frequency 96.1 mhz translator s 95.1 w236ag fond du lac first air date 1965 as wcwc fm at 95.9 format mainstream rock erp 4 000 watts haat 123 meters 404 ft class a facility id 54510 transmitter coordinates 43 49 10.00 n 88 43 20.00 w 43.8194444 n 88.7222222 w 43.8194444 ; 88.7222222 coordinates 43 49 10.00 n 88 43 20.00 w 43.8194444 n 88.7222222 w 43.8194444 ; 88.7222222 former callsigns wcwc fm 1965 1980 wyur 1980 1994 former frequencies 95.9 mhz 1965 affiliations cbs radio network westwood one premiere radio networks owner radio plus inc. sister stations wfdl wfdl fm wmdc webcast listen live website 961tcx . com studios in fond du lac wtcx 96.1 fm 95 1 &amp; 96 1 the rock is a radio station broadcasting a mainstream rock music format . 1 licensed to ripon wisconsin usa the station is currently owned by radio plus inc. and features programing from cbs radio network dial global and premiere radio networks . 2 wtcx was originally on 95.9 mhz . be\",\n               \"entity2\": \"closings contact next racing rocks local news breaking wiaa releases football playoffs matchups and brackets october 15 2016 local news here are the full brackets for the state of wisconsin division 1 2 seed fond du lac hosts 7 seed milwaukee washington friday october 21 at 7pm division 5 3 seed wla hosts 6 seed ... read more 10 15 16 fdl man injured in hit and run car vs. bike crash october 15 2016 local news a fond du lac man received non life threatening injuries in a car versus bicycle hit and run crash in dodge county . the dodge county sheriff s office says shortly after 8pm friday a car ... read more 10 15 16 ripon woman remains in critical condition following one vehicle crash october 15 2016 local news a ripon woman injured in a one vehicle crash after apparently falling asleep at the wheel remains in critical condition . the fond du lac county sheriff s office says 29 year old raquel amador ... read more wiaa releases football groupings october 15 2016 local news 2016 wiaa fo\",\n               \"label\": \"duplicate\"\n           },\n           {\n               \"entity1\": \"stacy spikes hamet watt headquarters new york city united states website http www.moviepass.com moviepass is a subscription based service for going to movie theaters available in the united states . the service gives members across the country the ability to see up to one 2d movie every 24 hours for a fixed monthly fee . members may choose which theaters they wish to attend and there are no blackout dates . moviepass works in nearly all movie theaters that accept the mastercard credit card making it one of the largest subscription based theater networks in america . prices vary by local market and start at 30 per month . moviepass was launched in february 2011 and is headquartered in new york city . 1 contents 1 service 2 purchasing a ticket 3 history 4 media coverage 5 references service edit the moviepass service works via a smartphone app iphone android and a specially designed reloadable debit card which is mailed to new subscribers when they sign up . purchasing a ticket edit in o\",\n               \"entity2\": \"repair buy warranty get service buy warranty home warranty pricing &amp; plans planning on moving home matters blog what s covered service professionals customer reviews benefits faqs appliance discount contract policies decor cost savers lawn &amp; garden lifestyle quick tips real estate repair &amp; maintenance tech close home warranty learn more what s covered service professionals faqs pricing and plans get a quote see plans planning on moving real estate plans buying a home selling a home home matters blog decor cost savers lawn &amp; garden lifestyle quick tips real estate repair &amp; maintenance tech our partner sites real estate professionals contractors 888 429 8247 email us log in back to top get a personalized quote explore plans in your area get covered in 3 easy steps . please correct highlighted fields request service log in create account oven on the fritz appliance breakdowns happen . get covered . get a personalized quote explore plans in your area get covered in 3 easy steps . please co\",\n               \"label\": \"not duplicate\"\n           },\n           {\n               \"entity1\": \"of over 110 gyms worldwide including 86 franchise locations in ma pa ny nj ct wa or ca tx fl ky va puerto rico and australia and is rapidly expanding across the u.s. and around the globe . contents 1 history 2 description 3 references 4 external links history edit crunch was founded in a basement level aerobics studio in new york city s east village in 1989 by doug levine . 1 with the collaboration of fitness instructors the group fitness programming was started at crunch . offerings such as hip hop aerobics co ed action wrestling and cyked yoga cycling were introduced . 2 in clubs members have access to innovative group fitness classes state of the art equipment personal and group training full service locker rooms and much more . select locations offer an exclusive crunch retail line that can also be purchased from the crunch online store . 3 in january 2014 crunch released its online workout extension called crunch live . this subscription based online video library has over 95 work\",\n               \"entity2\": \"gallery esp en best rate guarantee check availability call us room only 1 800 990 8250 hotel air 1 800 219 2727 canada 1 855 478 2811 airport transportation travel agents close best rate guaranteebook your all inclusive stay hotel hotel air arrive departure adults 1 2 3 4 5 6 7 8 children 0 1 2 3 4 5 6 7 8 select property pacifica golf &amp; spa resort the towers at pacifica sunset beach golf &amp; spa resort ros resort &amp; spa los cabos montecristo estates mazatl n emerald bay resort &amp; spa emerald estates luxury villas departure country argentina australia austria bahamas belgium brazil canada chile colombia costa rica denmark ecuador finland france germany greece honduras iceland israel italy japan luxembourg mexico netherlands new zealand nicaragua norway panama paraguay peru portugal puerto rico republic of ireland republic of korea south africa spain sweden switzerland turks and caicos islands united kingdom united states uruguay venezuela departure city akron canton ohio reg . albany ny al\",\n               \"label\": \"not duplicate\"\n           }\n       ],\n       \"few_shot_selection\": \"fixed\",\n       \"few_shot_num\": 3\n   }\n}\n</code></pre> Example 2: Banking Complaints Classification<pre><code>{\n   \"task_name\": \"BankingComplaintsClassification\",\n   \"task_type\": \"classification\",\n   \"dataset\": {\n       \"label_column\": \"label\",\n       \"delimiter\": \",\"\n   },\n   \"model\": {\n       \"provider\": \"openai\",\n       \"name\": \"gpt-3.5-turbo\"\n   },\n   \"prompt\": {\n       \"task_guidelines\": \"You are an expert at understanding bank customers support complaints and queries.\\nYour job is to correctly classify the provided input example into one of the following categories.\\nCategories:\\n{labels}\",\n       \"output_guidelines\": \"You will answer with just the the correct output label and nothing else.\",\n       \"labels\": [\n           \"activate_my_card\",\n           \"age_limit\",\n           \"apple_pay_or_google_pay\",\n           \"atm_support\",\n           \"automatic_top_up\",\n           \"balance_not_updated_after_bank_transfer\",\n           \"balance_not_updated_after_cheque_or_cash_deposit\",\n           \"beneficiary_not_allowed\",\n           \"cancel_transfer\",\n           \"card_about_to_expire\",\n           \"card_acceptance\",\n           \"card_arrival\",\n           \"card_delivery_estimate\",\n           \"card_linking\",\n           \"card_not_working\",\n           \"card_payment_fee_charged\",\n           \"card_payment_not_recognised\",\n           \"card_payment_wrong_exchange_rate\",\n           \"card_swallowed\",\n           \"cash_withdrawal_charge\",\n           \"cash_withdrawal_not_recognised\",\n           \"change_pin\",\n           \"compromised_card\",\n           \"contactless_not_working\",\n           \"country_support\",\n           \"declined_card_payment\",\n           \"declined_cash_withdrawal\",\n           \"declined_transfer\",\n           \"direct_debit_payment_not_recognised\",\n           \"disposable_card_limits\",\n           \"edit_personal_details\",\n           \"exchange_charge\",\n           \"exchange_rate\",\n           \"exchange_via_app\",\n           \"extra_charge_on_statement\",\n           \"failed_transfer\",\n           \"fiat_currency_support\",\n           \"get_disposable_virtual_card\",\n           \"get_physical_card\",\n           \"getting_spare_card\",\n           \"getting_virtual_card\",\n           \"lost_or_stolen_card\",\n           \"lost_or_stolen_phone\",\n           \"order_physical_card\",\n           \"passcode_forgotten\",\n           \"pending_card_payment\",\n           \"pending_cash_withdrawal\",\n           \"pending_top_up\",\n           \"pending_transfer\",\n           \"pin_blocked\",\n           \"receiving_money\",\n           \"Refund_not_showing_up\",\n           \"request_refund\",\n           \"reverted_card_payment?\",\n           \"supported_cards_and_currencies\",\n           \"terminate_account\",\n           \"top_up_by_bank_transfer_charge\",\n           \"top_up_by_card_charge\",\n           \"top_up_by_cash_or_cheque\",\n           \"top_up_failed\",\n           \"top_up_limits\",\n           \"top_up_reverted\",\n           \"topping_up_by_card\",\n           \"transaction_charged_twice\",\n           \"transfer_fee_charged\",\n           \"transfer_into_account\",\n           \"transfer_not_received_by_recipient\",\n           \"transfer_timing\",\n           \"unable_to_verify_identity\",\n           \"verify_my_identity\",\n           \"verify_source_of_funds\",\n           \"verify_top_up\",\n           \"virtual_card_not_working\",\n           \"visa_or_mastercard\",\n           \"why_verify_identity\",\n           \"wrong_amount_of_cash_received\",\n           \"wrong_exchange_rate_for_cash_withdrawal\"\n       ],\n       \"few_shot_examples\": \"seed.csv\",\n       \"few_shot_selection\": \"semantic_similarity\",\n       \"few_shot_num\": 10,\n       \"example_template\": \"Input: {example}\\nOutput: {label}\"\n   }\n}\n</code></pre>"},{"location":"guide/resources/refuel_datasets/","title":"Refuel-provided Datasets","text":"<p>Autolabel provides datasets out-of-the-box so you can easily get started with LLM-powered labeling. The full list of datasets is below:</p> Dataset Task Type banking Classification civil_comments Classification ledgar Classification movie_reviews Classification walmart_amazon Entity Matching company Entity Matching squad_v2 Question Answering sciq Question Answering conll2003 Named Entity Matching"},{"location":"guide/resources/refuel_datasets/#downloading-any-dataset","title":"Downloading any dataset","text":"<p>To download a specific dataset, such as <code>civil_comments</code>, run: <pre><code>from autolabel import get_data\n\nget_data('civil_comments')\n&gt; Downloading seed example dataset to \"data/civil_comments/seed.csv\"...\n&gt; 100% [..............................................................................] 65757 / 65757\n\n&gt; Downloading test dataset to \"data/civil_comments/test.csv\"...\n&gt; 100% [............................................................................] 610663 / 610663\n</code></pre></p>"},{"location":"guide/resources/synthetic_dataset_generation/","title":"Synthetic Dataset Generation","text":"<p>Few shot learning is one of the most powerful tools that autolabel offers to improve the accuracy of LLM generated labels. However, curating a seed dataset to use for few shot learning can be a time consuming and tedious process. To make this process easier, autolabel's LabelingAgent provides a method to generate synthetic datasets. These datasets can be used as seed datasets for few shot learning or any other purpose. This guide will walk you through the process of generating a synthetic dataset using autolabel.</p> <p>Currently, autolabel supports synthetic dataset generation for classification and entity matching tasks. We plan to add support for other task types in the future.</p>"},{"location":"guide/resources/synthetic_dataset_generation/#walkthrough-creating-a-synthetic-dataset-for-banking","title":"Walkthrough: Creating a Synthetic Dataset for Banking","text":"<ol> <li>The first step is to import the LabelingAgent from autolabel. This is the main class that we will use to generate the synthetic dataset.  <pre><code>from autolabel import LabelingAgent\n</code></pre> </li> <li>The next step is to create the task config. Make sure to add the <code>dataset_generation</code> section to the config. This section contains the parameters for the dataset generation process. The <code>guidelines</code> parameter is a string containing the guidelines for the dataset generation task. The <code>num_rows</code> parameter is an integer indicating the number of rows per label to generate in the dataset.  <pre><code>config = {\n  \"task_name\": \"BankingComplaintsClassification\",\n  \"task_type\": \"classification\",\n  \"dataset\": {\n    \"label_column\": \"label\",\n    \"delimiter\": \",\"\n  },\n  \"model\": {\n    \"provider\": \"openai\",\n    \"name\": \"gpt-3.5-turbo\"\n  },\n  \"prompt\": {\n    \"task_guidelines\": \"You are an expert at understanding bank customers support complaints and queries.\\nYour job is to correctly classify the provided input example into one of the following categories.\\nCategories:\\n{labels}\",\n    \"output_guidelines\": \"You will answer with just the the correct output label and nothing else.\",\n    \"labels\": {\n        \"activate_my_card\": \"the customer cannot activate their credit or debit card\",\n        \"age_limit\": \"the customer is under the age limit\",\n        \"apple_pay_or_google_pay\": \"the customer is having trouble using apple pay or google pay\",\n        ... # more labels\n    },\n    \"example_template\": \"Input: {example}\\nOutput: {label}\"\n  },\n  \"dataset_generation\": {\n    \"num_rows\": 5,\n    \"guidelines\": \"You are an expert at generating synthetic data. You will generate a dataset that satisfies the following criteria:\\n1. The data should be diverse and cover a wide range of scenarios.\\n2. The data should be as realistic as possible, closely mimicking real-world data.\\n3. The data should vary in length, some shorter and some longer.\\n4. The data should be generated in a csv format.\\n\\nEach row should contain a realistic bank complaint. Use CSV format, with each line containing just the complaint and nothing else.\"\n  }\n}\n</code></pre>  Note that here, we defined <code>labels</code> as a dictionary where the keys are the valid labels and the values are descriptions for those labels. This helps the LLM understand what each label means and can result in a higher quality dataset.  </li> <li>Now all that's left is to run the code that generates the dataset!  <pre><code>agent = LabelingAgent(config)\nds = agent.generate_synthetic_dataset()\n</code></pre> </li> </ol> <p>That's it! You now have a synthetic dataset that you can use for few shot learning or for any other purpose. You can save the dataset to a csv file using the following code:</p> <pre><code>ds.save(\"synthetic_dataset.csv\")\n</code></pre>"},{"location":"guide/resources/synthetic_dataset_generation/#model-and-model-parameters","title":"Model and Model Parameters","text":"<p>To edit the model used for synthetic dataset generation, simply change the <code>model</code> section of the config. We've found that setting a higher temperature for this task generally results in more realistic datasets. We recommend experimenting with different models and model parameters to see what works best for your use case.</p>"},{"location":"guide/tasks/attribute_extraction/","title":"Attribute extraction","text":""},{"location":"guide/tasks/attribute_extraction/#introduction","title":"Introduction","text":"<p>Attribute Extraction is a task that shows up in real world frequently. This task extracts multiple attributes or features from a single piece of text. For eg. extracting the colour, price and name from a product description paragraph. Instead of making multiple calls to the llm, we can extract all attributes in one call! Additionally, if the attributes are related to each other, doing attribute extraction means that the relationships between the outputs are respected i.e suppose we extract the length of a shirt along with its letter size. Doing attribute extraction would make sure the letter and the integer length are consistent.</p>"},{"location":"guide/tasks/attribute_extraction/#example","title":"Example","text":""},{"location":"guide/tasks/attribute_extraction/#dataset","title":"Dataset","text":"<p>Lets walk through using Autolabel for attribute extraction on the ethos dataset. The ethos dataset comprises of hate speech on social media platforms. Every datapoints consists of an exmaple with hate speech and corresponding to it, there are three attributes, i.e violence, gender and directed_vs_generalized.</p> <pre><code>{\n  \"example\": \"tweet containing hate speech\",\n  \"violence\": \"violent\",\n  \"directed_vs_generalized\": \"directed\",\n  \"gender\": \"false\"\n}\n</code></pre> <p>Thus the dataset contains of 4 columns, the example along with the 3 attributes. Here, Autolabel would be given the example input for a new datapoint and told to predict the labels for the 3 attributes.</p>"},{"location":"guide/tasks/attribute_extraction/#config","title":"Config","text":"<p>In order to run Autolabel, we need a config defining the 3 important things - task, llm and dataset. Let's assume gpt-3.5-turbo as the LLM for this section.</p> <pre><code>config = {\n  \"task_name\": \"EthosAttributeExtraction\",\n  \"task_type\": \"attribute_extraction\",\n  \"dataset\": {\n    \"text_column\": \"text\",\n    \"delimiter\": \",\"\n  },\n  \"model\": {\n    \"provider\": \"openai\",\n    \"name\": \"gpt-3.5-turbo\"\n  },\n  \"prompt\": {\n    \"task_guidelines\": \"You are an expert at classifying hate speech and identifying the type of hate speech. Read the following tweets and extract the following attributes from the text.\",\n    \"attributes\": [\n      {\n        \"name\": \"violence\",\n        \"options\": [\"not_violent\", \"violent\"],\n        \"description\": \"If the tweet mentions violence towards a person or a group.\"\n      },\n      {\n        \"name\": \"directed_vs_generalized\",\n        \"options\": [\n          \"generalized\",\n          \"directed\"\n        ],\n        \"description\": \"If the hate speech is generalized towards a group or directed towards a specific person.\"\n      },\n      {\n        \"name\": \"gender\",\n        \"options\": [\n          \"true\",\n          \"false\"\n        ],\n        \"description\": \"If the hate speech uses gendered language and attacks a particular gender.\"\n      }\n    ],\n    \"few_shot_examples\": \"seed.csv\",\n    \"few_shot_selection\": \"fixed\",\n    \"few_shot_num\": 5,\n    \"example_template\": \"Text: {text}\\nOutput: {output_dict}\"\n  }\n}\n</code></pre> <p>The <code>task_type</code> sets up the config for a specific task, attribute_extraction in this case.</p> <p>Take a look at the prompt section of the config. This defines the settings related to defining the task and the machinery around it.</p> <p>The <code>task_guidelines</code> key is the most important key, it defines the task for the LLM to understand and execute on. In this case, we first set up the task and tell the model the kind of data present in the dataset, by telling it that it is an expert at classifying hate speech.</p> <p>The <code>attributes</code> key is the most important key for defining attribute extraction well. For every attribute, we have atleast 2 keys -  a. <code>name</code> - This is the name of the attribute. b. <code>description</code> - This is the description of an attribute. This describes the attribute more concretely and prompts the model to extract the corresponding attribute. c. <code>options</code> - You can also define a list of options for the LLM. This is an optional field. In case the attribute has a list of values from which to choose the value, fill this list. Otherwise, the attribute is prompted to be any possible textual value.</p> <p>The <code>example_template</code> is one of the most important keys to set for a task. This defines the format of every example that will be sent to the LLM. This creates a prompt using the columns from the input dataset. Here we define the <code>output_dict</code> key, which is used in the example template for attribute extraction tasks. This will create a json of all the attributes, as key value pairs. The LLM is also prompted to output the attributes in a json.</p>"},{"location":"guide/tasks/attribute_extraction/#run-the-task","title":"Run the task","text":"<pre><code>from autolabel import LabelingAgent, AutolabelDataset\nagent = LabelingAgent(config)\nds = AutolabelDataset('test.csv', config = config)\nagent.plan(ds)\nagent.run(ds, max_items = 100)\n</code></pre>"},{"location":"guide/tasks/attribute_extraction/#evaluation-metrics","title":"Evaluation metrics","text":"<p>On running the above config, this is an example output expected for labeling 100 items.</p> <pre><code>Actual Cost: 0.0665\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 violence:\u2026 \u2503 violence:\u2026 \u2503 violence:\u2026 \u2503 directed_\u2026 \u2503 directed\u2026 \u2503 directed_\u2026 \u2503 gender:s\u2026 \u2503 gender:co\u2026 \u2503 gender:a\u2026 \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 100        \u2502 1.0        \u2502 0.89       \u2502 100        \u2502 1.0       \u2502 0.89       \u2502 100       \u2502 1.0        \u2502 0.94      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Accuracy - This is calculated by taking the exact match of the predicted tokens and their correct class. This may suffer from class imbalance.</p> <p>Completion Rate - There can be errors while running the LLM related to labeling for eg. the LLM may give a label which is not in the label list or provide an answer which is not parsable by the library. In this cases, we mark the example as not labeled successfully. The completion rate refers to the proportion of examples that were labeled successfully.</p>"},{"location":"guide/tasks/attribute_extraction/#confidence","title":"Confidence","text":"<p>You can calculate per attribute confidence metric as well by setting compute_confidence as true in the model config. This can help you decide which examples to keep per attribute.</p>"},{"location":"guide/tasks/attribute_extraction/#notebook","title":"Notebook","text":"<p>You can find a Jupyter notebook with code that you can run on your own here.</p>"},{"location":"guide/tasks/classification_task/","title":"Classification Task","text":""},{"location":"guide/tasks/classification_task/#introduction","title":"Introduction","text":"<p>Text classification is a fundamental task in natural language processing (NLP) that involves categorizing textual data into predefined classes or categories. It is employed in various applications such as sentiment analysis, spam detection, topic classification, intent recognition, and document categorization and can be used in any setting where there are well defined categories which the LLM can understand and put an input into.</p>"},{"location":"guide/tasks/classification_task/#example","title":"Example","text":""},{"location":"guide/tasks/classification_task/#dataset","title":"Dataset","text":"<p>Lets walk through using Autolabel for text classification on the Banking77 dataset. The Banking77 dataset comprises of 13,083 customer service queries labeled with 77 intents. It focuses on fine-grained single-domain intent detection. Every datapoint consists of an example and its corresponding label as shown below. The label belongs to a set of 77 predefined intents that the customer had for the particular datapoint for eg. activate_my_card, card_delivery_estimate, get_physical_card.</p> <pre><code>{\n    \"example\": \"What can I do if my card still hasn't arrived after 2 weeks?\",\n    \"label\": \"card_arrival\"\n}\n</code></pre> <p>Thus the dataset consists of just two columns, example and label. Here, Autolabel would be given the example input for a new datapoint and told to predict the label column which in this case is label.</p>"},{"location":"guide/tasks/classification_task/#config","title":"Config","text":"<p>In order to run Autolabel, we need a config defining the 3 important things - task, llm and dataset. Let's assume gpt-3.5-turbo as the LLM for this section.</p> <p><pre><code>config = {\n    \"task_name\": \"BankingClassification\",\n    \"task_type\": \"classification\",\n    \"dataset\": {\n        \"label_column\": \"label\",\n        \"delimiter\": \",\"\n    },\n    \"model\": {\n        \"provider\": \"openai\",\n        \"name\": \"gpt-3.5-turbo\"\n    },\n    \"prompt\": {\n        \"task_guidelines\": \"\"\"You are an expert at understanding banking transaction complaints.\\nYour job is to correctly label the provided input example into one of the following {num_labels} categories:\\n{labels}\"\"\",\n        \"output_guidelines\": \"You will just return one line consisting of the label for the given example.\",\n        \"labels\": [\n            \"activate_my_card\",\n            \"age_limit\",\n            \"apple_pay_or_google_pay\",\n            ...\n        ],\n        \"example_template\": \"Example: {example}\\nOutput: {label}\"\n    }\n}\n</code></pre> The <code>task_type</code> sets up the config for a specific task, classification in this case.</p> <p>Take a look at the prompt section of the config. This defines the settings related to defining the task and the machinery around it.  </p> <p>The <code>task_guidelines</code> key is the most important key, it defines the task for the LLM to understand and execute on. In this case, we first set up the task and tell the model the kind of data present in the dataset, by telling it that it is an expert at understanding banking transaction complaints. Next, we define the task more concretely using the num_labels and labels appropriately. <code>{num_labels}</code> will be internally translated by the library to be the number of elements in the <code>labels</code> list (defined below).  <code>{labels}</code> will be translated to be all the labels in the <code>labels</code> list separated by a newline. These are essential for setting up classification tasks by telling it the labels that it is constrained to, along with any meaning associated with a label.  </p> <p>The <code>labels</code> key defines the list of possible labels for the banking77 dataset which is a list of 77 possible labels.  </p> <p>The <code>example_template</code> is one of the most important keys to set for a task. This defines the format of every example that will be sent to the LLM. This creates a prompt using the columns from the input dataset, and sends this prompt to the LLM hoping for the llm to generate the column defined under the <code>label_column</code>, which is label in our case. For every input, the model will be given the example with all the columns from the datapoint filled in according to the specification in the <code>example_template</code>. The <code>label_column</code> will be empty, and the LLM will generate the label. The <code>example_template</code> will be used to format all seed examples.  </p>"},{"location":"guide/tasks/classification_task/#few-shot-config","title":"Few Shot Config","text":"<p>Let's assume we have access to a dataset of labeled seed examples. Here is a config which details how to use it.</p> <pre><code>config = {\n    \"task_name\": \"BankingClassification\",\n    \"task_type\": \"classification\",\n    \"dataset\": {\n        \"label_column\": \"label\",\n        \"delimiter\": \",\"\n    },\n    \"model\": {\n        \"provider\": \"openai\",\n        \"name\": \"gpt-3.5-turbo\"\n    },\n    \"prompt\": {\n        \"task_guidelines\": \"\"\"You are an expert at understanding banking transaction complaints.\\nYour job is to correctly label the provided input example into one of the following {num_labels} categories:\\n{labels}\"\"\",\n        \"output_guidelines\": \"You will just return one line consisting of the label for the given example.\",\n        \"labels\": [\n            \"activate_my_card\",\n            \"age_limit\",\n            \"apple_pay_or_google_pay\",\n            ...\n        ],\n        \"few_shot_examples\": \"../examples/banking/seed.csv\",\n        \"few_shot_selection\": \"semantic_similarity\",\n        \"few_shot_num\": 5,\n        \"example_template\": \"Example: {example}\\nOutput: {label}\"\n    }\n}\n</code></pre> <p>The <code>few_shot_examples</code> key defines the seed set of labeled examples that are present for the model to learn from. A subset of these examples will be picked while querying the LLM in order to help it understand the task better, and understand corner cases.  </p> <p>For the banking dataset, we found <code>semantic_similarity</code> search to work really well. This looks for examples similar to a query example from the seed set and sends those to the LLM when querying for a particular input. This is defined in the <code>few_shot_selection</code> key.  </p> <p><code>few_shot_num</code> defines the number of examples selected from the seed set and sent to the LLM. Experiment with this number based on the input token budget and performance degradation with longer inputs.</p>"},{"location":"guide/tasks/classification_task/#run-the-task","title":"Run the task","text":"<pre><code>from autolabel import LabelingAgent\nagent = LabelingAgent(config)\nds = AutolabelDataset('data/banking77.csv', config = config)\nagent.plan(ds)\nagent.run(ds, max_items = 100)\n</code></pre>"},{"location":"guide/tasks/classification_task/#evaluation-metrics","title":"Evaluation metrics","text":"<p>On running the above config, this is an example output expected for labeling 100 items. <pre><code>Cost in $=0.00, support=50, threshold=-inf, accuracy=0.6600, completion_rate=1.0000\nActual Cost: 0.0058579999999999995\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 support \u2503 threshold \u2503 accuracy \u2503 completion_rate \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 100     \u2502 -inf      \u2502 0.76     \u2502 1.0             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <p>Accuracy - We use accuracy as the main metric for evaluating classification tasks. This is done by checking the fraction of examples which are given the correct label in the training dataset.</p> <p>Completion Rate - There can be errors while running the LLM related to labeling for eg. the LLM may give a label which is not in the label list or provide an answer which is not parsable by the library. In this cases, we mark the example as not labeled successfully. The completion rate refers to the proportion of examples that were labeled successfully.</p>"},{"location":"guide/tasks/classification_task/#notebook","title":"Notebook","text":"<p>You can find a Jupyter notebook with code that you can run on your own here</p>"},{"location":"guide/tasks/classification_task/#classification-tasks-with-a-large-number-of-classes","title":"Classification Tasks with a Large Number of Classes","text":"<p>For classification tasks with a wide variety of possible classes, it is beneficial to run autolabel with <code>label_selection</code> turned on. In this mode, Autolabel will prune the list of possible classes to only include those that are similar to the example being labeled. This not only helps improve accuracy, but also substantially reduces labeling costs, as the size of the prompt decreases when classes are pruned.</p> <p>To enable label_selection, simply set <code>label_selection</code> to <code>true</code> in your config file. Similarly, you can choose how many classes to select in the similarity search by setting <code>label_selection_count</code> to a value of your choosing.</p> <pre><code>    \"label_selection\": true,\n    \"label_selection_count\": 10\n</code></pre> <p>In this example, the list of classes will be reduced to only the 10 classes most similar to the example being labeled.</p> <pre><code>config = {\n    \"task_name\": \"BankingClassification\",\n    \"task_type\": \"classification\",\n    \"dataset\": {\n        \"label_column\": \"label\",\n        \"delimiter\": \",\"\n    },\n    \"model\": {\n        \"provider\": \"openai\",\n        \"name\": \"gpt-3.5-turbo\"\n    },\n    \"prompt\": {\n        \"task_guidelines\": \"\"\"You are an expert at understanding banking transaction complaints.\\nYour job is to correctly label the provided input example into one of the following {num_labels} categories:\\n{labels}\"\"\",\n        \"output_guidelines\": \"You will just return one line consisting of the label for the given example.\",\n        \"labels\": [\n            \"activate_my_card\",\n            \"age_limit\",\n            \"apple_pay_or_google_pay\",\n            ...\n        ],\n        \"few_shot_examples\": \"../examples/banking/seed.csv\",\n        \"few_shot_selection\": \"semantic_similarity\",\n        \"few_shot_num\": 5,\n        \"example_template\": \"Example: {example}\\nOutput: {label}\",\n        \"label_selection\": true,\n        \"label_selection_count\": 10\n    }\n}\n</code></pre>"},{"location":"guide/tasks/entity_matching_task/","title":"Entity Matching Task","text":""},{"location":"guide/tasks/entity_matching_task/#introduction","title":"Introduction","text":"<p>Entity matching in natural language processing (NLP) is a task that involves identifying and matching entities from different sources or datasets based on various fields or attributes. The goal is to determine if two entities refer to the same real-world object or entity, even if they are described differently or come from different data sources.</p>"},{"location":"guide/tasks/entity_matching_task/#example","title":"Example","text":""},{"location":"guide/tasks/entity_matching_task/#dataset","title":"Dataset","text":"<p>Lets walk through using Autolabel for entity matching on the Walmart-Amazon dataset. This dataset consists of duplicate products listed on both Walmart and Amazon. These products would have different names and descriptions but would be the same product. The dataset consists of such examples, where given the name and the description, the task is to predict if the products are duplicate or not. An example from the Walmart-Amazon dataset,</p> <pre><code>{\n    \"entity1\": \"Title: zotac geforce gt430 1gb ddr3 pci-express 2.0 graphics card; Category: electronics - general; Brand: zotac; ModelNo: zt-40604-10l; Price: 88.88;\",\n    \"entity2\": \"Title: evga geforce gts450 superclocked 1 gb gddr5 pci-express 2.0 graphics card 01g-p3-1452-tr; Category: graphics cards; Brand: evga; ModelNo: 01g-p3-1452-tr; Price: 119.88;\",\n    \"label\": \"not duplicate\"\n}\n</code></pre> <p>The the dataset consists of two columns <code>entity1</code> and <code>entity2</code> which define the two entities. There could also be multiple columns defining an entity. The <code>label</code> column here defines if the two entities are duplicates or not.</p>"},{"location":"guide/tasks/entity_matching_task/#config","title":"Config","text":"<p>In order to run Autolabel, we need a config defining the 3 important things - task, llm and dataset. Let's assume gpt-3.5-turbo as the LLM for this section.</p> <p><pre><code>config = {\n    \"task_name\": \"ProductCatalogEntityMatch\",\n    \"task_type\": \"entity_matching\",\n    \"dataset\": {\n        \"label_column\": \"label\",\n        \"delimiter\": \",\"\n    },\n    \"model\": {\n        \"provider\": \"openai\",\n        \"name\": \"gpt-3.5-turbo\",\n        \"params\": {}\n    },\n    \"prompt\": {\n        \"task_guidelines\": \"You are an expert at identifying duplicate products from online product catalogs.\\nYou will be given information about two product entities, and your job is to tell if they are the same (duplicate) or different (not duplicate). Your answer must be from one of the following options:\\n{labels}\",\n        \"labels\": [\n            \"duplicate\",\n            \"not duplicate\"\n        ],\n        \"few_shot_examples\": [\n            {\n                \"entity1\": \"Title: lexmark extra high yield return pgm print cartridge - magenta; Category: printers; Brand: lexmark; ModelNo: c782u1mg; Price: 214.88;\",\n                \"entity2\": \"Title: lexmark 18c1428 return program print cartridge black; Category: inkjet printer ink; Brand: lexmark; ModelNo: 18c1428; Price: 19.97;\",\n                \"label\": \"not duplicate\"\n            },\n            {\n                \"entity1\": \"Title: edge tech proshot 4gb sdhc class 6 memory card; Category: usb drives; Brand: edge tech; ModelNo: pe209780; Price: 10.88;\",\n                \"entity2\": \"Title: 4gb edge proshot sdhc memory card class6; Category: computers accessories; Brand: edge; ModelNo: nan; Price: 17.83;\",\n                \"label\": \"duplicate\"\n            },\n            {\n                \"entity1\": \"Title: tomtom one carry case; Category: gps; Brand: tomtom; ModelNo: 9n00 .181; Price: 19.96;\",\n                \"entity2\": \"Title: tomtom one carrying case; Category: cases; Brand: tomtom; ModelNo: 9n00 .181; Price: 4.99;\",\n                \"label\": \"duplicate\"\n            },\n            {\n                \"entity1\": \"Title: iosafe rugged 250gb usb 3.0 portable external hard drive; Category: hard drives; Brand: iosafe; ModelNo: pa50250u5yr; Price: 249.99;\",\n                \"entity2\": \"Title: lacie rugged all-terrain 500 gb firewire 800 firewire 400 usb 2.0 portable external hard drive 301371; Category: external hard drives; Brand: lacie; ModelNo: 301371; Price: nan;\",\n                \"label\": \"not duplicate\"\n            }\n        ],\n        \"few_shot_selection\": \"fixed\",\n        \"few_shot_num\": 3,\n        \"example_template\": \"Entity1: {entity1}\\nEntity2: {entity2}\\nOutput: {label}\"\n    }\n}\n</code></pre> The <code>task_type</code> sets up the config for a specific task, entity_matching in this case.</p> <p>Take a look at the prompt section of the config. This defines the settings related to defining the task and the machinery around it.  </p> <p>The <code>task_guidelines</code> key is the most important key, it defines the task for the LLM to understand and execute on. In this case, we first set up the task and tell the model the kind of data present in the dataset, by telling it that it is an expert at identifying duplicate products. Next we explain the task to the model, saying that it has two identify if the given products are duplicate or not. We also make the output format clear by telling the model it has to choose from the options duplicate or not duplicate. </p> <p>The <code>example_template</code> is one of the most important keys to set for a task. This defines the format of every example that will be sent to the LLM. This creates a prompt using the columns from the input dataset, and sends this prompt to the LLM hoping for the llm to generate the column defined under the <code>label_column</code>, which is answer in our case. For every input, the model will be given the example with all the columns from the datapoint filled in according to the specification in the <code>example_template</code>. The <code>label_column</code> will be empty, and the LLM will generate the label. The <code>example_template</code> will be used to format all seed examples. Here we give the model both the entities separated by newlines and ask if the entities are duplicate or not duplicate.</p> <p>The <code>few_shot_examples</code> here is a list of json inputs which define handpicked examples to use as seed examples for the model. These labeled examples help the model understand the task better and how it supposed to answer a question. If there is a larger number of examples, we can specify a path to a csv instead of a list of examples.</p> <p><code>few_shot_num</code> defines the number of examples selected from the seed set and sent to the LLM. Experiment with this number based on the input token budget and performance degradation with longer inputs.</p> <p><code>few_shot_selection</code> is set to fixed in this case as we want to use all examples as seed examples. However, if we want to use a subset of examples as seed examples from a larger set, we can set the appropriate strategy like <code>semantic_similarity</code> here to get dynamic good seed examples.</p>"},{"location":"guide/tasks/entity_matching_task/#alternate-config-with-multiple-columns","title":"Alternate config with multiple columns","text":"<p>Let's consider the case in which there are multiple columns in the dataset which are combined to create an input for the model.</p> <pre><code>config = {\n    \"task_name\": \"ProductCatalogEntityMatch\",\n    \"task_type\": \"entity_matching\",\n    \"dataset\": {\n        \"label_column\": \"label\",\n        \"delimiter\": \",\"\n    },\n    \"model\": {\n        \"provider\": \"openai\",\n        \"name\": \"gpt-3.5-turbo\"\n    },\n    \"prompt\": {\n        \"task_guidelines\": \"You are an expert at identifying duplicate products from online product catalogs.\\nYou will be given information about two product entities, and your job is to tell if they are the same (duplicate) or different (not duplicate). Your answer must be from one of the following options:\\n{labels}\",\n        \"labels\": [\n            \"duplicate\",\n            \"not duplicate\"\n        ],\n        \"example_template\": \"Title of entity1: {Title_entity1}; category of entity1: {Category_entity1}; brand of entity1: {Brand_entity1}; model number of entity1: {ModelNo_entity1}; price of entity1: {Price_entity1}\\nTitle of entity2: {Title_entity2}; category of entity2: {Category_entity2}; brand of entity2: {Brand_entity2}; model number of entity2: {ModelNo_entity2}; price of entity2: {Price_entity2}\\nDuplicate or not: {label}\",\n        \"few_shot_examples\": [\n            {\n                \"Title_entity1\": \"lexmark extra high yield return pgm print cartridge - magenta\",\n                \"Category_entity1\": \"printers\",\n                \"Brand_entity1\": \"lexmark\",\n                \"ModelNo_entity1\": \"c782u1mg\",\n                \"Price_entity1\": \"214.88\",\n                \"Title_entity2\": \"lexmark 18c1428 return program print cartridge black\",\n                \"Category_entity2\": \"inkjet printer ink\",\n                \"Brand_entity2\": \"lexmark\",\n                \"ModelNo_entity2\": \"18c1428\",\n                \"Price_entity2\": \"19.97\",\n                \"label\": \"not duplicate\"\n            },\n            {\n                \"Title_entity1\": \"edge tech proshot 4gb sdhc class 6 memory card\",\n                \"Category_entity1\": \"usb drives\",\n                \"Brand_entity1\": \"edge tech\",\n                \"ModelNo_entity1\": \"pe209780\",\n                \"Price_entity1\": \"10.88\",\n                \"Title_entity2\": \"4gb edge proshot sdhc memory card class6\",\n                \"Category_entity2\": \"computers accessories\",\n                \"Brand_entity2\": \"edge\",\n                \"ModelNo_entity2\": \"nan\",\n                \"Price_entity2\": \"17.83\",\n                \"label\": \"duplicate\"\n            }\n        ],\n        \"few_shot_selection\": \"fixed\",\n        \"few_shot_num\": 2\n    }\n}\n</code></pre> <p>Notice how in this case, we specify how the different columns defining different aspects of every column are stitched together to form the final example template.</p>"},{"location":"guide/tasks/entity_matching_task/#run-the-task","title":"Run the task","text":"<pre><code>from autolabel import LabelingAgent\nagent = LabelingAgent(config)\nds = AutolabelDataset('data/walmart_amazon_test.csv', config = config)\nagent.plan(ds)\nagent.run(ds, max_items = 100)\n</code></pre>"},{"location":"guide/tasks/entity_matching_task/#evaluation-metrics","title":"Evaluation metrics","text":"<p>On running the above config, this is an example output expected for labeling 100 items. <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 support \u2503 threshold \u2503 accuracy \u2503 completion_rate \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 100     \u2502 -inf      \u2502 0.96     \u2502 1.0             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <p>Accuracy - This measures the proportion of examples which are marked correctly by the model - for eg which mark duplicate entities correctly.</p> <p>Completion Rate - There can be errors while running the LLM related to labeling for eg. the LLM may give a label which is not in the label list or provide an answer which is not parsable by the library. In this cases, we mark the example as not labeled successfully. The completion rate refers to the proportion of examples that were labeled successfully.</p>"},{"location":"guide/tasks/multilabel_classification_task/","title":"Multilabel Classification Task","text":""},{"location":"guide/tasks/multilabel_classification_task/#introduction","title":"Introduction","text":"<p>Multilabel text classification is a fundamental task in natural language processing (NLP) where textual data is categorized into predefined classes or categories. It expands upon traditional text classification by assigning multiple labels to each text instance. This approach finds applications in sentiment analysis, spam detection, topic classification, intent recognition, and document categorization. By considering multiple labels, it allows for a more nuanced representation of text data, accommodating scenarios where multiple topics or attributes are associated with a document. Multilabel text classification enables a flexible and comprehensive approach to categorizing textual data, providing a richer understanding of content and facilitating more nuanced decision-making in various NLP applications.</p>"},{"location":"guide/tasks/multilabel_classification_task/#example","title":"Example","text":""},{"location":"guide/tasks/multilabel_classification_task/#dataset","title":"Dataset","text":"<p>Lets walk through using Autolabel for multilabel text classification on the sem_eval_2018_task_1 dataset which we call twitter-emotion-detection for clarity. The twitter-emotion-detection dataset comprises of 10,983 English tweets and 11 emotions. If no emotions were selected for a row, we classified it as <code>neutral</code>.</p> <pre><code>{\n  \"example\": \"I blew that opportunity -__- #mad\",\n  \"label\": \"anger, disgust, sadness\"\n}\n</code></pre> <p>Thus the dataset consists of just two columns, example and labels. Here, Autolabel would be given the example input for a new datapoint and told to predict the label column which in this case is labels.</p>"},{"location":"guide/tasks/multilabel_classification_task/#config","title":"Config","text":"<p>In order to run Autolabel, we need a config defining the 3 important things - task, llm and dataset. Let's assume gpt-3.5-turbo as the LLM for this section.</p> <pre><code>config = {\n    \"task_name\": \"EmotionClassification\",\n    \"task_type\": \"multilabel_classification\",\n    \"dataset\": {\n        \"label_column\": \"labels\",\n        \"label_separator\": \", \",\n        \"delimiter\": \",\"\n    },\n    \"model\": {\n        \"provider\": \"openai\",\n        \"name\": \"gpt-3.5-turbo\"\n    },\n    \"prompt\": {\n        \"task_guidelines\": \"You are an expert at classifying tweets as neutral or one or more of the given emotions that best represent the mental state of the poster.\\nYour job is to correctly label the provided input example into one or more of the following categories:\\n{labels}\",\n        \"output_guidelines\": \"You will return the answer as a comma separated list of labels sorted in alphabetical order. For example: \\\"label1, label2, label3\\\"\",\n        \"labels\": [\n            \"neutral\",\n            \"anger\",\n            \"anticipation\",\n            ...\n        ],\n        \"example_template\": \"Input: {example}\\nOutput: {labels}\"\n    }\n}\n</code></pre> <p>The <code>task_type</code> sets up the config for a specific task, multilabel_classification in this case.</p> <p>Take a look at the prompt section of the config. This defines the settings related to defining the task and the machinery around it.</p> <p>The <code>task_guidelines</code> key is the most important key, it defines the task for the LLM to understand and execute on. In this case, we first set up the task and tell the model the kind of data present in the dataset, by telling it that it is an expert at classifying tweets. Next, we define the task more concretely using labels appropriately. <code>{labels}</code> will be translated to be all the labels in the <code>labels</code> list separated by a newline. These are essential for setting up classification tasks by telling it the labels that it is constrained to, along with any meaning associated with a label.</p> <p>The <code>labels</code> key defines the list of possible labels for the twitter-emotion-detection dataset which is a list of 12 possible labels.</p> <p>The <code>example_template</code> is one of the most important keys to set for a task. This defines the format of every example that will be sent to the LLM. This creates a prompt using the columns from the input dataset, and sends this prompt to the LLM hoping for the llm to generate the column defined under the <code>label_column</code>, which is labels in our case. For every input, the model will be given the example with all the columns from the datapoint filled in according to the specification in the <code>example_template</code>. The <code>label_column</code> will be empty, and the LLM will generate the labels. The <code>example_template</code> will be used to format all seed examples.</p>"},{"location":"guide/tasks/multilabel_classification_task/#few-shot-config","title":"Few Shot Config","text":"<p>Let's assume we have access to a dataset of labeled seed examples. Here is a config which details how to use it.</p> <pre><code>config = {\n    \"task_name\": \"EmotionClassification\",\n    \"task_type\": \"multilabel_classification\",\n    \"dataset\": {\n        \"label_column\": \"labels\",\n        \"label_separator\": \", \",\n        \"delimiter\": \",\"\n    },\n    \"model\": {\n        \"provider\": \"openai\",\n        \"name\": \"gpt-3.5-turbo\"\n    },\n    \"prompt\": {\n        \"task_guidelines\": \"You are an expert at classifying tweets as neutral or one or more of the given emotions that best represent the mental state of the poster.\\nYour job is to correctly label the provided input example into one or more of the following categories:\\n{labels}\",\n        \"output_guidelines\": \"You will return the answer as a comma separated list of labels sorted in alphabetical order. For example: \\\"label1, label2, label3\\\"\",\n        \"labels\": [\n            \"neutral\",\n            \"anger\",\n            \"anticipation\",\n            ...\n        ],\n        \"few_shot_examples\": \"seed.csv\",\n        \"few_shot_selection\": \"semantic_similarity\",\n        \"few_shot_num\": 5,\n        \"example_template\": \"Input: {example}\\nOutput: {labels}\"\n    }\n}\n</code></pre> <p>The <code>few_shot_examples</code> key defines the seed set of labeled examples that are present for the model to learn from. A subset of these examples will be picked while querying the LLM in order to help it understand the task better, and understand corner cases.</p> <p>For the twitter dataset, we found <code>semantic_similarity</code> search to work really well. This looks for examples similar to a query example from the seed set and sends those to the LLM when querying for a particular input. This is defined in the <code>few_shot_selection</code> key.</p> <p><code>few_shot_num</code> defines the number of examples selected from the seed set and sent to the LLM. Experiment with this number based on the input token budget and performance degradation with longer inputs.</p>"},{"location":"guide/tasks/multilabel_classification_task/#run-the-task","title":"Run the task","text":"<pre><code>from autolabel import LabelingAgent, AutolabelDataset\nagent = LabelingAgent(config)\nds = AutolabelDataset('twitter_emotion_detection.csv', config = config)\nagent.plan(ds)\nagent.run(ds, max_items = 100)\n</code></pre>"},{"location":"guide/tasks/multilabel_classification_task/#evaluation-metrics","title":"Evaluation metrics","text":"<p>On running the above config, this is an example output expected for labeling 100 items.</p> <pre><code>Actual Cost: 0.0025\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 f1     \u2503 support \u2503 accuracy \u2503 completion_rate \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 0.4507 \u2502 100     \u2502 0.08     \u2502 1.0             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Accuracy - This is calculated by taking the exact match of the predicted tokens and their correct class. This may suffer from class imbalance.</p> <p>F1 - This is calculated using the precision and recall of the predicted tokens and their classes. We use a macro average to get to one F1 score for all classes.</p> <p>Completion Rate - There can be errors while running the LLM related to labeling for eg. the LLM may give a label which is not in the label list or provide an answer which is not parsable by the library. In this cases, we mark the example as not labeled successfully. The completion rate refers to the proportion of examples that were labeled successfully.</p>"},{"location":"guide/tasks/multilabel_classification_task/#notebook","title":"Notebook","text":"<p>You can find a Jupyter notebook with code that you can run on your own here.</p>"},{"location":"guide/tasks/named_entity_recognition_task/","title":"Named Entity Recognition Task","text":""},{"location":"guide/tasks/named_entity_recognition_task/#introduction","title":"Introduction","text":"<p>Named Entity Recognition (NER) is a crucial task in natural language processing (NLP) that involves identifying and classifying named entities in text. Named entities refer to specific individuals, organizations, locations, dates, quantities, and other named entities present in the text. The goal of NER is to extract and classify these entities accurately, providing valuable information for various NLP applications such as information extraction, question answering, and sentiment analysis.</p>"},{"location":"guide/tasks/named_entity_recognition_task/#example","title":"Example","text":""},{"location":"guide/tasks/named_entity_recognition_task/#dataset","title":"Dataset","text":"<p>Lets walk through using Autolabel for named entity recognition on the CONLL2003 dataset. The CONLL2003 dataset comprises of sentences with entities in the sentence labeled LOC (location), ORG (organization), PER (person) or MISC (Miscellaneous).  </p> <pre><code>{\n    \"example\": \"The role of the 70,000 mainly Kurdish village guards who fight Kurdistan Workers Party ( PKK ) guerrillas in the southeast has been questioned recently after media allegations that many of them are involved in common crime .\",\n    \"CategorizedLabels\": \"{'Location': [], 'Organization': ['Kurdistan Workers Party', 'PKK'], 'Person': [], 'Miscellaneous': ['Kurdish']}\"\n}\n</code></pre> <p>Thus the dataset consists of the <code>example</code> and <code>CategorizedLabels</code> columns. Here <code>example</code> mentions the sentence which needs to be labeled. The <code>CategorizedLabels</code> contains the entities for every label as a list.</p>"},{"location":"guide/tasks/named_entity_recognition_task/#config","title":"Config","text":"<p>In order to run Autolabel, we need a config defining the 3 important things - task, llm and dataset. Let's assume gpt-3.5-turbo as the LLM for this section.</p> <p><pre><code>config = {\n    \"task_name\": \"PersonLocationOrgMiscNER\",\n    \"task_type\": \"named_entity_recognition\",\n    \"dataset\": {\n        \"label_column\": \"CategorizedLabels\",\n        \"text_column\": \"example\"\n    },\n    \"model\": {\n        \"provider\": \"anthropic\",\n        \"name\": \"claude-v1\"\n    },\n    \"prompt\": {\n        \"task_guidelines\": \"You are an expert at extracting Person, Organization, Location, and Miscellaneous entities from text. Your job is to extract named entities mentioned in text, and classify them into one of the following categories.\\nCategories:\\n{labels}\\n \",\n        \"labels\": [\n            \"Location\",\n            \"Organization\",\n            \"Person\",\n            \"Miscellaneous\"\n        ],\n        \"example_template\": \"Example: {example}\\nOutput: {CategorizedLabels}\",\n        \"few_shot_examples\": \"data/conll2003_seed.csv\",\n        \"few_shot_selection\": \"semantic_similarity\",\n        \"few_shot_num\": 5\n    }\n}\n</code></pre> The <code>task_type</code> sets up the config for a specific task, named_entity_recognition in this case.</p> <p>Take a look at the prompt section of the config. This defines the settings related to defining the task and the machinery around it.  </p> <p>The <code>task_guidelines</code> key is the most important key, it defines the task for the LLM to understand and execute on. In this case, we first set up the task and tell the model the kind of data present in the dataset, by telling it that it is an expert at extracting entities from text and classifying them into the necessary labels. Next, we tell the model the list of categories that it should classify every entity into. This ensures that every entity is assigned to one category.  </p> <p>The <code>example_template</code> is one of the most important keys to set for a task. This defines the format of every example that will be sent to the LLM. This creates a prompt using the columns from the input dataset, and sends this prompt to the LLM hoping for the llm to generate the column defined under the <code>label_column</code>, which is <code>CategorizedLabels</code> in our case. For every input, the model will be given the example with all the columns from the datapoint filled in according to the specification in the <code>example_template</code>. The <code>label_column</code> will be empty, and the LLM will generate the label. The <code>example_template</code> will be used to format all seed examples.  </p> <p>The <code>few_shot_examples</code> here is a path to a csv which defines a set of labeled examples which the model can use to understand the task better. These examples will be used as a reference by the model.</p> <p><code>few_shot_num</code> defines the number of examples selected from the seed set and sent to the LLM. Experiment with this number based on the input token budget and performance degradation with longer inputs.</p> <p><code>few_shot_selection</code> is set to <code>semantic_similarity</code> in this case as we want to use a subset of examples as seed examples from a larger set to get dynamically good seed examples.</p>"},{"location":"guide/tasks/named_entity_recognition_task/#run-the-task","title":"Run the task","text":"<pre><code>from autolabel import LabelingAgent, AutolabelDataset\nagent = LabelingAgent(config)\nds = AutolabelDataset('examples/squad_v2/test.csv', config = config)\nagent.plan(ds, max_items = 100)\nagent.run(ds, max_items = 100)\n</code></pre>"},{"location":"guide/tasks/named_entity_recognition_task/#evaluation-metrics","title":"Evaluation metrics","text":"<p>On running the above config, this is an example output expected for labeling 100 items. <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 f1     \u2503 support \u2503 threshold \u2503 accuracy \u2503 completion_rate \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 0.7834 \u2502 100     \u2502 -inf      \u2502 0.7834   \u2502 1.0             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <p>Accuracy - This is calculated by taking the exact match of the predicted tokens and their correct class. This may suffer from class imbalance.</p> <p>F1 - This is calculated using the precision and recall of the predicted tokens and their classes. We use a macro average to get to one F1 score for all classes.</p> <p>Completion Rate - There can be errors while running the LLM related to labeling for eg. the LLM may provide an answer which is not parsable by the library. In this cases, we mark the example as not labeled successfully. The completion rate refers to the proportion of examples that were labeled successfully.</p>"},{"location":"guide/tasks/question_answering_task/","title":"Question Answering Task","text":""},{"location":"guide/tasks/question_answering_task/#introduction","title":"Introduction","text":"<p>Question answering is the most fundamental task that can be solved using LLMs. Most tasks can be reduced to some form of question answering where the model is optionally given some context and then asked to answer a question. There can be a broad classification of question answering tasks into 2 categories -  </p> <ol> <li> <p>Open Book QA - In this variant, the model is given a context along with a question and then asked to answer using the context. Here, we do not rely on knowledge present in the model parameters and instead rely on the reasoning abilities and commonsense properties of the model to answer correctly.</p> </li> <li> <p>Closed Book QA - In this variant, the model is just given a question, without any context or knowledge source and asked to answer based on pretrained knowledge. This requires more knowledge to be present in the model parameters and thus favours bigger LLMs.</p> </li> </ol> <p>In addition to context, question answering tasks can also differ in the way that the answers are generated. The easiest form is one where there is a predefined set of options (for eg. yes or no) and the model needs to choose from one of these options. Another variant allows separate options for each question similar to SAT questions. The last variant is one where the model is free to generate its own answers. This variant is harder to evaluate because multiple answers could mean the same thing.</p>"},{"location":"guide/tasks/question_answering_task/#example","title":"Example","text":""},{"location":"guide/tasks/question_answering_task/#dataset","title":"Dataset","text":"<p>Lets walk through using Autolabel for question answering on the Squad dataset. The Squad dataset comprises of 100k questions and answers along with a context for each question which contains the answer for the question. Additionally, the correct answer is a continuous text span from the context. However, in addition to correct answers, it also contains 50k pairs where the question is unanswerable given the context, that is, the context does not have enough information to answer the question correctly. Here is an example datapoint from the dataset,</p> <pre><code>{\n    \"question\": \"When did Beyonce start becoming popular?\",\n    \"context\": \"Beyonc\u00e9 Giselle Knowles-Carter (/bi\u02d0\u02c8j\u0252nse\u026a/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&amp;B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyonc\u00e9's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles 'Crazy in Love' and 'Baby Boy'.\",\n    \"answer\": \"in the late 1990s\"\n}\n</code></pre> <p>Thus the dataset consists of the <code>question</code>, <code>context</code> and <code>answer</code>. For datasets like SciQ, there may be an additional field called <code>options</code> which is a list of strings which are possible answers for a particular question.</p>"},{"location":"guide/tasks/question_answering_task/#config","title":"Config","text":"<p>In order to run Autolabel, we need a config defining the 3 important things - task, llm and dataset. Let's assume gpt-3.5-turbo as the LLM for this section.</p> <p><pre><code>config = {\n    \"task_name\": \"OpenbookQAWikipedia\",\n    \"task_type\": \"question_answering\",\n    \"dataset\": {\n        \"label_column\": \"answer\",\n        \"delimiter\": \",\"\n    },\n    \"model\": {\n        \"provider\": \"openai\",\n        \"name\": \"gpt-3.5-turbo\",\n        \"params\": {}\n    },\n    \"prompt\": {\n        \"task_guidelines\": \"You are an expert at answering questions based on wikipedia articles. Your job is to answer the following questions using the context provided with the question. The answer is a continuous span of words from the context. Use the context to answer the question. If the question cannot be answered using the context, answer the question as unanswerable.\",\n        \"few_shot_examples\": [\n            {\n                \"question\": \"What was created by the modern Conservative Party in 1859 to define basic Conservative principles?\",\n                \"answer\": \"unanswerable\",\n                \"context\": \"The modern Conservative Party was created out of the 'Pittite' Tories of the early 19th century. In the late 1820s disputes over political reform broke up this grouping. A government led by the Duke of Wellington collapsed amidst dire election results. Following this disaster Robert Peel set about assembling a new coalition of forces. Peel issued the Tamworth Manifesto in 1834 which set out the basic principles of Conservatism; \u2013 the necessity in specific cases of reform in order to survive, but an opposition to unnecessary change, that could lead to 'a perpetual vortex of agitation'. Meanwhile, the Whigs, along with free trade Tory followers of Robert Peel, and independent Radicals, formed the Liberal Party under Lord Palmerston in 1859, and transformed into a party of the growing urban middle-class, under the long leadership of William Ewart Gladstone.\"\n            },\n            {\n                \"question\": \"When is King Mom symbolically burnt?\",\n                \"answer\": \"On the evening before Lent\",\n                \"context\": \"Carnival means weeks of events that bring colourfully decorated floats, contagiously throbbing music, luxuriously costumed groups of celebrants of all ages, King and Queen elections, electrifying jump-ups and torchlight parades, the Jouvert morning: the Children's Parades and finally the Grand Parade. Aruba's biggest celebration is a month-long affair consisting of festive 'jump-ups' (street parades), spectacular parades and creative contests. Music and flamboyant costumes play a central role, from the Queen elections to the Grand Parade. Street parades continue in various districts throughout the month, with brass band, steel drum and roadmarch tunes. On the evening before Lent, Carnival ends with the symbolic burning of King Momo.\"\n            },\n            {\n                \"question\": \"How far does the Alps range stretch?\",\n                \"answer\": \"the Mediterranean Sea north above the Po basin, extending through France from Grenoble, eastward through mid and southern Switzerland\",\n                \"context\": \"The Alps are a crescent shaped geographic feature of central Europe that ranges in a 800 km (500 mi) arc from east to west and is 200 km (120 mi) in width. The mean height of the mountain peaks is 2.5 km (1.6 mi). The range stretches from the Mediterranean Sea north above the Po basin, extending through France from Grenoble, eastward through mid and southern Switzerland. The range continues toward Vienna in Austria, and east to the Adriatic Sea and into Slovenia. To the south it dips into northern Italy and to the north extends to the south border of Bavaria in Germany. In areas like Chiasso, Switzerland, and Neuschwanstein, Bavaria, the demarcation between the mountain range and the flatlands are clear; in other places such as Geneva, the demarcation is less clear. The countries with the greatest alpine territory are Switzerland, France, Austria and Italy.\"\n            }\n        ],\n        \"few_shot_selection\": \"fixed\",\n        \"few_shot_num\": 3,\n        \"example_template\": \"Context: {context}\\nQuestion: {question}\\nAnswer: {answer}\"\n    }\n}\n</code></pre> The <code>task_type</code> sets up the config for a specific task, question_answering in this case.</p> <p>Take a look at the prompt section of the config. This defines the settings related to defining the task and the machinery around it.  </p> <p>The <code>task_guidelines</code> key is the most important key, it defines the task for the LLM to understand and execute on. In this case, we first set up the task and tell the model the kind of data present in the dataset, by telling it that it is an expert at understanding wikipedia articles. Next, we define the task more concretely by telling the model how to answer the question given the context. We tell the model that the answer is a continuous text span from the context and that in some cases, the answer can be unanswerable and how the model should handle such questions.  </p> <p>The <code>example_template</code> is one of the most important keys to set for a task. This defines the format of every example that will be sent to the LLM. This creates a prompt using the columns from the input dataset, and sends this prompt to the LLM hoping for the llm to generate the column defined under the <code>label_column</code>, which is answer in our case. For every input, the model will be given the example with all the columns from the datapoint filled in according to the specification in the <code>example_template</code>. The <code>label_column</code> will be empty, and the LLM will generate the label. The <code>example_template</code> will be used to format all seed examples. Here we also see the ordering of the context followed by question and answer, and also see the <code>Context:</code> string to inform the model which part of the text is the context.</p> <p>The <code>few_shot_examples</code> here is a list of json inputs which define handpicked examples to use as seed examples for the model. These labeled examples help the model understand the task better and how it supposed to answer a question. If there is a larger number of examples, we can specify a path to a csv instead of a list of examples.</p> <p><code>few_shot_num</code> defines the number of examples selected from the seed set and sent to the LLM. Experiment with this number based on the input token budget and performance degradation with longer inputs.</p> <p><code>few_shot_selection</code> is set to fixed in this case as we want to use all examples as seed examples. However, if we want to use a subset of examples as seed examples from a larger set, we can set the appropriate strategy like <code>semantic_similarity</code> here to get dynamic good seed examples.</p>"},{"location":"guide/tasks/question_answering_task/#alternate-config-for-closedbook-qa","title":"Alternate config for ClosedBook QA","text":"<p>Let's consider a dataset like sciq which is a closed book QA with multiple choice questions. Here we have an example config for this dataset,</p> <pre><code>config = {\n    \"task_name\": \"ClosedBookQAScienceQuestions\",\n    \"task_type\": \"question_answering\",\n    \"dataset\": {\n        \"label_column\": \"answer\",\n        \"delimiter\": \",\"\n    },\n    \"model\": {\n        \"provider\": \"openai\",\n        \"name\": \"gpt-3.5-turbo\",\n        \"params\": {}\n    },\n    \"prompt\": {\n        \"task_guidelines\": \"You are an expert at answering science questions. Choose an answer from the given options. Use your knowledge of science and common sense to best answer the question.\",\n        \"few_shot_examples\": \"../examples/squad_v2/seed.csv\",\n        \"few_shot_selection\": \"fixed\",\n        \"few_shot_num\": 3,\n        \"example_template\": \"Question: {question}\\nOptions: {options}\\nAnswer: {answer}\"\n    }\n}\n</code></pre> <p>Notice in this case we don't have the <code>context</code> and pass in the <code>options</code> as list of string options. These are present in the dataset and are appropriately called in the example template.</p>"},{"location":"guide/tasks/question_answering_task/#run-the-task","title":"Run the task","text":"<pre><code>from autolabel import LabelingAgent, AutolabelDataset\nagent = LabelingAgent(config)\nds = AutolabelDataset('data/squad_v2_test.csv', config = config)\nagent.plan(ds)\nagent.run(ds, max_items = 100)\n</code></pre>"},{"location":"guide/tasks/question_answering_task/#evaluation-metrics","title":"Evaluation metrics","text":"<p>On running the above config, this is an example output expected for labeling 100 items. <pre><code>Actual Cost: 0.13500600000000001\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 f1                 \u2503 support \u2503 threshold \u2503 accuracy \u2503 completion_rate \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 0.7018720299348971 \u2502 100     \u2502 -inf      \u2502 0.59     \u2502 1.0             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</code></pre></p> <p>Accuracy - This is the exact match performance based on the reference answer. Here we give the model 1 if the answer matches exactly with the correct answer and 0 otherwise. This is particularly harsh for the model in cases where there isnt a multi choice given to the model for eg. Squad. Even if the model gets one word wrong without changing the meaning, the model will get penalized.</p> <p>F1 - This is calculated by treating the predicted and the ground truth tokens as a list of tokens. Using this, an F1 score is calculated for every examples. This score can then be averaged over the entire dataset to get the final score. An exact match would get an F1 score of 1. This metric allows the model to make small mistakes in the predicted tokens and might be a more accurate metric for cases where the answers are not restricted to a set of options.</p> <p>Completion Rate - There can be errors while running the LLM related to labeling for eg. the LLM may give a label which is not in the label list or provide an answer which is not parsable by the library. In this cases, we mark the example as not labeled successfully. The completion rate refers to the proportion of examples that were labeled successfully.</p>"},{"location":"guide/transforms/image_transform/","title":"Image Transform","text":"<p>The image transform allows users to extract text from image files. Autolabel uses optical character recognition (OCR) to read the images. To use this transform, follow these steps:</p>"},{"location":"guide/transforms/image_transform/#installation","title":"Installation","text":"<p>Use the following command to download all dependencies for the image transform.</p> <pre><code>pip install pillow pytesseract\n</code></pre> <p>The tesseract engine is also required for OCR text extraction. See the tesseract docs for installation instructions.</p>"},{"location":"guide/transforms/image_transform/#parameters-for-this-transform","title":"Parameters for this transform","text":"<ol> <li>file_path_column: the name of the column containing the file paths of the pdf files to extract text from</li> <li>lang: a string indicating the language of the text in the pdf file. See the tesseract docs for a full list of supported languages</li> </ol>"},{"location":"guide/transforms/image_transform/#using-the-transform","title":"Using the transform","text":"<p>Below is an example of an image transform to extract text from an image file:</p> <pre><code>{\n  ..., # other config parameters\n  \"transforms\": [\n    ..., # other transforms\n    {\n      \"name\": \"image\",\n      \"params\": {\n        \"file_path_column\": \"file_path\",\n        \"lang\": \"eng\"\n      },\n      \"output_columns\": {\n        \"content_column\": \"content\",\n        \"metadata_column\": \"metadata\"\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"guide/transforms/image_transform/#run-the-transform","title":"Run the transform","text":"<pre><code>from autolabel import LabelingAgent, AutolabelDataset\nagent = LabelingAgent(config)\nds = agent.transform(ds)\n</code></pre> <p>This runs the transformation. We will see the content in the correct column. Access this using <code>ds.df</code> in the AutolabelDataset.</p>"},{"location":"guide/transforms/introduction/","title":"Introduction","text":"<p>Autolabel supports transformation of the input data! Input datasets are available in many shapes and form(at)s. We help you ingest your data in the format that you want in a way that is most useful for the downstream LLM or labeling task that you have in mind. We have tried to make the transforms performant, configurable and the outputs formatted in a way useful for the LLM.</p>"},{"location":"guide/transforms/introduction/#example","title":"Example","text":"<p>Here we will show you how to run an example transform. We will use the Webpage Transform to ingest national park websites and label the state that every national park belongs to. You can find a Jupyter notebook with code that you can run on your own here </p> <p>Use this webpage transform yourself here in a Colab - </p>"},{"location":"guide/transforms/introduction/#changes-to-config","title":"Changes to config","text":"<pre><code>{\n    \"task_name\": \"NationalPark\",\n    \"task_type\": \"question_answering\",\n    \"dataset\": {\n    },\n    \"model\": {\n        \"provider\": \"openai\",\n        \"name\": \"gpt-3.5-turbo\"\n    },\n    \"transforms\": [{\n        \"name\": \"webpage_transform\",\n        \"params\": {\n            \"url_column\": \"url\"\n        },\n        \"output_columns\": {\n            \"content_column\": \"content\"\n        }\n    }],\n    \"prompt\": {\n        \"task_guidelines\": \"You are an expert at understanding websites of national parks. You will be given a webpage about a national park. Answer with the US State that the national park is located in.\",\n        \"output_guidelines\": \"Answer in one word the state that the national park is located in.\",\n        \"example_template\": \"Content of wikipedia page: {content}\\State:\",\n    }\n}\n</code></pre> <p>Notice the <code>transforms</code> key in the config. This is where we define our transforms. Notice that this is a list meaning we can define multiple transforms here. Every element of this list is a transform. A transform is a json requiring 3 inputs - 1. <code>name</code>: This tells the agent which transform needs to be loaded. Here we are using the webpage transform. 2. <code>params</code>: This is the set of parameters that will be passed to the transform. Read the documentation of the separate transform to see what params can be passed to the transform here. Here we pass the url_column, i.e the column containing the webpages that need to be loaded. 3. <code>output_columns</code>: Each transform can define multiple outputs. In this dictionary we map the output we need, in case <code>content_column</code> to the name of the column in the output dataset in which we want to populate this.</p>"},{"location":"guide/transforms/introduction/#running-the-transform","title":"Running the transform","text":"<pre><code>from autolabel import LabelingAgent, AutolabelDataset\nagent = LabelingAgent(config)\nds = agent.transform(ds)\n</code></pre> <p>This runs the transformation. We will see the content in the correct column. Access this using <code>ds.df</code> in the AutolabelDataset.</p>"},{"location":"guide/transforms/introduction/#running-the-labeling-job","title":"Running the labeling job","text":"<pre><code>ds = agent.run(ds)\n</code></pre> <p>Simply run the labeling job on the transformed dataset. This will extract the state of the national park from each webpage.</p> <p> </p> Output of the transformation labeling run"},{"location":"guide/transforms/introduction/#custom-transforms","title":"Custom Transforms","text":"<p>We support the following transforms -</p> <ol> <li>Webpage Transform</li> <li>PDF Transform</li> </ol> <p>We expect this list to grow in the future and need the help of the community to build transforms that work the best for their data. For this, we provide an abstraction that is easy to use. Any new transform just needs to be extend the <code>BaseTransform</code> class as penciled down below.</p> <p>rendering: show_root_heading: yes show_root_full_path: no</p>"},{"location":"guide/transforms/introduction/#src.autolabel.transforms.base.BaseTransform","title":"<code>BaseTransform</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Base class for all transforms.</p> Source code in <code>src/autolabel/transforms/base.py</code> <pre><code>class BaseTransform(ABC):\n    \"\"\"Base class for all transforms.\"\"\"\n\n    TTL_MS = 60 * 60 * 24 * 7 * 1000  # 1 week\n    NULL_TRANSFORM_TOKEN = \"NO_TRANSFORM\"\n\n    def __init__(self, cache: BaseCache, output_columns: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        Initialize a transform.\n        Args:\n            cache: A cache object to use for caching the results of this transform.\n            output_columns: A dictionary of output columns. The keys are the names of the output columns as expected by the transform. The values are the column names they should be mapped to in the dataset.\n        \"\"\"\n        super().__init__()\n        self._output_columns = output_columns\n        self.cache = cache\n\n    @staticmethod\n    @abstractmethod\n    def name() -&gt; str:\n        \"\"\"\n        Returns the name of the transform.\n        \"\"\"\n        pass\n\n    @property\n    def output_columns(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Returns a dictionary of output columns. The keys are the names of the output columns\n        as expected by the transform. The values are the column names they should be mapped to in\n        the dataset.\n        \"\"\"\n        return {k: self._output_columns.get(k, None) for k in self.COLUMN_NAMES}\n\n    @property\n    def transform_error_column(self) -&gt; str:\n        \"\"\"\n        Returns the name of the column that stores the error if transformation fails.\n        \"\"\"\n        return f\"{self.name()}_error\"\n\n    @abstractmethod\n    async def _apply(self, row: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"\n        Applies the transform to the given row.\n        Args:\n            row: A dictionary representing a row in the dataset. The keys are the column names and the values are the column values.\n        Returns:\n            A dictionary representing the transformed row. The keys are the column names and the values are the column values.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def params(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Returns a dictionary of parameters that can be used to uniquely identify this transform.\n        Returns:\n            A dictionary of parameters that can be used to uniquely identify this transform.\n        \"\"\"\n        return {}\n\n    async def apply(self, row: Dict[str, Any]) -&gt; Dict[str, Any]:\n        if self.cache is not None:\n            cache_entry = TransformCacheEntry(\n                transform_name=self.name(),\n                transform_params=self.params(),\n                input=row,\n                ttl_ms=self.TTL_MS,\n            )\n            output = self.cache.lookup(cache_entry)\n\n            if output is not None:\n                # Cache hit\n                return output\n\n        try:\n            output = await self._apply(row)\n        except Exception as e:\n            logger.error(f\"Error applying transform {self.name()}. Exception: {str(e)}\")\n            output = {\n                k: self.NULL_TRANSFORM_TOKEN\n                for k in self.output_columns.values()\n                if k is not None\n            }\n            output[self.transform_error_column] = str(e)\n            return output\n\n        if self.cache is not None:\n            cache_entry.output = output\n            self.cache.update(cache_entry)\n\n        return output\n\n    def _return_output_row(self, row: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"\n        Returns the output row with the correct column names.\n        Args:\n            row: The output row.\n        Returns:\n            The output row with the correct column names.\n        \"\"\"\n        # remove null key\n        row.pop(None, None)\n        return row\n</code></pre>"},{"location":"guide/transforms/introduction/#src.autolabel.transforms.base.BaseTransform.output_columns","title":"<code>output_columns: Dict[str, Any]</code>  <code>property</code>","text":"<p>Returns a dictionary of output columns. The keys are the names of the output columns as expected by the transform. The values are the column names they should be mapped to in the dataset.</p>"},{"location":"guide/transforms/introduction/#src.autolabel.transforms.base.BaseTransform.transform_error_column","title":"<code>transform_error_column: str</code>  <code>property</code>","text":"<p>Returns the name of the column that stores the error if transformation fails.</p>"},{"location":"guide/transforms/introduction/#src.autolabel.transforms.base.BaseTransform.__init__","title":"<code>__init__(cache, output_columns)</code>","text":"<p>Initialize a transform. Args:     cache: A cache object to use for caching the results of this transform.     output_columns: A dictionary of output columns. The keys are the names of the output columns as expected by the transform. The values are the column names they should be mapped to in the dataset.</p> Source code in <code>src/autolabel/transforms/base.py</code> <pre><code>def __init__(self, cache: BaseCache, output_columns: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Initialize a transform.\n    Args:\n        cache: A cache object to use for caching the results of this transform.\n        output_columns: A dictionary of output columns. The keys are the names of the output columns as expected by the transform. The values are the column names they should be mapped to in the dataset.\n    \"\"\"\n    super().__init__()\n    self._output_columns = output_columns\n    self.cache = cache\n</code></pre>"},{"location":"guide/transforms/introduction/#src.autolabel.transforms.base.BaseTransform.name","title":"<code>name()</code>  <code>abstractmethod</code> <code>staticmethod</code>","text":"<p>Returns the name of the transform.</p> Source code in <code>src/autolabel/transforms/base.py</code> <pre><code>@staticmethod\n@abstractmethod\ndef name() -&gt; str:\n    \"\"\"\n    Returns the name of the transform.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"guide/transforms/introduction/#src.autolabel.transforms.base.BaseTransform.params","title":"<code>params()</code>  <code>abstractmethod</code>","text":"<p>Returns a dictionary of parameters that can be used to uniquely identify this transform. Returns:     A dictionary of parameters that can be used to uniquely identify this transform.</p> Source code in <code>src/autolabel/transforms/base.py</code> <pre><code>@abstractmethod\ndef params(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Returns a dictionary of parameters that can be used to uniquely identify this transform.\n    Returns:\n        A dictionary of parameters that can be used to uniquely identify this transform.\n    \"\"\"\n    return {}\n</code></pre>"},{"location":"guide/transforms/introduction/#_apply-abstractmethod","title":"<code>_apply()</code> <code>abstractmethod</code>","text":"<p>Applies the transform to the given row. Args:     row: A dictionary representing a row in the dataset. The keys are the column names and the values are the column values. Returns:     A dictionary representing the transformed row. The keys are the column names and the values are the column values.</p> Source code in <code>src/autolabel/transforms/base.py</code> <pre><code>@abstractmethod\nasync def _apply(self, row: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"\n    Applies the transform to the given row.\n    Args:\n        row: A dictionary representing a row in the dataset. The keys are the column names and the values are the column values.\n    Returns:\n        A dictionary representing the transformed row. The keys are the column names and the values are the column values.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"guide/transforms/pdf_transform/","title":"PDF Transform","text":"<p>The PDF transform allows users to extract text from pdf files. Autolabel offers both direct text extraction, useful for extracting text from pdfs that contain text, and optical character recognition (OCR) text extraction, useful for extracting text from pdfs that contain images. To use this transform, follow these steps:</p>"},{"location":"guide/transforms/pdf_transform/#installation","title":"Installation","text":"<p>For direct text extraction, install the <code>pdfplumber</code> package:</p> <pre><code>pip install pdfplumber\n</code></pre> <p>For OCR text extraction, install the <code>pdf2image</code> and <code>pytesseract</code> packages:</p> <pre><code>pip install pdf2image pytesseract\n</code></pre> <p>The tesseract engine is also required for OCR text extraction. See the tesseract docs for installation instructions.</p>"},{"location":"guide/transforms/pdf_transform/#parameters-for-this-transform","title":"Parameters for this transform","text":"<ol> <li>file_path_column: the name of the column containing the file paths of the pdf files to extract text from</li> <li>ocr_enabled: a boolean indicating whether to use OCR text extraction or not</li> <li>page_format: a string containing the format to use for each page of the pdf file. The following fields can be used in the format string: <ul> <li>page_num: the page number of the page</li> <li>page_content: the content of the page</li> </ul></li> <li>page_sep: a string containing the separator to use between each page of the pdf file</li> <li>lang: a string indicating the language of the text in the pdf file. See the [tesseract docs](https://tesseract-ocr.github.io/tessdoc/Data-Files-in-different-versions.html) for a full list of supported languages</li> </ol>"},{"location":"guide/transforms/pdf_transform/#output-format","title":"Output Format","text":"<p>The page_format and page_sep parameters define how the text extracted from the pdf will be formatted. For example, if the pdf file contained 2 pages with \"Hello,\" on the first page and \"World!\" on the second, a page_format of <code>{page_num} - {page_content}</code> and a page_sep of <code>\\n</code> would result in the following output:</p> <pre><code>\"1 - Hello,\\n2 - World!\"\n</code></pre> <p>The metadata column contains a dict with the field \"num_pages\" indicating the number of pages in the pdf file.</p>"},{"location":"guide/transforms/pdf_transform/#using-the-transform","title":"Using the transform","text":"<p>Below is an example of a pdf transform to extract text from a pdf file:</p> <pre><code>{\n  ..., # other config parameters\n  \"transforms\": [\n    ..., # other transforms\n    {\n      \"name\": \"pdf\",\n      \"params\": {\n        \"file_path_column\": \"file_path\",\n        \"ocr_enabled\": true,\n        \"page_format\": \"Page {page_num}: {page_content}\",\n        \"page_sep\": \"\\n\\n\"\n      },\n      \"output_columns\": {\n        \"content_column\": \"content\",\n        \"metadata_column\": \"metadata\"\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"guide/transforms/pdf_transform/#run-the-transform","title":"Run the transform","text":"<pre><code>from autolabel import LabelingAgent, AutolabelDataset\nagent = LabelingAgent(config)\nds = agent.transform(ds)\n</code></pre> <p>This runs the transformation. We will see the content in the correct column. Access this using <code>ds.df</code> in the AutolabelDataset.</p>"},{"location":"guide/transforms/webpage_transform/","title":"Webpage Transform","text":"<p>The Webpage transform supports loading and processing webpage urls. Given a url, this transform will send the request to load the webpage and then parse the webpage returned to collect the text to send to the LLM.</p> <p>Use this transform yourself here in a Colab - </p> <p>In order to use this transform, use the following steps:</p>"},{"location":"guide/transforms/webpage_transform/#installation","title":"Installation","text":"<p>Use the following command to download all dependencies for the webpage transform. <code>beautifulsoup4</code> must be version <code>4.12.2</code> or higher.</p> <pre><code>pip install beautifulsoup4 httpx fake_useragent\n</code></pre> <p>Make sure to do this before running the transform.</p>"},{"location":"guide/transforms/webpage_transform/#parameters-for-this-transform","title":"Parameters for this transform","text":"<ol> <li><code>url_column: str (Required)</code>: The column to retrieve the url from. This is the webpage that will be loaded by the transform.</li> <li><code>timeout: int (Optional: Default = 5)</code>: The timeout to wait until for loading the webpage. The request to the webpage will timeout after this. We will log an error and send an empty response after the timeout is reached.</li> <li><code>headers: Dict[str,str] (Optional: Default = {})</code>: Any headers that need to be passed into the webpage load request. Underneath we use requests to get the webpage and the headers are passed to request.</li> </ol>"},{"location":"guide/transforms/webpage_transform/#using-the-transform","title":"Using the transform","text":"<p>Below is an example of a webpage transform to extract text from a webpage:</p> <pre><code>{\n  ..., # other config parameters\n  \"transforms\": [\n    ..., # other transforms\n    {\n      \"name\": \"webpage_transform\",\n      \"params\": {\n        \"url_column\": \"url\"\n      },\n      \"output_columns\": {\n        \"content_column\": \"webpage_content\",\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"guide/transforms/webpage_transform/#run-the-transform","title":"Run the transform","text":"<pre><code>from autolabel import LabelingAgent, AutolabelDataset\nagent = LabelingAgent(config)\nds = agent.transform(ds)\n</code></pre> <p>This runs the transformation. We will see the content in the webpage_content column. Access this using <code>ds.df</code> in the AutolabelDataset.</p>"},{"location":"reference/cache/","title":"Cache","text":"<p>             Bases: <code>ABC</code></p> <p>used to store AutoLabeling results, allowing for interrupted labeling runs to be continued from a save point without the need to restart from the beginning. Any custom Cache classes should extend from BaseCache.</p> Source code in <code>src/autolabel/cache/base.py</code> <pre><code>class BaseCache(ABC):\n    \"\"\"used to store AutoLabeling results, allowing for interrupted labeling runs to be continued from a save point without the need to restart from the beginning. Any custom Cache classes should extend from BaseCache.\"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__()\n\n    @abstractmethod\n    def initialize():\n        \"\"\"initialize the cache. Must be implemented by classes derived from BaseCache.\"\"\"\n        pass\n\n    @abstractmethod\n    def lookup(self, entry):\n        \"\"\"abstract method to retrieve a cached entry. Must be implemented by classes derived from BaseCache.\"\"\"\n        pass\n\n    @abstractmethod\n    def update(self, entry):\n        \"\"\"abstract method to update the cache with a new entry. Must be implemented by classes derived from BaseCache.\"\"\"\n        pass\n\n    @abstractmethod\n    def clear(self) -&gt; None:\n        \"\"\"abstract method to clear the cache. Must be implemented by classes derived from BaseCache.\"\"\"\n        pass\n</code></pre> <p>rendering: show_root_heading: yes show_root_full_path: no</p> <p>             Bases: <code>BaseCache</code></p> <p>A cache system implemented with SQL Alchemy</p> Source code in <code>src/autolabel/cache/sqlalchemy_generation_cache.py</code> <pre><code>class SQLAlchemyGenerationCache(BaseCache):\n    \"\"\"A cache system implemented with SQL Alchemy\"\"\"\n\n    def __init__(self):\n        self.engine = None\n        self.base = Base\n        self.session = None\n\n    def initialize(self):\n        self.engine = create_db_engine()\n        self.base.metadata.create_all(self.engine)\n        self.session = sessionmaker(bind=self.engine)()\n\n    def lookup(\n        self, entry: GenerationCacheEntry\n    ) -&gt; List[Union[Generation, ChatGeneration]]:\n        \"\"\"Retrieves an entry from the Cache. Returns an empty list [] if not found.\n        Args:\n            entry: GenerationCacheEntry we wish to retrieve from the Cache\n        Returns:\n            result: A list of langchain Generation objects, containing the results of the labeling run for this GenerationCacheEntry. Empty list [] if not found.\n        \"\"\"\n        cache_entry = GenerationCacheEntryModel.get(self.session, entry)\n        if cache_entry is None:\n            logger.debug(\"Cache miss\")\n            return []\n\n        logger.debug(\"Cache hit\")\n        return cache_entry.generations\n\n    def update(self, entry: GenerationCacheEntry) -&gt; None:\n        \"\"\"Inserts the provided GenerationCacheEntry into the Cache, overriding it if it already exists\n        Args:\n            entry: GenerationCacheEntry we wish to put into the Cache\n        \"\"\"\n        GenerationCacheEntryModel.insert(self.session, entry)\n\n    def clear(self) -&gt; None:\n        \"\"\"Clears the entire Cache\"\"\"\n        GenerationCacheEntryModel.clear(self.session)\n</code></pre> <p>rendering: show_root_heading: yes show_root_full_path: no</p> <p>             Bases: <code>BaseCache</code></p> <p>A cache system implemented with SQL Alchemy for storing the output of transforms. This cache system is used to avoid re-computing the output of transforms that have already been computed. This currently stores the input and the outputs of the transform. Caching is based on the transform name, params and input.</p> Source code in <code>src/autolabel/cache/sqlalchemy_transform_cache.py</code> <pre><code>class SQLAlchemyTransformCache(BaseCache):\n    \"\"\"\n    A cache system implemented with SQL Alchemy for storing the output of transforms.\n    This cache system is used to avoid re-computing the output of transforms that have already been computed.\n    This currently stores the input and the outputs of the transform.\n    Caching is based on the transform name, params and input.\n    \"\"\"\n\n    def __init__(self):\n        self.engine = None\n        self.base = Base\n        self.session = None\n\n    def initialize(self):\n        self.engine = create_db_engine()\n        self.base.metadata.create_all(self.engine)\n        self.session = sessionmaker(bind=self.engine)()\n\n    def lookup(self, entry: TransformCacheEntry) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Retrieves an entry from the Cache. Returns None if not found.\n        Args:\n            entry: TransformCacheEntry we wish to retrieve from the Cache\n        Returns:\n            result: The output of the transform for this input. None if not found.\n        \"\"\"\n        cache_entry = TransformCacheEntryModel.get(self.session, entry)\n        if cache_entry is None:\n            return None\n\n        return cache_entry.output\n\n    def update(self, entry: TransformCacheEntry) -&gt; None:\n        \"\"\"Inserts the provided TransformCacheEntry into the Cache, overriding it if it already exists\n        Args:\n            entry: TransformCacheEntry we wish to put into the Cache\n        \"\"\"\n        TransformCacheEntryModel.insert(self.session, entry)\n\n    def clear(self, use_ttl: bool = True) -&gt; None:\n        \"\"\"Clears the entire Cache based on ttl\"\"\"\n        TransformCacheEntryModel.clear(self.session, use_ttl=use_ttl)\n</code></pre> <p>rendering: show_root_heading: yes show_root_full_path: no</p>"},{"location":"reference/cache/#src.autolabel.cache.base.BaseCache.clear","title":"<code>clear()</code>  <code>abstractmethod</code>","text":"<p>abstract method to clear the cache. Must be implemented by classes derived from BaseCache.</p> Source code in <code>src/autolabel/cache/base.py</code> <pre><code>@abstractmethod\ndef clear(self) -&gt; None:\n    \"\"\"abstract method to clear the cache. Must be implemented by classes derived from BaseCache.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/cache/#src.autolabel.cache.base.BaseCache.initialize","title":"<code>initialize()</code>  <code>abstractmethod</code>","text":"<p>initialize the cache. Must be implemented by classes derived from BaseCache.</p> Source code in <code>src/autolabel/cache/base.py</code> <pre><code>@abstractmethod\ndef initialize():\n    \"\"\"initialize the cache. Must be implemented by classes derived from BaseCache.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/cache/#src.autolabel.cache.base.BaseCache.lookup","title":"<code>lookup(entry)</code>  <code>abstractmethod</code>","text":"<p>abstract method to retrieve a cached entry. Must be implemented by classes derived from BaseCache.</p> Source code in <code>src/autolabel/cache/base.py</code> <pre><code>@abstractmethod\ndef lookup(self, entry):\n    \"\"\"abstract method to retrieve a cached entry. Must be implemented by classes derived from BaseCache.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/cache/#src.autolabel.cache.base.BaseCache.update","title":"<code>update(entry)</code>  <code>abstractmethod</code>","text":"<p>abstract method to update the cache with a new entry. Must be implemented by classes derived from BaseCache.</p> Source code in <code>src/autolabel/cache/base.py</code> <pre><code>@abstractmethod\ndef update(self, entry):\n    \"\"\"abstract method to update the cache with a new entry. Must be implemented by classes derived from BaseCache.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/cache/#src.autolabel.cache.sqlalchemy_generation_cache.SQLAlchemyGenerationCache.clear","title":"<code>clear()</code>","text":"<p>Clears the entire Cache</p> Source code in <code>src/autolabel/cache/sqlalchemy_generation_cache.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Clears the entire Cache\"\"\"\n    GenerationCacheEntryModel.clear(self.session)\n</code></pre>"},{"location":"reference/cache/#src.autolabel.cache.sqlalchemy_generation_cache.SQLAlchemyGenerationCache.lookup","title":"<code>lookup(entry)</code>","text":"<p>Retrieves an entry from the Cache. Returns an empty list [] if not found. Args:     entry: GenerationCacheEntry we wish to retrieve from the Cache Returns:     result: A list of langchain Generation objects, containing the results of the labeling run for this GenerationCacheEntry. Empty list [] if not found.</p> Source code in <code>src/autolabel/cache/sqlalchemy_generation_cache.py</code> <pre><code>def lookup(\n    self, entry: GenerationCacheEntry\n) -&gt; List[Union[Generation, ChatGeneration]]:\n    \"\"\"Retrieves an entry from the Cache. Returns an empty list [] if not found.\n    Args:\n        entry: GenerationCacheEntry we wish to retrieve from the Cache\n    Returns:\n        result: A list of langchain Generation objects, containing the results of the labeling run for this GenerationCacheEntry. Empty list [] if not found.\n    \"\"\"\n    cache_entry = GenerationCacheEntryModel.get(self.session, entry)\n    if cache_entry is None:\n        logger.debug(\"Cache miss\")\n        return []\n\n    logger.debug(\"Cache hit\")\n    return cache_entry.generations\n</code></pre>"},{"location":"reference/cache/#src.autolabel.cache.sqlalchemy_generation_cache.SQLAlchemyGenerationCache.update","title":"<code>update(entry)</code>","text":"<p>Inserts the provided GenerationCacheEntry into the Cache, overriding it if it already exists Args:     entry: GenerationCacheEntry we wish to put into the Cache</p> Source code in <code>src/autolabel/cache/sqlalchemy_generation_cache.py</code> <pre><code>def update(self, entry: GenerationCacheEntry) -&gt; None:\n    \"\"\"Inserts the provided GenerationCacheEntry into the Cache, overriding it if it already exists\n    Args:\n        entry: GenerationCacheEntry we wish to put into the Cache\n    \"\"\"\n    GenerationCacheEntryModel.insert(self.session, entry)\n</code></pre>"},{"location":"reference/cache/#src.autolabel.cache.sqlalchemy_transform_cache.SQLAlchemyTransformCache.clear","title":"<code>clear(use_ttl=True)</code>","text":"<p>Clears the entire Cache based on ttl</p> Source code in <code>src/autolabel/cache/sqlalchemy_transform_cache.py</code> <pre><code>def clear(self, use_ttl: bool = True) -&gt; None:\n    \"\"\"Clears the entire Cache based on ttl\"\"\"\n    TransformCacheEntryModel.clear(self.session, use_ttl=use_ttl)\n</code></pre>"},{"location":"reference/cache/#src.autolabel.cache.sqlalchemy_transform_cache.SQLAlchemyTransformCache.lookup","title":"<code>lookup(entry)</code>","text":"<p>Retrieves an entry from the Cache. Returns None if not found. Args:     entry: TransformCacheEntry we wish to retrieve from the Cache Returns:     result: The output of the transform for this input. None if not found.</p> Source code in <code>src/autolabel/cache/sqlalchemy_transform_cache.py</code> <pre><code>def lookup(self, entry: TransformCacheEntry) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Retrieves an entry from the Cache. Returns None if not found.\n    Args:\n        entry: TransformCacheEntry we wish to retrieve from the Cache\n    Returns:\n        result: The output of the transform for this input. None if not found.\n    \"\"\"\n    cache_entry = TransformCacheEntryModel.get(self.session, entry)\n    if cache_entry is None:\n        return None\n\n    return cache_entry.output\n</code></pre>"},{"location":"reference/cache/#src.autolabel.cache.sqlalchemy_transform_cache.SQLAlchemyTransformCache.update","title":"<code>update(entry)</code>","text":"<p>Inserts the provided TransformCacheEntry into the Cache, overriding it if it already exists Args:     entry: TransformCacheEntry we wish to put into the Cache</p> Source code in <code>src/autolabel/cache/sqlalchemy_transform_cache.py</code> <pre><code>def update(self, entry: TransformCacheEntry) -&gt; None:\n    \"\"\"Inserts the provided TransformCacheEntry into the Cache, overriding it if it already exists\n    Args:\n        entry: TransformCacheEntry we wish to put into the Cache\n    \"\"\"\n    TransformCacheEntryModel.insert(self.session, entry)\n</code></pre>"},{"location":"reference/configs/","title":"Config","text":""},{"location":"reference/configs/#src.autolabel.configs.base.BaseConfig","title":"<code>BaseConfig</code>","text":"<p>Used for parsing, validating, and storing information about the labeling task passed to the LabelingAgent. Additional config classes should extend from this base class.</p> Source code in <code>src/autolabel/configs/base.py</code> <pre><code>class BaseConfig:\n    \"\"\"Used for parsing, validating, and storing information about the labeling task passed to the LabelingAgent. Additional config classes should extend from this base class.\"\"\"\n\n    def __init__(self, config: Union[str, Dict], validate: bool = True) -&gt; None:\n        if isinstance(config, str):\n            self.config = self._safe_load_json(config)\n        else:\n            self.config = config\n        if validate:\n            self._validate()\n\n    def _safe_load_json(self, json_file_path: str) -&gt; Dict:\n        \"\"\"Loads config settings from a provided json file\"\"\"\n        try:\n            with open(json_file_path, \"r\") as config_file:\n                return json.load(config_file)\n        except ValueError as e:\n            logger.error(\n                f\"JSON file: {json_file_path} not loaded successfully. Error: {repr(e)}\"\n            )\n            return {}\n\n    def get(self, key: str, default_value: Any = None) -&gt; Any:\n        return self.config.get(key, default_value)\n\n    def keys(self) -&gt; List:\n        return list(self.config.keys())\n\n    def __getitem__(self, key):\n        return self.config[key]\n\n    def to_json(self) -&gt; str:\n        \"\"\"Returns the BaseConfig object in JSON format\"\"\"\n        return json.dumps(self.config, sort_keys=True)\n\n    def __str__(self):\n        return self.to_json()\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.base.BaseConfig.to_json","title":"<code>to_json()</code>","text":"<p>Returns the BaseConfig object in JSON format</p> Source code in <code>src/autolabel/configs/base.py</code> <pre><code>def to_json(self) -&gt; str:\n    \"\"\"Returns the BaseConfig object in JSON format\"\"\"\n    return json.dumps(self.config, sort_keys=True)\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig","title":"<code>AutolabelConfig</code>","text":"<p>             Bases: <code>BaseConfig</code></p> <p>Class to parse and store configs passed to Autolabel agent.</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>class AutolabelConfig(BaseConfig):\n    \"\"\"Class to parse and store configs passed to Autolabel agent.\"\"\"\n\n    # Top-level config keys\n    TASK_NAME_KEY = \"task_name\"\n    TASK_TYPE_KEY = \"task_type\"\n    DATASET_CONFIG_KEY = \"dataset\"\n    MODEL_CONFIG_KEY = \"model\"\n    EMBEDDING_CONFIG_KEY = \"embedding\"\n    PROMPT_CONFIG_KEY = \"prompt\"\n    DATASET_GENERATION_CONFIG_KEY = \"dataset_generation\"\n    CHUNKING_CONFIG_KEY = \"chunking\"\n\n    # Dataset config keys (config[\"dataset\"][&lt;key&gt;])\n    LABEL_COLUMN_KEY = \"label_column\"\n    LABEL_SEPARATOR_KEY = \"label_separator\"\n    EXPLANATION_COLUMN_KEY = \"explanation_column\"\n    IMAGE_COLUMN_KEY = \"image_url_column\"\n    TEXT_COLUMN_KEY = \"text_column\"\n    INPUT_COLUMNS_KEY = \"input_columns\"\n    DELIMITER_KEY = \"delimiter\"\n    DISABLE_QUOTING = \"disable_quoting\"\n\n    # Model config keys (config[\"model\"][&lt;key&gt;])\n    PROVIDER_KEY = \"provider\"\n    MODEL_NAME_KEY = \"name\"\n    MODEL_PARAMS_KEY = \"params\"\n    COMPUTE_CONFIDENCE_KEY = \"compute_confidence\"\n    LOGIT_BIAS_KEY = \"logit_bias\"\n\n    # Embedding config keys (config[\"embedding\"][&lt;key&gt;])\n    EMBEDDING_PROVIDER_KEY = \"provider\"\n    EMBEDDING_MODEL_NAME_KEY = \"model\"\n\n    # Prompt config keys (config[\"prompt\"][&lt;key&gt;])\n    TASK_GUIDELINE_KEY = \"task_guidelines\"\n    VALID_LABELS_KEY = \"labels\"\n    FEW_SHOT_EXAMPLE_SET_KEY = \"few_shot_examples\"\n    FEW_SHOT_SELECTION_ALGORITHM_KEY = \"few_shot_selection\"\n    FEW_SHOT_NUM_KEY = \"few_shot_num\"\n    VECTOR_STORE_PARAMS_KEY = \"vector_store_params\"\n    EXAMPLE_TEMPLATE_KEY = \"example_template\"\n    OUTPUT_GUIDELINE_KEY = \"output_guidelines\"\n    OUTPUT_FORMAT_KEY = \"output_format\"\n    CHAIN_OF_THOUGHT_KEY = \"chain_of_thought\"\n    LABEL_SELECTION_KEY = \"label_selection\"\n    LABEL_SELECTION_COUNT_KEY = \"label_selection_count\"\n    LABEL_SELECTION_THRESHOLD = \"label_selection_threshold\"\n    ATTRIBUTES_KEY = \"attributes\"\n    TRANSFORM_KEY = \"transforms\"\n\n    # Dataset generation config keys (config[\"dataset_generation\"][&lt;key&gt;])\n    DATASET_GENERATION_GUIDELINES_KEY = \"guidelines\"\n    DATASET_GENERATION_NUM_ROWS_KEY = \"num_rows\"\n\n    # Chunking config keys (config[\"chunking\"][&lt;key&gt;])\n    CONFIDENCE_CHUNK_COLUMN_KEY = \"confidence_chunk_column\"\n    CONFIDENCE_CHUNK_SIZE_KEY = \"confidence_chunk_size\"\n    CONFIDENCE_MERGE_FUNCTION_KEY = \"confidence_merge_function\"\n\n    def __init__(self, config: Union[str, Dict], validate: bool = True) -&gt; None:\n        super().__init__(config, validate=validate)\n\n    def _validate(self) -&gt; bool:\n        \"\"\"Returns true if the config settings are valid\"\"\"\n        from autolabel.configs.schema import schema\n\n        validate(\n            instance=self.config,\n            schema=schema,\n        )\n        return True\n\n    @cached_property\n    def _dataset_config(self) -&gt; Dict:\n        \"\"\"Returns information about the dataset being used for labeling (e.g. label_column, text_column, delimiter)\"\"\"\n        return self.config.get(self.DATASET_CONFIG_KEY, {})\n\n    @cached_property\n    def _model_config(self) -&gt; Dict:\n        \"\"\"Returns information about the model being used for labeling (e.g. provider name, model name, parameters)\"\"\"\n        return self.config[self.MODEL_CONFIG_KEY]\n\n    @cached_property\n    def _embedding_config(self) -&gt; Dict:\n        \"\"\"Returns information about the model being used for computing embeddings (e.g. provider name, model name)\"\"\"\n        return self.config.get(self.EMBEDDING_CONFIG_KEY, {})\n\n    @cached_property\n    def _prompt_config(self) -&gt; Dict:\n        \"\"\"Returns information about the prompt we are passing to the model (e.g. task guidelines, examples, output formatting)\"\"\"\n        return self.config[self.PROMPT_CONFIG_KEY]\n\n    @cached_property\n    def _dataset_generation_config(self) -&gt; Dict:\n        \"\"\"Returns information about the prompt for synthetic dataset generation\"\"\"\n        return self.config.get(self.DATASET_GENERATION_CONFIG_KEY, {})\n\n    @cached_property\n    def _chunking_config(self) -&gt; Dict:\n        \"\"\"Returns information about the chunking config\"\"\"\n        return self.config.get(self.CHUNKING_CONFIG_KEY, {})\n\n    # project and task definition config\n    def task_name(self) -&gt; str:\n        return self.config[self.TASK_NAME_KEY]\n\n    def task_type(self) -&gt; str:\n        \"\"\"Returns the type of task we have configured the labeler to perform (e.g. Classification, Question Answering)\"\"\"\n        return self.config[self.TASK_TYPE_KEY]\n\n    # Dataset config\n    def label_column(self) -&gt; str:\n        \"\"\"Returns the name of the column containing labels for the dataset. Used for comparing accuracy of autolabel results vs ground truth\"\"\"\n        return self._dataset_config.get(self.LABEL_COLUMN_KEY, None)\n\n    def label_separator(self) -&gt; str:\n        \"\"\"Returns the token used to seperate multiple labels in the dataset. Defaults to a semicolon ';'\"\"\"\n        return self._dataset_config.get(self.LABEL_SEPARATOR_KEY, \";\")\n\n    def text_column(self) -&gt; str:\n        \"\"\"Returns the name of the column containing text data we intend to label\"\"\"\n        return self._dataset_config.get(self.TEXT_COLUMN_KEY, None)\n\n    def input_columns(self) -&gt; List[str]:\n        \"\"\"Returns the names of the input columns from the dataset that are used in the prompt\"\"\"\n        return self._dataset_config.get(self.INPUT_COLUMNS_KEY, [])\n\n    def explanation_column(self) -&gt; str:\n        \"\"\"Returns the name of the column containing an explanation as to why the data is labeled a certain way\"\"\"\n        return self._dataset_config.get(self.EXPLANATION_COLUMN_KEY, None)\n\n    def image_column(self) -&gt; str:\n        \"\"\"Returns the name of the column containing an image url for the given item\"\"\"\n        return self._dataset_config.get(self.IMAGE_COLUMN_KEY, None)\n\n    def delimiter(self) -&gt; str:\n        \"\"\"Returns the token used to seperate cells in the dataset. Defaults to a comma ','\"\"\"\n        return self._dataset_config.get(self.DELIMITER_KEY, \",\")\n\n    def disable_quoting(self) -&gt; bool:\n        \"\"\"Returns true if quoting is disabled. Defaults to false\"\"\"\n        return self._dataset_config.get(self.DISABLE_QUOTING, False)\n\n    # Model config\n    def provider(self) -&gt; str:\n        \"\"\"Returns the name of the entity that provides the currently configured model (e.g. OpenAI, Anthropic, Refuel)\"\"\"\n        return self._model_config[self.PROVIDER_KEY]\n\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the name of the model being used for labeling (e.g. gpt-4, claude-v1)\"\"\"\n        return self._model_config[self.MODEL_NAME_KEY]\n\n    def model_params(self) -&gt; Dict:\n        \"\"\"Returns a dict of configured settings for the model (e.g. hyperparameters)\"\"\"\n        return self._model_config.get(self.MODEL_PARAMS_KEY, {})\n\n    def confidence(self) -&gt; bool:\n        \"\"\"Returns true if the model is able to return a confidence score along with its predictions\"\"\"\n        return self._model_config.get(self.COMPUTE_CONFIDENCE_KEY, False)\n\n    def logit_bias(self) -&gt; float:\n        \"\"\"Returns the logit bias for the labels specified in the config\"\"\"\n        return self._model_config.get(self.LOGIT_BIAS_KEY, 0.0)\n\n    # Embedding config\n    def embedding_provider(self) -&gt; str:\n        \"\"\"Returns the name of the entity that provides the model used for computing embeddings\"\"\"\n        return self._embedding_config.get(self.EMBEDDING_PROVIDER_KEY, self.provider())\n\n    def embedding_model_name(self) -&gt; str:\n        \"\"\"Returns the name of the model being used for computing embeddings (e.g. sentence-transformers/all-mpnet-base-v2)\"\"\"\n        return self._embedding_config.get(self.EMBEDDING_MODEL_NAME_KEY, None)\n\n    # Prompt config\n    def task_guidelines(self) -&gt; str:\n        return self._prompt_config.get(self.TASK_GUIDELINE_KEY, \"\")\n\n    def labels_list(self) -&gt; List[str]:\n        \"\"\"Returns a list of valid labels\"\"\"\n        if isinstance(self._prompt_config.get(self.VALID_LABELS_KEY, []), List):\n            return self._prompt_config.get(self.VALID_LABELS_KEY, [])\n        else:\n            return list(self._prompt_config.get(self.VALID_LABELS_KEY, {}).keys())\n\n    def label_descriptions(self) -&gt; Dict[str, str]:\n        \"\"\"Returns a dict of label descriptions\"\"\"\n        if isinstance(self._prompt_config.get(self.VALID_LABELS_KEY, []), List):\n            return {}\n        else:\n            return self._prompt_config.get(self.VALID_LABELS_KEY, {})\n\n    def few_shot_example_set(self) -&gt; Union[str, List]:\n        \"\"\"Returns examples of how data should be labeled, used to guide context to the model about the task it is performing\"\"\"\n        return self._prompt_config.get(self.FEW_SHOT_EXAMPLE_SET_KEY, [])\n\n    def few_shot_algorithm(self) -&gt; str:\n        \"\"\"Returns which algorithm is being used to construct the set of examples being given to the model about the labeling task\"\"\"\n        return self._prompt_config.get(self.FEW_SHOT_SELECTION_ALGORITHM_KEY, None)\n\n    def few_shot_num_examples(self) -&gt; int:\n        \"\"\"Returns how many examples should be given to the model in its instruction prompt\"\"\"\n        return self._prompt_config.get(self.FEW_SHOT_NUM_KEY, 0)\n\n    def vector_store_params(self) -&gt; Dict:\n        \"\"\"Returns any parameters to be passed to the vector store\"\"\"\n        return self._prompt_config.get(self.VECTOR_STORE_PARAMS_KEY, {})\n\n    def example_template(self) -&gt; str:\n        \"\"\"Returns a string containing a template for how examples will be formatted in the prompt\"\"\"\n        example_template = self._prompt_config.get(self.EXAMPLE_TEMPLATE_KEY, None)\n        if not example_template:\n            raise ValueError(\"An example template needs to be specified in the config.\")\n        return example_template\n\n    def output_format(self) -&gt; str:\n        return self._prompt_config.get(self.OUTPUT_FORMAT_KEY, None)\n\n    def output_guidelines(self) -&gt; str:\n        return self._prompt_config.get(self.OUTPUT_GUIDELINE_KEY, None)\n\n    def chain_of_thought(self) -&gt; bool:\n        \"\"\"Returns true if the model is able to perform chain of thought reasoning.\"\"\"\n        return self._prompt_config.get(self.CHAIN_OF_THOUGHT_KEY, False)\n\n    def label_selection(self) -&gt; bool:\n        \"\"\"Returns true if label selection is enabled. Label selection is the process of\n        narrowing down the list of possible labels by similarity to a given input. Useful for\n        classification tasks with a large number of possible classes.\"\"\"\n        return self._prompt_config.get(self.LABEL_SELECTION_KEY, False)\n\n    def max_selected_labels(self) -&gt; int:\n        \"\"\"Returns the number of labels to select in LabelSelector\"\"\"\n        k = self._prompt_config.get(self.LABEL_SELECTION_COUNT_KEY, 10)\n        if k &lt; 1:\n            return len(self.labels_list())\n        return k\n\n    def label_selection_threshold(self) -&gt; float:\n        \"\"\"Returns the threshold for label selection in LabelSelector\n        If the similarity score ratio with the top Score is above this threshold,\n        the label is selected.\"\"\"\n        return self._prompt_config.get(self.LABEL_SELECTION_THRESHOLD, 0.0)\n\n    def attributes(self) -&gt; List[Dict]:\n        \"\"\"Returns a list of attributes to extract from the text.\"\"\"\n        return self._prompt_config.get(self.ATTRIBUTES_KEY, [])\n\n    def transforms(self) -&gt; List[Dict]:\n        \"\"\"Returns a list of transforms to apply to the data before sending to the model.\"\"\"\n        return self.config.get(self.TRANSFORM_KEY, [])\n\n    def dataset_generation_guidelines(self) -&gt; str:\n        \"\"\"Returns a string containing guidelines for how to generate a synthetic dataset\"\"\"\n        return self._dataset_generation_config.get(\n            self.DATASET_GENERATION_GUIDELINES_KEY, \"\"\n        )\n\n    def dataset_generation_num_rows(self) -&gt; int:\n        \"\"\"Returns the number of rows to generate for the synthetic dataset\"\"\"\n        return self._dataset_generation_config.get(\n            self.DATASET_GENERATION_NUM_ROWS_KEY, 1\n        )\n\n    def confidence_chunk_column(self) -&gt; str:\n        \"\"\"Returns the column name to use for confidence chunking\"\"\"\n        return self._chunking_config.get(self.CONFIDENCE_CHUNK_COLUMN_KEY)\n\n    def confidence_chunk_size(self) -&gt; int:\n        \"\"\"Returns the chunk size for confidence chunking\"\"\"\n        return self._chunking_config.get(self.CONFIDENCE_CHUNK_SIZE_KEY, 3400)\n\n    def confidence_merge_function(self) -&gt; str:\n        \"\"\"Returns the function to use when merging confidence scores\"\"\"\n        return self._chunking_config.get(self.CONFIDENCE_MERGE_FUNCTION_KEY, \"max\")\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.attributes","title":"<code>attributes()</code>","text":"<p>Returns a list of attributes to extract from the text.</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def attributes(self) -&gt; List[Dict]:\n    \"\"\"Returns a list of attributes to extract from the text.\"\"\"\n    return self._prompt_config.get(self.ATTRIBUTES_KEY, [])\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.chain_of_thought","title":"<code>chain_of_thought()</code>","text":"<p>Returns true if the model is able to perform chain of thought reasoning.</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def chain_of_thought(self) -&gt; bool:\n    \"\"\"Returns true if the model is able to perform chain of thought reasoning.\"\"\"\n    return self._prompt_config.get(self.CHAIN_OF_THOUGHT_KEY, False)\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.confidence","title":"<code>confidence()</code>","text":"<p>Returns true if the model is able to return a confidence score along with its predictions</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def confidence(self) -&gt; bool:\n    \"\"\"Returns true if the model is able to return a confidence score along with its predictions\"\"\"\n    return self._model_config.get(self.COMPUTE_CONFIDENCE_KEY, False)\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.confidence_chunk_column","title":"<code>confidence_chunk_column()</code>","text":"<p>Returns the column name to use for confidence chunking</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def confidence_chunk_column(self) -&gt; str:\n    \"\"\"Returns the column name to use for confidence chunking\"\"\"\n    return self._chunking_config.get(self.CONFIDENCE_CHUNK_COLUMN_KEY)\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.confidence_chunk_size","title":"<code>confidence_chunk_size()</code>","text":"<p>Returns the chunk size for confidence chunking</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def confidence_chunk_size(self) -&gt; int:\n    \"\"\"Returns the chunk size for confidence chunking\"\"\"\n    return self._chunking_config.get(self.CONFIDENCE_CHUNK_SIZE_KEY, 3400)\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.confidence_merge_function","title":"<code>confidence_merge_function()</code>","text":"<p>Returns the function to use when merging confidence scores</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def confidence_merge_function(self) -&gt; str:\n    \"\"\"Returns the function to use when merging confidence scores\"\"\"\n    return self._chunking_config.get(self.CONFIDENCE_MERGE_FUNCTION_KEY, \"max\")\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.dataset_generation_guidelines","title":"<code>dataset_generation_guidelines()</code>","text":"<p>Returns a string containing guidelines for how to generate a synthetic dataset</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def dataset_generation_guidelines(self) -&gt; str:\n    \"\"\"Returns a string containing guidelines for how to generate a synthetic dataset\"\"\"\n    return self._dataset_generation_config.get(\n        self.DATASET_GENERATION_GUIDELINES_KEY, \"\"\n    )\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.dataset_generation_num_rows","title":"<code>dataset_generation_num_rows()</code>","text":"<p>Returns the number of rows to generate for the synthetic dataset</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def dataset_generation_num_rows(self) -&gt; int:\n    \"\"\"Returns the number of rows to generate for the synthetic dataset\"\"\"\n    return self._dataset_generation_config.get(\n        self.DATASET_GENERATION_NUM_ROWS_KEY, 1\n    )\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.delimiter","title":"<code>delimiter()</code>","text":"<p>Returns the token used to seperate cells in the dataset. Defaults to a comma ','</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def delimiter(self) -&gt; str:\n    \"\"\"Returns the token used to seperate cells in the dataset. Defaults to a comma ','\"\"\"\n    return self._dataset_config.get(self.DELIMITER_KEY, \",\")\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.disable_quoting","title":"<code>disable_quoting()</code>","text":"<p>Returns true if quoting is disabled. Defaults to false</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def disable_quoting(self) -&gt; bool:\n    \"\"\"Returns true if quoting is disabled. Defaults to false\"\"\"\n    return self._dataset_config.get(self.DISABLE_QUOTING, False)\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.embedding_model_name","title":"<code>embedding_model_name()</code>","text":"<p>Returns the name of the model being used for computing embeddings (e.g. sentence-transformers/all-mpnet-base-v2)</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def embedding_model_name(self) -&gt; str:\n    \"\"\"Returns the name of the model being used for computing embeddings (e.g. sentence-transformers/all-mpnet-base-v2)\"\"\"\n    return self._embedding_config.get(self.EMBEDDING_MODEL_NAME_KEY, None)\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.embedding_provider","title":"<code>embedding_provider()</code>","text":"<p>Returns the name of the entity that provides the model used for computing embeddings</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def embedding_provider(self) -&gt; str:\n    \"\"\"Returns the name of the entity that provides the model used for computing embeddings\"\"\"\n    return self._embedding_config.get(self.EMBEDDING_PROVIDER_KEY, self.provider())\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.example_template","title":"<code>example_template()</code>","text":"<p>Returns a string containing a template for how examples will be formatted in the prompt</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def example_template(self) -&gt; str:\n    \"\"\"Returns a string containing a template for how examples will be formatted in the prompt\"\"\"\n    example_template = self._prompt_config.get(self.EXAMPLE_TEMPLATE_KEY, None)\n    if not example_template:\n        raise ValueError(\"An example template needs to be specified in the config.\")\n    return example_template\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.explanation_column","title":"<code>explanation_column()</code>","text":"<p>Returns the name of the column containing an explanation as to why the data is labeled a certain way</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def explanation_column(self) -&gt; str:\n    \"\"\"Returns the name of the column containing an explanation as to why the data is labeled a certain way\"\"\"\n    return self._dataset_config.get(self.EXPLANATION_COLUMN_KEY, None)\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.few_shot_algorithm","title":"<code>few_shot_algorithm()</code>","text":"<p>Returns which algorithm is being used to construct the set of examples being given to the model about the labeling task</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def few_shot_algorithm(self) -&gt; str:\n    \"\"\"Returns which algorithm is being used to construct the set of examples being given to the model about the labeling task\"\"\"\n    return self._prompt_config.get(self.FEW_SHOT_SELECTION_ALGORITHM_KEY, None)\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.few_shot_example_set","title":"<code>few_shot_example_set()</code>","text":"<p>Returns examples of how data should be labeled, used to guide context to the model about the task it is performing</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def few_shot_example_set(self) -&gt; Union[str, List]:\n    \"\"\"Returns examples of how data should be labeled, used to guide context to the model about the task it is performing\"\"\"\n    return self._prompt_config.get(self.FEW_SHOT_EXAMPLE_SET_KEY, [])\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.few_shot_num_examples","title":"<code>few_shot_num_examples()</code>","text":"<p>Returns how many examples should be given to the model in its instruction prompt</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def few_shot_num_examples(self) -&gt; int:\n    \"\"\"Returns how many examples should be given to the model in its instruction prompt\"\"\"\n    return self._prompt_config.get(self.FEW_SHOT_NUM_KEY, 0)\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.image_column","title":"<code>image_column()</code>","text":"<p>Returns the name of the column containing an image url for the given item</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def image_column(self) -&gt; str:\n    \"\"\"Returns the name of the column containing an image url for the given item\"\"\"\n    return self._dataset_config.get(self.IMAGE_COLUMN_KEY, None)\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.input_columns","title":"<code>input_columns()</code>","text":"<p>Returns the names of the input columns from the dataset that are used in the prompt</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def input_columns(self) -&gt; List[str]:\n    \"\"\"Returns the names of the input columns from the dataset that are used in the prompt\"\"\"\n    return self._dataset_config.get(self.INPUT_COLUMNS_KEY, [])\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.label_column","title":"<code>label_column()</code>","text":"<p>Returns the name of the column containing labels for the dataset. Used for comparing accuracy of autolabel results vs ground truth</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def label_column(self) -&gt; str:\n    \"\"\"Returns the name of the column containing labels for the dataset. Used for comparing accuracy of autolabel results vs ground truth\"\"\"\n    return self._dataset_config.get(self.LABEL_COLUMN_KEY, None)\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.label_descriptions","title":"<code>label_descriptions()</code>","text":"<p>Returns a dict of label descriptions</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def label_descriptions(self) -&gt; Dict[str, str]:\n    \"\"\"Returns a dict of label descriptions\"\"\"\n    if isinstance(self._prompt_config.get(self.VALID_LABELS_KEY, []), List):\n        return {}\n    else:\n        return self._prompt_config.get(self.VALID_LABELS_KEY, {})\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.label_selection","title":"<code>label_selection()</code>","text":"<p>Returns true if label selection is enabled. Label selection is the process of narrowing down the list of possible labels by similarity to a given input. Useful for classification tasks with a large number of possible classes.</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def label_selection(self) -&gt; bool:\n    \"\"\"Returns true if label selection is enabled. Label selection is the process of\n    narrowing down the list of possible labels by similarity to a given input. Useful for\n    classification tasks with a large number of possible classes.\"\"\"\n    return self._prompt_config.get(self.LABEL_SELECTION_KEY, False)\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.label_selection_threshold","title":"<code>label_selection_threshold()</code>","text":"<p>Returns the threshold for label selection in LabelSelector If the similarity score ratio with the top Score is above this threshold, the label is selected.</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def label_selection_threshold(self) -&gt; float:\n    \"\"\"Returns the threshold for label selection in LabelSelector\n    If the similarity score ratio with the top Score is above this threshold,\n    the label is selected.\"\"\"\n    return self._prompt_config.get(self.LABEL_SELECTION_THRESHOLD, 0.0)\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.label_separator","title":"<code>label_separator()</code>","text":"<p>Returns the token used to seperate multiple labels in the dataset. Defaults to a semicolon ';'</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def label_separator(self) -&gt; str:\n    \"\"\"Returns the token used to seperate multiple labels in the dataset. Defaults to a semicolon ';'\"\"\"\n    return self._dataset_config.get(self.LABEL_SEPARATOR_KEY, \";\")\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.labels_list","title":"<code>labels_list()</code>","text":"<p>Returns a list of valid labels</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def labels_list(self) -&gt; List[str]:\n    \"\"\"Returns a list of valid labels\"\"\"\n    if isinstance(self._prompt_config.get(self.VALID_LABELS_KEY, []), List):\n        return self._prompt_config.get(self.VALID_LABELS_KEY, [])\n    else:\n        return list(self._prompt_config.get(self.VALID_LABELS_KEY, {}).keys())\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.logit_bias","title":"<code>logit_bias()</code>","text":"<p>Returns the logit bias for the labels specified in the config</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def logit_bias(self) -&gt; float:\n    \"\"\"Returns the logit bias for the labels specified in the config\"\"\"\n    return self._model_config.get(self.LOGIT_BIAS_KEY, 0.0)\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.max_selected_labels","title":"<code>max_selected_labels()</code>","text":"<p>Returns the number of labels to select in LabelSelector</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def max_selected_labels(self) -&gt; int:\n    \"\"\"Returns the number of labels to select in LabelSelector\"\"\"\n    k = self._prompt_config.get(self.LABEL_SELECTION_COUNT_KEY, 10)\n    if k &lt; 1:\n        return len(self.labels_list())\n    return k\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.model_name","title":"<code>model_name()</code>","text":"<p>Returns the name of the model being used for labeling (e.g. gpt-4, claude-v1)</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def model_name(self) -&gt; str:\n    \"\"\"Returns the name of the model being used for labeling (e.g. gpt-4, claude-v1)\"\"\"\n    return self._model_config[self.MODEL_NAME_KEY]\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.model_params","title":"<code>model_params()</code>","text":"<p>Returns a dict of configured settings for the model (e.g. hyperparameters)</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def model_params(self) -&gt; Dict:\n    \"\"\"Returns a dict of configured settings for the model (e.g. hyperparameters)\"\"\"\n    return self._model_config.get(self.MODEL_PARAMS_KEY, {})\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.provider","title":"<code>provider()</code>","text":"<p>Returns the name of the entity that provides the currently configured model (e.g. OpenAI, Anthropic, Refuel)</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def provider(self) -&gt; str:\n    \"\"\"Returns the name of the entity that provides the currently configured model (e.g. OpenAI, Anthropic, Refuel)\"\"\"\n    return self._model_config[self.PROVIDER_KEY]\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.task_type","title":"<code>task_type()</code>","text":"<p>Returns the type of task we have configured the labeler to perform (e.g. Classification, Question Answering)</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def task_type(self) -&gt; str:\n    \"\"\"Returns the type of task we have configured the labeler to perform (e.g. Classification, Question Answering)\"\"\"\n    return self.config[self.TASK_TYPE_KEY]\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.text_column","title":"<code>text_column()</code>","text":"<p>Returns the name of the column containing text data we intend to label</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def text_column(self) -&gt; str:\n    \"\"\"Returns the name of the column containing text data we intend to label\"\"\"\n    return self._dataset_config.get(self.TEXT_COLUMN_KEY, None)\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.transforms","title":"<code>transforms()</code>","text":"<p>Returns a list of transforms to apply to the data before sending to the model.</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def transforms(self) -&gt; List[Dict]:\n    \"\"\"Returns a list of transforms to apply to the data before sending to the model.\"\"\"\n    return self.config.get(self.TRANSFORM_KEY, [])\n</code></pre>"},{"location":"reference/configs/#src.autolabel.configs.config.AutolabelConfig.vector_store_params","title":"<code>vector_store_params()</code>","text":"<p>Returns any parameters to be passed to the vector store</p> Source code in <code>src/autolabel/configs/config.py</code> <pre><code>def vector_store_params(self) -&gt; Dict:\n    \"\"\"Returns any parameters to be passed to the vector store\"\"\"\n    return self._prompt_config.get(self.VECTOR_STORE_PARAMS_KEY, {})\n</code></pre>"},{"location":"reference/data_models/","title":"Data Models","text":"<p>The Data Model classes are used to save the progress of AutoLabel jobs in an SQL database.</p> <p>Saved data is stored in .autolabel.db</p> <p>Every Data Model class implements its own \"get\" and \"create\" methods for accessing this saved data.</p> <p>             Bases: <code>Base</code></p> Source code in <code>src/autolabel/data_models/annotation.py</code> <pre><code>class AnnotationModel(Base):\n    __tablename__ = \"annotations\"\n\n    id = Column(Integer, primary_key=True, autoincrement=True)\n    created_at = Column(DateTime(timezone=True), server_default=func.now())\n    index = Column(Integer)\n    llm_annotation = Column(TEXT)\n    task_run_id = Column(Integer, ForeignKey(\"task_runs.id\"))\n    task_runs = relationship(\"TaskRunModel\", back_populates=\"annotations\")\n\n    def __repr__(self):\n        return f\"&lt;AnnotationModel(id={self.id}, index={self.index}, annotation={self.llm_annotation})\"\n\n    @classmethod\n    def create_from_llm_annotation(\n        cls, db, llm_annotation: LLMAnnotation, index: int, task_run_id: int\n    ):\n        db_object = cls(\n            llm_annotation=pickle.dumps(llm_annotation),\n            index=index,\n            task_run_id=task_run_id,\n        )\n        db.add(db_object)\n        db.commit()\n        db_object = db.query(cls).order_by(cls.id.desc()).first()\n        logger.debug(f\"created new annotation: {db_object}\")\n        return db_object\n\n    @classmethod\n    def get_annotations_by_task_run_id(cls, db, task_run_id: int):\n        annotations = (\n            db.query(cls)\n            .filter(cls.task_run_id == task_run_id)\n            .order_by(cls.index)\n            .all()\n        )\n        filtered_annotations = []\n        ids = {}\n        for annotation in annotations:\n            if annotation.index not in ids:\n                ids[annotation.index] = True\n                filtered_annotations.append(annotation)\n        return filtered_annotations\n\n    @classmethod\n    def from_pydantic(cls, annotation: BaseModel):\n        return cls(**json.loads(annotation.json()))\n\n    def delete(self, db):\n        db.delete(self)\n        db.commit()\n</code></pre> <p>rendering: show_root_heading: yes show_root_full_path: no</p> <p>             Bases: <code>Base</code></p> <p>an SQLAlchemy based Cache system for storing and retriving CacheEntries</p> Source code in <code>src/autolabel/data_models/generation_cache.py</code> <pre><code>class GenerationCacheEntryModel(Base):\n    \"\"\"an SQLAlchemy based Cache system for storing and retriving CacheEntries\"\"\"\n\n    __tablename__ = \"generation_cache\"\n\n    id = Column(Integer, primary_key=True)\n    model_name = Column(String(50))\n    prompt = Column(Text)\n    model_params = Column(Text)\n    generations = Column(JSON)\n    creation_time_ms = Column(Integer)\n    ttl_ms = Column(Integer)\n\n    def __repr__(self):\n        return f\"&lt;Cache(model_name={self.model_name},prompt={self.prompt},model_params={self.model_params},generations={self.generations})&gt;\"\n\n    @classmethod\n    def get(cls, db, cache_entry: GenerationCacheEntry):\n        looked_up_entry = (\n            db.query(cls)\n            .filter(\n                cls.model_name == cache_entry.model_name,\n                cls.prompt == cache_entry.prompt,\n                cls.model_params == cache_entry.model_params,\n            )\n            .first()\n        )\n\n        if not looked_up_entry:\n            return None\n\n        generations = json.loads(looked_up_entry.generations)[\"generations\"]\n        generations = [\n            Generation(**gen) if gen[\"type\"] == \"Generation\" else ChatGeneration(**gen)\n            for gen in generations\n        ]\n\n        entry = GenerationCacheEntry(\n            model_name=looked_up_entry.model_name,\n            prompt=looked_up_entry.prompt,\n            model_params=looked_up_entry.model_params,\n            generations=generations,\n            creation_time_ms=looked_up_entry.creation_time_ms,\n            ttl_ms=looked_up_entry.ttl_ms,\n        )\n        return entry\n\n    @classmethod\n    def insert(cls, db, cache_entry: BaseModel):\n        generations = {\"generations\": [gen.dict() for gen in cache_entry.generations]}\n        db_object = cls(\n            model_name=cache_entry.model_name,\n            prompt=cache_entry.prompt,\n            model_params=cache_entry.model_params,\n            generations=json.dumps(generations),\n            creation_time_ms=int(time.time() * 1000),\n            ttl_ms=cache_entry.ttl_ms,\n        )\n        db.add(db_object)\n        db.commit()\n        return cache_entry\n\n    @classmethod\n    def clear(cls, db, use_ttl: bool = True) -&gt; None:\n        if use_ttl:\n            current_time_ms = int(time.time() * 1000)\n            db.query(cls).filter(\n                current_time_ms - cls.creation_time_ms &gt; cls.ttl_ms\n            ).delete()\n        else:\n            db.query(cls).delete()\n        db.commit()\n</code></pre> <p>rendering: show_root_heading: yes show_root_full_path: no</p> <p>             Bases: <code>Base</code></p> <p>an SQLAlchemy based Cache system for storing and retriving CacheEntries</p> Source code in <code>src/autolabel/data_models/transform_cache.py</code> <pre><code>class TransformCacheEntryModel(Base):\n    \"\"\"an SQLAlchemy based Cache system for storing and retriving CacheEntries\"\"\"\n\n    __tablename__ = \"transform_cache\"\n\n    id = Column(String, primary_key=True)\n    transform_name = Column(String(50))\n    transform_params = Column(TEXT)\n    input = Column(TEXT)\n    output = Column(TEXT)\n    creation_time_ms = Column(Integer)\n    ttl_ms = Column(Integer)\n\n    def __repr__(self):\n        return f\"&lt;TransformCache(id={self.id},transform_name={self.transform_name},transform_params={self.transform_params},input={self.input},output={self.output})&gt;\"\n\n    @classmethod\n    def get(cls, db, cache_entry: TransformCacheEntry) -&gt; TransformCacheEntry:\n        id = cache_entry.get_id()\n        looked_up_entry = db.query(cls).filter(cls.id == id).first()\n\n        if not looked_up_entry:\n            return None\n\n        entry = TransformCacheEntry(\n            transform_name=looked_up_entry.transform_name,\n            transform_params=pickle.loads(looked_up_entry.transform_params),\n            input=pickle.loads(looked_up_entry.input),\n            output=pickle.loads(looked_up_entry.output),\n            creation_time_ms=looked_up_entry.creation_time_ms,\n            ttl_ms=looked_up_entry.ttl_ms,\n        )\n        return entry\n\n    @classmethod\n    def insert(cls, db, cache_entry: TransformCacheEntry) -&gt; None:\n        db_object = cls(\n            id=cache_entry.get_id(),\n            transform_name=cache_entry.transform_name,\n            transform_params=pickle.dumps(cache_entry.transform_params),\n            input=pickle.dumps(cache_entry.input),\n            output=pickle.dumps(cache_entry.output),\n            creation_time_ms=int(time.time() * 1000),\n            ttl_ms=cache_entry.ttl_ms,\n        )\n        db.add(db_object)\n        db.commit()\n        return db_object\n\n    @classmethod\n    def clear(cls, db, use_ttl: bool = True) -&gt; None:\n        if use_ttl:\n            current_time_ms = int(time.time() * 1000)\n            db.query(cls).filter(\n                current_time_ms - cls.creation_time_ms &gt; cls.ttl_ms\n            ).delete()\n        else:\n            db.query(cls).delete()\n        db.commit()\n</code></pre> <p>rendering: show_root_heading: yes show_root_full_path: no</p> <p>             Bases: <code>Base</code></p> Source code in <code>src/autolabel/data_models/dataset.py</code> <pre><code>class DatasetModel(Base):\n    __tablename__ = \"datasets\"\n\n    id = Column(String(32), primary_key=True)\n    input_file = Column(String(50))\n    start_index = Column(Integer)\n    end_index = Column(Integer)\n    task_runs = relationship(\"TaskRunModel\", back_populates=\"dataset\")\n\n    def __repr__(self):\n        return f\"&lt;DatasetModel(id={self.id}, input_file={self.input_file}, start_index={self.start_index}, end_index={self.end_index})&gt;\"\n\n    @classmethod\n    def create(cls, db, dataset: BaseModel):\n        db_object = cls(**json.loads(dataset.json()))\n        db.add(db_object)\n        db.commit()\n        return db_object\n\n    @classmethod\n    def get_by_id(cls, db, id: int):\n        return db.query(cls).filter(cls.id == id).first()\n\n    @classmethod\n    def get_by_input_file(cls, db, input_file: str):\n        return db.query(cls).filter(cls.input_file == input_file).first()\n\n    def delete(self, db):\n        db.delete(self)\n        db.commit()\n</code></pre> <p>rendering: show_root_heading: yes show_root_full_path: no</p> <p>             Bases: <code>Base</code></p> Source code in <code>src/autolabel/data_models/task.py</code> <pre><code>class TaskModel(Base):\n    __tablename__ = \"tasks\"\n\n    id = Column(String(32), primary_key=True)\n    task_type = Column(String(50))\n    provider = Column(String(50))\n    model_name = Column(String(50))\n    config = Column(Text)\n    task_runs = relationship(\"TaskRunModel\", back_populates=\"task\")\n\n    def __repr__(self):\n        return f\"&lt;TaskModel(id={self.id}, task_type={self.task_type}, provider={self.provider}, model_name={self.model_name})&gt;\"\n\n    @classmethod\n    def create(cls, db, task: BaseModel):\n        db_object = cls(**json.loads(task.json()))\n        db.add(db_object)\n        db.commit()\n        return db_object\n\n    @classmethod\n    def get_by_id(cls, db, id: int):\n        return db.query(cls).filter(cls.id == id).first()\n\n    def delete(self, db):\n        db.delete(self)\n        db.commit()\n</code></pre> <p>rendering: show_root_heading: yes show_root_full_path: no</p> <p>             Bases: <code>Base</code></p> Source code in <code>src/autolabel/data_models/task_run.py</code> <pre><code>class TaskRunModel(Base):\n    __tablename__ = \"task_runs\"\n\n    id = Column(\n        Integer,\n        default=lambda: uuid.uuid4().int &gt;&gt; (128 - 32),\n        primary_key=True,\n    )\n    task_id = Column(String(32), ForeignKey(\"tasks.id\"))\n    created_at = Column(DateTime(timezone=True), server_default=func.now())\n    dataset_id = Column(String(32), ForeignKey(\"datasets.id\"))\n    current_index = Column(Integer)\n    error = Column(String(256))\n    metrics = Column(Text)\n    output_file = Column(String(50))\n    status = Column(String(50))\n    task = relationship(\"TaskModel\", back_populates=\"task_runs\")\n    dataset = relationship(\"DatasetModel\", back_populates=\"task_runs\")\n    annotations = relationship(\"AnnotationModel\", back_populates=\"task_runs\")\n\n    def __repr__(self):\n        return f\"&lt;TaskRunModel(id={self.id}, created_at={str(self.created_at)}, task_id={self.task_id}, dataset_id={self.dataset_id}, output_file={self.output_file}, current_index={self.current_index}, status={self.status}, error={self.error}, metrics={self.metrics})\"\n\n    @classmethod\n    def create(cls, db, task_run: BaseModel):\n        logger.debug(f\"creating new task: {task_run}\")\n        db_object = cls(**task_run.dict())\n        db.add(db_object)\n        db.commit()\n        db.refresh(db_object)\n        logger.debug(f\"created new task: {db_object}\")\n        return db_object\n\n    @classmethod\n    def get(cls, db, task_id: str, dataset_id: str):\n        return (\n            db.query(cls)\n            .filter(cls.task_id == task_id, cls.dataset_id == dataset_id)\n            .first()\n        )\n\n    @classmethod\n    def from_pydantic(cls, task_run: BaseModel):\n        return cls(**json.loads(task_run.json()))\n\n    @classmethod\n    def update(cls, db, task_run: BaseModel):\n        task_run_id = task_run.id\n        task_run_orm = db.query(cls).filter(cls.id == task_run_id).first()\n        logger.debug(f\"updating task_run: {task_run}\")\n        for key, value in task_run.dict().items():\n            setattr(task_run_orm, key, value)\n        db.commit()\n        logger.debug(f\"task_run updated: {task_run}\")\n        return TaskRun.from_orm(task_run_orm)\n\n    @classmethod\n    def delete_by_id(cls, db, id: int):\n        db.query(cls).filter(cls.id == id).delete()\n\n    def delete(self, db):\n        db.delete(self)\n        db.commit()\n</code></pre> <p>rendering: show_root_heading: yes show_root_full_path: no</p> Source code in <code>src/autolabel/database/state_manager.py</code> <pre><code>class StateManager:\n    def __init__(self):\n        self.engine = create_db_engine()\n        self.base = Base\n        self.session = None\n\n    def initialize(self):\n        self.base.metadata.create_all(self.engine)\n        self.session = sessionmaker(bind=self.engine)()\n\n    def initialize_dataset(\n        self,\n        dataset: Union[str, pd.DataFrame],\n        config: AutolabelConfig,\n        start_index: int = 0,\n        max_items: Optional[int] = None,\n    ):\n        # TODO: Check if this works for max_items = None\n\n        dataset_id = Dataset.create_id(dataset, config, start_index, max_items)\n        dataset_orm = DatasetModel.get_by_id(self.session, dataset_id)\n        if dataset_orm:\n            return Dataset.from_orm(dataset_orm)\n\n        dataset = Dataset(\n            id=dataset_id,\n            input_file=dataset if isinstance(dataset, str) else \"\",\n            start_index=start_index,\n            end_index=start_index + max_items if max_items else -1,\n        )\n        return Dataset.from_orm(DatasetModel.create(self.session, dataset))\n\n    def initialize_task(self, config: AutolabelConfig):\n        task_id = Task.create_id(config)\n        task_orm = TaskModel.get_by_id(self.session, task_id)\n        if task_orm:\n            return Task.from_orm(task_orm)\n\n        task = Task(\n            id=task_id,\n            config=config.to_json(),\n            task_type=config.task_type(),\n            provider=config.provider(),\n            model_name=config.model_name(),\n        )\n        return Task.from_orm(TaskModel.create(self.session, task))\n\n    def get_task_run(self, task_id: str, dataset_id: str):\n        task_run_orm = TaskRunModel.get(self.session, task_id, dataset_id)\n        if task_run_orm:\n            return TaskRun.from_orm(task_run_orm)\n        else:\n            return None\n\n    def create_task_run(\n        self, output_file: str, task_id: str, dataset_id: str\n    ) -&gt; TaskRun:\n        logger.debug(f\"creating new task_run\")\n        new_task_run = TaskRun(\n            task_id=task_id,\n            dataset_id=dataset_id,\n            status=TaskStatus.ACTIVE,\n            current_index=0,\n            output_file=output_file,\n            created_at=datetime.now(),\n        )\n        task_run_orm = TaskRunModel.create(self.session, new_task_run)\n        return TaskRun.from_orm(task_run_orm)\n</code></pre> <p>rendering: show_root_heading: yes show_root_full_path: no</p> Source code in <code>src/autolabel/database/engine.py</code> <pre><code>def create_db_engine(db_path: Optional[str] = DB_PATH) -&gt; Engine:\n    global DB_ENGINE\n    if DB_ENGINE is None:\n        DB_ENGINE = create_engine(f\"sqlite:///{db_path}\", pool_size=0)\n    return DB_ENGINE\n</code></pre> <p>rendering: show_root_heading: yes show_root_full_path: no</p>"},{"location":"reference/example_select/","title":"Example Selector","text":"<p>             Bases: <code>BaseExampleSelector</code>, <code>BaseModel</code></p> <p>Example selector to handle the case of fixed few-shot context i.e. every input prompt to the labeling model has the same few-shot examples</p> Source code in <code>src/autolabel/few_shot/fixed_example_selector.py</code> <pre><code>class FixedExampleSelector(BaseExampleSelector, BaseModel):\n    \"\"\"Example selector to handle the case of fixed few-shot context\n    i.e. every input prompt to the labeling model has the same few-shot examples\n    \"\"\"\n\n    examples: List[dict]\n    \"\"\"A list of the examples that the prompt template expects.\"\"\"\n\n    k: int = 4\n    \"\"\"Number of examples to select\"\"\"\n\n    class Config:\n        \"\"\"Configuration for this pydantic object.\"\"\"\n\n        extra = Extra.forbid\n        arbitrary_types_allowed = True\n\n    def add_example(self, example: Dict[str, str]) -&gt; None:\n        self.examples.append(example)\n\n    def select_examples(\n        self,\n        input_variables: Dict[str, str],\n        **kwargs,\n    ) -&gt; List[dict]:\n        \"\"\"Select which examples to use based on the input lengths.\"\"\"\n        label_column = kwargs.get(\"label_column\")\n        selected_labels = kwargs.get(\"selected_labels\")\n\n        if not selected_labels:\n            return self.examples[: self.k]\n\n        if not label_column:\n            print(\"No label column provided, returning all examples\")\n            return self.examples[: self.k]\n\n        # get the examples where label matches the selected labels\n        valid_examples = [\n            example\n            for example in self.examples\n            if example.get(label_column) in selected_labels\n        ]\n        return valid_examples[: min(self.k, len(valid_examples))]\n\n    @classmethod\n    def from_examples(\n        cls,\n        examples: List,\n        k: int = 4,\n    ) -&gt; FixedExampleSelector:\n        \"\"\"Create pass-through example selector using example list\n\n        Returns:\n            The FixedExampleSelector instantiated\n        \"\"\"\n\n        return cls(examples=examples, k=k)\n</code></pre>"},{"location":"reference/example_select/#src.autolabel.few_shot.fixed_example_selector.FixedExampleSelector.examples","title":"<code>examples: List[dict]</code>  <code>instance-attribute</code>","text":"<p>A list of the examples that the prompt template expects.</p>"},{"location":"reference/example_select/#src.autolabel.few_shot.fixed_example_selector.FixedExampleSelector.k","title":"<code>k: int = 4</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of examples to select</p>"},{"location":"reference/example_select/#src.autolabel.few_shot.fixed_example_selector.FixedExampleSelector.Config","title":"<code>Config</code>","text":"<p>Configuration for this pydantic object.</p> Source code in <code>src/autolabel/few_shot/fixed_example_selector.py</code> <pre><code>class Config:\n    \"\"\"Configuration for this pydantic object.\"\"\"\n\n    extra = Extra.forbid\n    arbitrary_types_allowed = True\n</code></pre>"},{"location":"reference/example_select/#src.autolabel.few_shot.fixed_example_selector.FixedExampleSelector.from_examples","title":"<code>from_examples(examples, k=4)</code>  <code>classmethod</code>","text":"<p>Create pass-through example selector using example list</p> <p>Returns:</p> Type Description <code>FixedExampleSelector</code> <p>The FixedExampleSelector instantiated</p> Source code in <code>src/autolabel/few_shot/fixed_example_selector.py</code> <pre><code>@classmethod\ndef from_examples(\n    cls,\n    examples: List,\n    k: int = 4,\n) -&gt; FixedExampleSelector:\n    \"\"\"Create pass-through example selector using example list\n\n    Returns:\n        The FixedExampleSelector instantiated\n    \"\"\"\n\n    return cls(examples=examples, k=k)\n</code></pre>"},{"location":"reference/example_select/#src.autolabel.few_shot.fixed_example_selector.FixedExampleSelector.select_examples","title":"<code>select_examples(input_variables, **kwargs)</code>","text":"<p>Select which examples to use based on the input lengths.</p> Source code in <code>src/autolabel/few_shot/fixed_example_selector.py</code> <pre><code>def select_examples(\n    self,\n    input_variables: Dict[str, str],\n    **kwargs,\n) -&gt; List[dict]:\n    \"\"\"Select which examples to use based on the input lengths.\"\"\"\n    label_column = kwargs.get(\"label_column\")\n    selected_labels = kwargs.get(\"selected_labels\")\n\n    if not selected_labels:\n        return self.examples[: self.k]\n\n    if not label_column:\n        print(\"No label column provided, returning all examples\")\n        return self.examples[: self.k]\n\n    # get the examples where label matches the selected labels\n    valid_examples = [\n        example\n        for example in self.examples\n        if example.get(label_column) in selected_labels\n    ]\n    return valid_examples[: min(self.k, len(valid_examples))]\n</code></pre>"},{"location":"reference/example_select/#src.autolabel.few_shot.vector_store.VectorStoreWrapper","title":"<code>VectorStoreWrapper</code>","text":"<p>             Bases: <code>VectorStore</code></p> Source code in <code>src/autolabel/few_shot/vector_store.py</code> <pre><code>class VectorStoreWrapper(VectorStore):\n    def __init__(\n        self,\n        embedding_function: Optional[Embeddings] = None,\n        corpus_embeddings: Optional[Tensor] = None,\n        texts: Optional[List[str]] = None,\n        metadatas: Optional[List[Dict[str, str]]] = None,\n        cache: bool = True,\n    ) -&gt; None:\n        self._embedding_function = embedding_function\n        self._corpus_embeddings = corpus_embeddings\n        self._texts = texts\n        self._metadatas = metadatas\n        if cache:\n            self._db_engine = create_db_engine()\n            with self._db_engine.connect() as conn:\n                query = f\"CREATE TABLE IF NOT EXISTS {EMBEDDINGS_TABLE} (embedding_function TEXT, text TEXT, embedding BLOB)\"\n                conn.execute(sql_text(query))\n                conn.commit()\n        else:\n            self._db_engine = None\n\n    def _get_embeddings(self, texts: Iterable[str]) -&gt; List[List[float]]:\n        \"\"\"Get embeddings from the database. If not found, compute them and add them to the database.\n\n        If no database is used, compute the embeddings and return them.\n\n        Args:\n            texts (Iterable[str]): Iterable of texts to embed.\n        Returns:\n            List[List[float]]: List of embeddings.\n        \"\"\"\n        if self._db_engine:\n            with self._db_engine.connect() as conn:\n                embeddings = []\n                uncached_texts = []\n                uncached_texts_indices = []\n                for idx, text in enumerate(texts):\n                    query = sql_text(\n                        f\"SELECT embedding FROM {EMBEDDINGS_TABLE} WHERE embedding_function = :x AND text = :y\",\n                    )\n                    params = {\n                        \"x\": (\n                            self._embedding_function.model\n                            if self._embedding_function.__class__.__name__\n                            not in [\"HuggingFaceEmbeddings\", \"VertexAIEmbeddings\"]\n                            else self._embedding_function.model_name\n                        ),\n                        \"y\": text,\n                    }\n                    result = conn.execute(query, params).fetchone()\n                    if result:\n                        embeddings.append(pickle.loads(result[0]))\n                    else:\n                        embeddings.append(None)\n                        uncached_texts.append(text)\n                        uncached_texts_indices.append(idx)\n\n                uncached_embeddings = self._embedding_function.embed_documents(\n                    uncached_texts\n                )\n                self._add_embeddings_to_cache(uncached_texts, uncached_embeddings)\n                for idx, embedding in zip(uncached_texts_indices, uncached_embeddings):\n                    embeddings[idx] = embedding\n\n                return embeddings\n        else:\n            return self._embedding_function.embed_documents(list(texts))\n\n    def _add_embeddings_to_cache(\n        self, texts: Iterable[str], embeddings: List[List[float]]\n    ) -&gt; None:\n        \"\"\"Save embeddings to the database. If self._db_engine is None, do nothing.\n        Args:\n            texts (Iterable[str]): Iterable of texts.\n            embeddings (List[List[float]]): List of embeddings.\n        \"\"\"\n        if self._db_engine:\n            with self._db_engine.connect() as conn:\n                for text, embedding in zip(texts, embeddings):\n                    query = sql_text(\n                        f\"INSERT INTO {EMBEDDINGS_TABLE} (embedding_function, text, embedding) VALUES (:x, :y, :z)\"\n                    )\n                    params = {\n                        \"x\": (\n                            self._embedding_function.model\n                            if self._embedding_function.__class__.__name__\n                            not in [\"HuggingFaceEmbeddings\", \"VertexAIEmbeddings\"]\n                            else self._embedding_function.model_name\n                        ),\n                        \"y\": text,\n                        \"z\": pickle.dumps(embedding),\n                    }\n                    conn.execute(query, params)\n                    conn.commit()\n\n    def add_texts(\n        self,\n        texts: Iterable[str],\n        metadatas: Optional[List[Dict[str, str]]] = None,\n    ) -&gt; List[str]:\n        \"\"\"Run texts through the embeddings and add to the vectorstore. Currently, the vectorstore is reinitialized each time, because we do not require a persistent vector store for example selection.\n        Args:\n            texts (Iterable[str]): Texts to add to the vectorstore.\n            metadatas (Optional[List[dict]], optional): Optional list of metadatas.\n        Returns:\n            List[str]: List of IDs of the added texts.\n        \"\"\"\n        if self._embedding_function is not None:\n            embeddings = self._get_embeddings(texts)\n\n        self._corpus_embeddings = torch.tensor(embeddings)\n        self._texts = texts\n        self._metadatas = metadatas\n        return metadatas\n\n    def similarity_search(\n        self,\n        query: str,\n        k: int = 4,\n        filter: Optional[Dict[str, str]] = None,\n        **kwargs: Any,\n    ) -&gt; List[Document]:\n        \"\"\"Run semantic similarity search.\n        Args:\n            query (str): Query text to search for.\n            k (int): Number of results to return. Defaults to 4.\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n        Returns:\n            List[Document]: List of documents most similar to the query text.\n        \"\"\"\n        docs_and_scores = self.similarity_search_with_score(query, k, filter=filter)\n        return [doc for doc, _ in docs_and_scores]\n\n    def similarity_search_with_score(\n        self,\n        query: str,\n        k: int = 4,\n        filter: Optional[Dict[str, str]] = None,\n        **kwargs: Any,\n    ) -&gt; List[Tuple[Document, float]]:\n        \"\"\"Run semantic similarity search and retrieve distances.\n        Args:\n            query (str): Query text to search for.\n            k (int): Number of results to return. Defaults to 4.\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n        Returns:\n            List[Tuple[Document, float]]: List of documents most similar to the query\n                text with distance in float.\n        \"\"\"\n        query_embeddings = torch.tensor([self._get_embeddings([query])[0]])\n        result_ids_and_scores = semantic_search(\n            corpus_embeddings=self._corpus_embeddings,\n            query_embeddings=query_embeddings,\n            top_k=k,\n        )\n        result_ids = [result[\"corpus_id\"] for result in result_ids_and_scores[0]]\n        scores = [result[\"score\"] for result in result_ids_and_scores[0]]\n        results = {}\n        results[\"documents\"] = [[self._texts[index] for index in result_ids]]\n        results[\"distances\"] = [scores]\n        results[\"metadatas\"] = [[self._metadatas[index] for index in result_ids]]\n        return _results_to_docs_and_scores(results)\n\n    def label_diversity_similarity_search(\n        self,\n        query: str,\n        label_key: str,\n        k: int = 4,\n        filter: Optional[Dict[str, str]] = None,\n        **kwargs: Any,\n    ) -&gt; List[Document]:\n        \"\"\"Run semantic similarity search.\n        Args:\n            query (str): Query text to search for.\n            k (int): Number of results to return per label.\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n        Returns:\n            List[Document]: List of documents most similar to the query text.\n        \"\"\"\n        docs_and_scores = self.label_diversity_similarity_search_with_score(\n            query, label_key, k, filter=filter\n        )\n        return [doc for doc, _ in docs_and_scores]\n\n    def label_diversity_similarity_search_with_score(\n        self,\n        query: str,\n        label_key: str,\n        k: int = 4,\n        filter: Optional[Dict[str, str]] = None,\n        **kwargs: Any,\n    ) -&gt; List[Tuple[Document, float]]:\n        \"\"\"Run semantic similarity search and retrieve distances.\n        Args:\n            query (str): Query text to search for.\n            k (int): Number of results to return. Defaults to 4.\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n        Returns:\n            List[Tuple[Document, float]]: List of documents most similar to the query\n                text with distance in float.\n        \"\"\"\n        query_embeddings = torch.tensor([self._get_embeddings([query])[0]])\n        data = []\n        data = zip(self._corpus_embeddings, self._texts, self._metadatas)\n        sorted_data = sorted(data, key=lambda item: item[2].get(label_key))\n\n        documents = []\n        scores = []\n        metadatas = []\n        for label, label_examples in groupby(\n            sorted_data, key=lambda item: item[2].get(label_key)\n        ):\n            label_examples_list = list(label_examples)\n            label_embeddings = list(\n                map(lambda label_example: label_example[0], label_examples_list)\n            )\n            label_texts = list(\n                map(lambda label_example: label_example[1], label_examples_list)\n            )\n            label_metadatas = list(\n                map(lambda label_example: label_example[2], label_examples_list)\n            )\n\n            result_ids_and_scores = semantic_search(\n                corpus_embeddings=label_embeddings,\n                query_embeddings=query_embeddings,\n                top_k=k,\n            )\n            result_ids = [result[\"corpus_id\"] for result in result_ids_and_scores[0]]\n            documents.extend([label_texts[index] for index in result_ids])\n            metadatas.extend([label_metadatas[index] for index in result_ids])\n            scores.extend([result[\"score\"] for result in result_ids_and_scores[0]])\n        results = {}\n\n        results[\"documents\"] = [documents]\n        results[\"distances\"] = [scores]\n        results[\"metadatas\"] = [metadatas]\n\n        return _results_to_docs_and_scores(results)\n\n    def max_marginal_relevance_search_by_vector(\n        self,\n        query: str,\n        k: int = 4,\n        fetch_k: int = 20,\n        lambda_mult: float = 0.5,\n        **kwargs: Any,\n    ) -&gt; List[Document]:\n        query_embedding = self._get_embeddings([query])[0]\n        query_embeddings = torch.tensor([query_embedding])\n        result_ids_and_scores = semantic_search(\n            corpus_embeddings=self._corpus_embeddings,\n            query_embeddings=query_embeddings,\n            top_k=fetch_k,\n        )\n        result_ids = [result[\"corpus_id\"] for result in result_ids_and_scores[0]]\n        scores = [result[\"score\"] for result in result_ids_and_scores[0]]\n\n        fetched_embeddings = torch.index_select(\n            input=self._corpus_embeddings, dim=0, index=torch.tensor(result_ids)\n        ).tolist()\n        mmr_selected = maximal_marginal_relevance(\n            np.array([query_embedding], dtype=np.float32),\n            fetched_embeddings,\n            k=k,\n            lambda_mult=lambda_mult,\n        )\n        selected_result_ids = [result_ids[i] for i in mmr_selected]\n        selected_scores = [scores[i] for i in mmr_selected]\n        results = {}\n        results[\"documents\"] = [[self._texts[index] for index in selected_result_ids]]\n        results[\"distances\"] = [selected_scores]\n        results[\"metadatas\"] = [\n            [self._metadatas[index] for index in selected_result_ids]\n        ]\n\n        return _results_to_docs_and_scores(results)\n\n    def max_marginal_relevance_search(\n        self,\n        query: str,\n        k: int = 4,\n        fetch_k: int = 20,\n        lambda_mult: float = 0.5,\n        **kwargs: Any,\n    ) -&gt; List[Document]:\n        docs_and_scores = self.max_marginal_relevance_search_by_vector(\n            query, k, fetch_k, lambda_mult=lambda_mult\n        )\n        return [doc for doc, _ in docs_and_scores]\n\n    @classmethod\n    def from_texts(\n        cls: Type[VectorStoreWrapper],\n        texts: List[str],\n        embedding: Optional[Embeddings] = None,\n        metadatas: Optional[List[dict]] = None,\n        cache: bool = True,\n        **kwargs: Any,\n    ) -&gt; VectorStoreWrapper:\n        \"\"\"Create a vectorstore from raw text.\n        The data will be ephemeral in-memory.\n        Args:\n            texts (List[str]): List of texts to add to the collection.\n            embedding (Optional[Embeddings]): Embedding function. Defaults to None.\n            metadatas (Optional[List[dict]]): List of metadatas. Defaults to None.\n            cache (bool): Whether to cache the embeddings. Defaults to True.\n        Returns:\n            vector_store: Vectorstore with seedset embeddings\n        \"\"\"\n        vector_store = cls(\n            embedding_function=embedding,\n            corpus_embeddings=None,\n            texts=None,\n            cache=cache,\n            **kwargs,\n        )\n        vector_store.add_texts(texts=texts, metadatas=metadatas)\n        return vector_store\n</code></pre>"},{"location":"reference/example_select/#src.autolabel.few_shot.vector_store.VectorStoreWrapper.add_texts","title":"<code>add_texts(texts, metadatas=None)</code>","text":"<p>Run texts through the embeddings and add to the vectorstore. Currently, the vectorstore is reinitialized each time, because we do not require a persistent vector store for example selection. Args:     texts (Iterable[str]): Texts to add to the vectorstore.     metadatas (Optional[List[dict]], optional): Optional list of metadatas. Returns:     List[str]: List of IDs of the added texts.</p> Source code in <code>src/autolabel/few_shot/vector_store.py</code> <pre><code>def add_texts(\n    self,\n    texts: Iterable[str],\n    metadatas: Optional[List[Dict[str, str]]] = None,\n) -&gt; List[str]:\n    \"\"\"Run texts through the embeddings and add to the vectorstore. Currently, the vectorstore is reinitialized each time, because we do not require a persistent vector store for example selection.\n    Args:\n        texts (Iterable[str]): Texts to add to the vectorstore.\n        metadatas (Optional[List[dict]], optional): Optional list of metadatas.\n    Returns:\n        List[str]: List of IDs of the added texts.\n    \"\"\"\n    if self._embedding_function is not None:\n        embeddings = self._get_embeddings(texts)\n\n    self._corpus_embeddings = torch.tensor(embeddings)\n    self._texts = texts\n    self._metadatas = metadatas\n    return metadatas\n</code></pre>"},{"location":"reference/example_select/#src.autolabel.few_shot.vector_store.VectorStoreWrapper.from_texts","title":"<code>from_texts(texts, embedding=None, metadatas=None, cache=True, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a vectorstore from raw text. The data will be ephemeral in-memory. Args:     texts (List[str]): List of texts to add to the collection.     embedding (Optional[Embeddings]): Embedding function. Defaults to None.     metadatas (Optional[List[dict]]): List of metadatas. Defaults to None.     cache (bool): Whether to cache the embeddings. Defaults to True. Returns:     vector_store: Vectorstore with seedset embeddings</p> Source code in <code>src/autolabel/few_shot/vector_store.py</code> <pre><code>@classmethod\ndef from_texts(\n    cls: Type[VectorStoreWrapper],\n    texts: List[str],\n    embedding: Optional[Embeddings] = None,\n    metadatas: Optional[List[dict]] = None,\n    cache: bool = True,\n    **kwargs: Any,\n) -&gt; VectorStoreWrapper:\n    \"\"\"Create a vectorstore from raw text.\n    The data will be ephemeral in-memory.\n    Args:\n        texts (List[str]): List of texts to add to the collection.\n        embedding (Optional[Embeddings]): Embedding function. Defaults to None.\n        metadatas (Optional[List[dict]]): List of metadatas. Defaults to None.\n        cache (bool): Whether to cache the embeddings. Defaults to True.\n    Returns:\n        vector_store: Vectorstore with seedset embeddings\n    \"\"\"\n    vector_store = cls(\n        embedding_function=embedding,\n        corpus_embeddings=None,\n        texts=None,\n        cache=cache,\n        **kwargs,\n    )\n    vector_store.add_texts(texts=texts, metadatas=metadatas)\n    return vector_store\n</code></pre>"},{"location":"reference/example_select/#src.autolabel.few_shot.vector_store.VectorStoreWrapper.label_diversity_similarity_search","title":"<code>label_diversity_similarity_search(query, label_key, k=4, filter=None, **kwargs)</code>","text":"<p>Run semantic similarity search. Args:     query (str): Query text to search for.     k (int): Number of results to return per label.     filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None. Returns:     List[Document]: List of documents most similar to the query text.</p> Source code in <code>src/autolabel/few_shot/vector_store.py</code> <pre><code>def label_diversity_similarity_search(\n    self,\n    query: str,\n    label_key: str,\n    k: int = 4,\n    filter: Optional[Dict[str, str]] = None,\n    **kwargs: Any,\n) -&gt; List[Document]:\n    \"\"\"Run semantic similarity search.\n    Args:\n        query (str): Query text to search for.\n        k (int): Number of results to return per label.\n        filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n    Returns:\n        List[Document]: List of documents most similar to the query text.\n    \"\"\"\n    docs_and_scores = self.label_diversity_similarity_search_with_score(\n        query, label_key, k, filter=filter\n    )\n    return [doc for doc, _ in docs_and_scores]\n</code></pre>"},{"location":"reference/example_select/#src.autolabel.few_shot.vector_store.VectorStoreWrapper.label_diversity_similarity_search_with_score","title":"<code>label_diversity_similarity_search_with_score(query, label_key, k=4, filter=None, **kwargs)</code>","text":"<p>Run semantic similarity search and retrieve distances. Args:     query (str): Query text to search for.     k (int): Number of results to return. Defaults to 4.     filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None. Returns:     List[Tuple[Document, float]]: List of documents most similar to the query         text with distance in float.</p> Source code in <code>src/autolabel/few_shot/vector_store.py</code> <pre><code>def label_diversity_similarity_search_with_score(\n    self,\n    query: str,\n    label_key: str,\n    k: int = 4,\n    filter: Optional[Dict[str, str]] = None,\n    **kwargs: Any,\n) -&gt; List[Tuple[Document, float]]:\n    \"\"\"Run semantic similarity search and retrieve distances.\n    Args:\n        query (str): Query text to search for.\n        k (int): Number of results to return. Defaults to 4.\n        filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n    Returns:\n        List[Tuple[Document, float]]: List of documents most similar to the query\n            text with distance in float.\n    \"\"\"\n    query_embeddings = torch.tensor([self._get_embeddings([query])[0]])\n    data = []\n    data = zip(self._corpus_embeddings, self._texts, self._metadatas)\n    sorted_data = sorted(data, key=lambda item: item[2].get(label_key))\n\n    documents = []\n    scores = []\n    metadatas = []\n    for label, label_examples in groupby(\n        sorted_data, key=lambda item: item[2].get(label_key)\n    ):\n        label_examples_list = list(label_examples)\n        label_embeddings = list(\n            map(lambda label_example: label_example[0], label_examples_list)\n        )\n        label_texts = list(\n            map(lambda label_example: label_example[1], label_examples_list)\n        )\n        label_metadatas = list(\n            map(lambda label_example: label_example[2], label_examples_list)\n        )\n\n        result_ids_and_scores = semantic_search(\n            corpus_embeddings=label_embeddings,\n            query_embeddings=query_embeddings,\n            top_k=k,\n        )\n        result_ids = [result[\"corpus_id\"] for result in result_ids_and_scores[0]]\n        documents.extend([label_texts[index] for index in result_ids])\n        metadatas.extend([label_metadatas[index] for index in result_ids])\n        scores.extend([result[\"score\"] for result in result_ids_and_scores[0]])\n    results = {}\n\n    results[\"documents\"] = [documents]\n    results[\"distances\"] = [scores]\n    results[\"metadatas\"] = [metadatas]\n\n    return _results_to_docs_and_scores(results)\n</code></pre>"},{"location":"reference/example_select/#src.autolabel.few_shot.vector_store.VectorStoreWrapper.similarity_search","title":"<code>similarity_search(query, k=4, filter=None, **kwargs)</code>","text":"<p>Run semantic similarity search. Args:     query (str): Query text to search for.     k (int): Number of results to return. Defaults to 4.     filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None. Returns:     List[Document]: List of documents most similar to the query text.</p> Source code in <code>src/autolabel/few_shot/vector_store.py</code> <pre><code>def similarity_search(\n    self,\n    query: str,\n    k: int = 4,\n    filter: Optional[Dict[str, str]] = None,\n    **kwargs: Any,\n) -&gt; List[Document]:\n    \"\"\"Run semantic similarity search.\n    Args:\n        query (str): Query text to search for.\n        k (int): Number of results to return. Defaults to 4.\n        filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n    Returns:\n        List[Document]: List of documents most similar to the query text.\n    \"\"\"\n    docs_and_scores = self.similarity_search_with_score(query, k, filter=filter)\n    return [doc for doc, _ in docs_and_scores]\n</code></pre>"},{"location":"reference/example_select/#src.autolabel.few_shot.vector_store.VectorStoreWrapper.similarity_search_with_score","title":"<code>similarity_search_with_score(query, k=4, filter=None, **kwargs)</code>","text":"<p>Run semantic similarity search and retrieve distances. Args:     query (str): Query text to search for.     k (int): Number of results to return. Defaults to 4.     filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None. Returns:     List[Tuple[Document, float]]: List of documents most similar to the query         text with distance in float.</p> Source code in <code>src/autolabel/few_shot/vector_store.py</code> <pre><code>def similarity_search_with_score(\n    self,\n    query: str,\n    k: int = 4,\n    filter: Optional[Dict[str, str]] = None,\n    **kwargs: Any,\n) -&gt; List[Tuple[Document, float]]:\n    \"\"\"Run semantic similarity search and retrieve distances.\n    Args:\n        query (str): Query text to search for.\n        k (int): Number of results to return. Defaults to 4.\n        filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n    Returns:\n        List[Tuple[Document, float]]: List of documents most similar to the query\n            text with distance in float.\n    \"\"\"\n    query_embeddings = torch.tensor([self._get_embeddings([query])[0]])\n    result_ids_and_scores = semantic_search(\n        corpus_embeddings=self._corpus_embeddings,\n        query_embeddings=query_embeddings,\n        top_k=k,\n    )\n    result_ids = [result[\"corpus_id\"] for result in result_ids_and_scores[0]]\n    scores = [result[\"score\"] for result in result_ids_and_scores[0]]\n    results = {}\n    results[\"documents\"] = [[self._texts[index] for index in result_ids]]\n    results[\"distances\"] = [scores]\n    results[\"metadatas\"] = [[self._metadatas[index] for index in result_ids]]\n    return _results_to_docs_and_scores(results)\n</code></pre>"},{"location":"reference/example_select/#src.autolabel.few_shot.vector_store.cos_sim","title":"<code>cos_sim(a, b)</code>","text":"<p>Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j. Returns:     cos_sim: Matrix with res(i)(j) = cos_sim(a[i], b[j])</p> Source code in <code>src/autolabel/few_shot/vector_store.py</code> <pre><code>def cos_sim(a: Tensor, b: Tensor) -&gt; Tensor:\n    \"\"\"\n    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n    Returns:\n        cos_sim: Matrix with res(i)(j) = cos_sim(a[i], b[j])\n    \"\"\"\n    if not isinstance(a, torch.Tensor):\n        a = torch.tensor(a)\n\n    if not isinstance(b, torch.Tensor):\n        b = torch.tensor(b)\n\n    if len(a.shape) == 1:\n        a = a.unsqueeze(0)\n\n    if len(b.shape) == 1:\n        b = b.unsqueeze(0)\n\n    a_norm = torch.nn.functional.normalize(a, p=2, dim=1)\n    b_norm = torch.nn.functional.normalize(b, p=2, dim=1)\n    return torch.mm(a_norm, b_norm.transpose(0, 1))\n</code></pre>"},{"location":"reference/example_select/#src.autolabel.few_shot.vector_store.semantic_search","title":"<code>semantic_search(query_embeddings, corpus_embeddings, query_chunk_size=100, corpus_chunk_size=500000, top_k=10, score_function=cos_sim)</code>","text":"<p>Semantic similarity search based on cosine similarity score. Implementation from this project: https://github.com/UKPLab/sentence-transformers</p> Source code in <code>src/autolabel/few_shot/vector_store.py</code> <pre><code>def semantic_search(\n    query_embeddings: Tensor,\n    corpus_embeddings: Tensor,\n    query_chunk_size: int = 100,\n    corpus_chunk_size: int = 500000,\n    top_k: int = 10,\n    score_function: Callable[[Tensor, Tensor], Tensor] = cos_sim,\n):\n    \"\"\"\n    Semantic similarity search based on cosine similarity score. Implementation from this project: https://github.com/UKPLab/sentence-transformers\n    \"\"\"\n\n    if isinstance(query_embeddings, (np.ndarray, np.generic)):\n        query_embeddings = torch.from_numpy(query_embeddings)\n    elif isinstance(query_embeddings, list):\n        query_embeddings = torch.stack(query_embeddings)\n\n    if len(query_embeddings.shape) == 1:\n        query_embeddings = query_embeddings.unsqueeze(0)\n\n    if isinstance(corpus_embeddings, (np.ndarray, np.generic)):\n        corpus_embeddings = torch.from_numpy(corpus_embeddings)\n    elif isinstance(corpus_embeddings, list):\n        corpus_embeddings = torch.stack(corpus_embeddings)\n\n    # Check that corpus and queries are on the same device\n    if corpus_embeddings.device != query_embeddings.device:\n        query_embeddings = query_embeddings.to(corpus_embeddings.device)\n\n    queries_result_list = [[] for _ in range(len(query_embeddings))]\n\n    for query_start_idx in range(0, len(query_embeddings), query_chunk_size):\n        # Iterate over chunks of the corpus\n        for corpus_start_idx in range(0, len(corpus_embeddings), corpus_chunk_size):\n            # Compute cosine similarities\n            cos_scores = score_function(\n                query_embeddings[query_start_idx : query_start_idx + query_chunk_size],\n                corpus_embeddings[\n                    corpus_start_idx : corpus_start_idx + corpus_chunk_size\n                ],\n            )\n\n            # Get top-k scores\n            cos_scores_top_k_values, cos_scores_top_k_idx = torch.topk(\n                cos_scores,\n                min(top_k, len(cos_scores[0])),\n                dim=1,\n                largest=True,\n                sorted=False,\n            )\n            cos_scores_top_k_values = cos_scores_top_k_values.cpu().tolist()\n            cos_scores_top_k_idx = cos_scores_top_k_idx.cpu().tolist()\n\n            for query_itr in range(len(cos_scores)):\n                for sub_corpus_id, score in zip(\n                    cos_scores_top_k_idx[query_itr], cos_scores_top_k_values[query_itr]\n                ):\n                    corpus_id = corpus_start_idx + sub_corpus_id\n                    query_id = query_start_idx + query_itr\n                    if len(queries_result_list[query_id]) &lt; top_k:\n                        heapq.heappush(\n                            queries_result_list[query_id], (score, corpus_id)\n                        )  # heaqp tracks the quantity of the first element in the tuple\n                    else:\n                        heapq.heappushpop(\n                            queries_result_list[query_id], (score, corpus_id)\n                        )\n\n    # change the data format and sort\n    for query_id in range(len(queries_result_list)):\n        for doc_itr in range(len(queries_result_list[query_id])):\n            score, corpus_id = queries_result_list[query_id][doc_itr]\n            queries_result_list[query_id][doc_itr] = {\n                \"corpus_id\": corpus_id,\n                \"score\": score,\n            }\n        queries_result_list[query_id] = sorted(\n            queries_result_list[query_id], key=lambda x: x[\"score\"], reverse=True\n        )\n    return queries_result_list\n</code></pre>"},{"location":"reference/labeler/","title":"AutoLabeler","text":"Source code in <code>src/autolabel/labeler.py</code> <pre><code>class LabelingAgent:\n    COST_KEY = \"Cost in $\"\n    CONFIDENCE_MAX_CONTEXT_LENGTH = 3400\n\n    def __init__(\n        self,\n        config: Union[AutolabelConfig, str, dict],\n        cache: Optional[bool] = True,\n        example_selector: Optional[BaseExampleSelector] = None,\n        create_task: Optional[bool] = False,\n        console_output: Optional[bool] = True,\n        generation_cache: Optional[BaseCache] = SQLAlchemyGenerationCache(),\n        transform_cache: Optional[BaseCache] = SQLAlchemyTransformCache(),\n        confidence_cache: Optional[BaseCache] = SQLAlchemyConfidenceCache(),\n        confidence_tokenizer: Optional[AutoTokenizer] = None,\n    ) -&gt; None:\n        self.create_task = create_task\n        self.db = StateManager() if self.create_task else None\n        self.generation_cache = generation_cache\n        self.transform_cache = transform_cache\n        self.confidence_cache = confidence_cache\n        if not cache:\n            logger.warning(\n                f\"cache parameter is deprecated and will be removed soon. Please use generation_cache, transform_cache and confidence_cache instead.\"\n            )\n            self.generation_cache = None\n            self.transform_cache = None\n            self.confidence_cache = None\n\n        if self.generation_cache is not None:\n            self.generation_cache.initialize()\n        if self.transform_cache is not None:\n            self.transform_cache.initialize()\n        if self.confidence_cache is not None:\n            self.confidence_cache.initialize()\n\n        self.console = Console(quiet=not console_output)\n\n        self.config = (\n            config if isinstance(config, AutolabelConfig) else AutolabelConfig(config)\n        )\n        self.task = TaskFactory.from_config(self.config)\n        self.llm: BaseModel = ModelFactory.from_config(\n            self.config, cache=self.generation_cache\n        )\n\n        if self.config.confidence_chunk_column():\n            if not confidence_tokenizer:\n                self.confidence_tokenizer = AutoTokenizer.from_pretrained(\n                    \"google/flan-t5-xxl\"\n                )\n            else:\n                self.confidence_tokenizer = confidence_tokenizer\n        score_type = \"logprob_average\"\n        if self.config.task_type() == TaskType.ATTRIBUTE_EXTRACTION:\n            score_type = \"logprob_average_per_key\"\n        self.confidence = ConfidenceCalculator(\n            score_type=score_type,\n            llm=self.llm,\n            cache=self.confidence_cache,\n        )\n\n        self.example_selector = example_selector\n\n        # Only used if we don't use task management\n        self.all_annotations = []\n\n        if self.create_task:\n            logger.warning(\n                f\"create_task parameter is deprecated and will be removed soon. The LLM calls are getting cached and should handle most use cases.\"\n            )\n\n        if in_notebook():\n            import nest_asyncio\n\n            nest_asyncio.apply()\n\n    def run(\n        self,\n        dataset: AutolabelDataset,\n        output_name: Optional[str] = None,\n        max_items: Optional[int] = None,\n        start_index: int = 0,\n        additional_metrics: Optional[List[BaseMetric]] = [],\n        skip_eval: Optional[bool] = False,\n    ) -&gt; Tuple[pd.Series, pd.DataFrame, List[MetricResult]]:\n        \"\"\"Labels data in a given dataset. Output written to new CSV file.\n\n        Args:\n            dataset: path to CSV dataset to be annotated\n            max_items: maximum items in dataset to be annotated\n            output_name: custom name of output CSV file\n            start_index: skips annotating [0, start_index)\n        \"\"\"\n\n        dataset = dataset.get_slice(max_items=max_items, start_index=start_index)\n\n        if self.create_task:\n            self.db.initialize()\n            self.dataset_obj = self.db.initialize_dataset(dataset.df, self.config)\n            self.task_object = self.db.initialize_task(self.config)\n        else:\n            self.all_annotations = []\n\n        if isinstance(dataset, str):\n            csv_file_name = (\n                output_name\n                if output_name\n                else f\"{dataset.replace('.csv','')}_labeled.csv\"\n            )\n        else:\n            csv_file_name = f\"{self.config.task_name()}_labeled.csv\"\n\n        if self.create_task:\n            # Initialize task run and check if it already exists\n            self.task_run = self.db.get_task_run(\n                self.task_object.id, self.dataset_obj.id\n            )\n            # Resume/Delete the task if it already exists or create a new task run\n            if self.task_run:\n                logger.info(\"Task run already exists.\")\n                self.task_run = self.handle_existing_task_run(\n                    self.task_run,\n                    csv_file_name,\n                    gt_labels=dataset.gt_labels,\n                    additional_metrics=additional_metrics,\n                )\n            else:\n                self.task_run = self.db.create_task_run(\n                    csv_file_name, self.task_object.id, self.dataset_obj.id\n                )\n\n        # Get the seed examples from the dataset config\n        seed_examples = self.config.few_shot_example_set()\n\n        # If this dataset config is a string, read the corrresponding csv file\n        if isinstance(seed_examples, str):\n            seed_loader = AutolabelDataset(seed_examples, self.config)\n            seed_examples = seed_loader.inputs\n\n        # Check explanations are present in data if explanation_column is passed in\n        if (\n            self.config.explanation_column()\n            and len(seed_examples) &gt; 0\n            and self.config.explanation_column() not in list(seed_examples[0].keys())\n        ):\n            raise ValueError(\n                f\"Explanation column {self.config.explanation_column()} not found in dataset.\\nMake sure that explanations were generated using labeler.generate_explanations(seed_file).\"\n            )\n\n        if self.example_selector is None:\n            if (\n                self.config.label_selection()\n                and self.config.few_shot_algorithm() != \"fixed\"\n            ):\n                # TODO: Add support for other few shot algorithms specially semantic similarity\n                raise ValueError(\n                    \"Error: Only 'fixed' few shot example selector is supported for label selection.\"\n                )\n\n            self.example_selector = ExampleSelectorFactory.initialize_selector(\n                self.config,\n                [safe_serialize_to_string(example) for example in seed_examples],\n                dataset.df.keys().tolist(),\n                cache=self.generation_cache is not None,\n            )\n\n        if self.config.label_selection():\n            if self.config.task_type() != TaskType.CLASSIFICATION:\n                self.console.print(\n                    \"Warning: label_selection only supported for classification tasks!\"\n                )\n            else:\n                self.label_selector = LabelSelector(\n                    config=self.config,\n                    embedding_func=PROVIDER_TO_MODEL.get(\n                        self.config.embedding_provider(), DEFAULT_EMBEDDING_PROVIDER\n                    )(model=self.config.embedding_model_name()),\n                )\n\n        current_index = self.task_run.current_index if self.create_task else 0\n        cost = 0.0\n        postfix_dict = {}\n\n        indices = range(current_index, len(dataset.inputs))\n        selected_labels = self.config.labels_list()\n        for current_index in track_with_stats(\n            indices,\n            postfix_dict,\n            total=len(dataset.inputs) - current_index,\n            console=self.console,\n        ):\n            chunk = dataset.inputs[current_index]\n            examples = []\n\n            if (\n                self.config.label_selection()\n                and self.config.task_type() == TaskType.CLASSIFICATION\n            ):\n                # get every column except the one we want to label\n                toEmbed = chunk.copy()\n                if self.config.label_column() and self.config.label_column() in toEmbed:\n                    del toEmbed[self.config.label_column()]\n\n                # convert this to a string\n                toEmbed = json.dumps(toEmbed)\n\n                selected_labels = self.label_selector.select_labels(toEmbed)\n\n                if self.example_selector:\n                    examples = self.example_selector.select_examples(\n                        safe_serialize_to_string(chunk),\n                        selected_labels=selected_labels,\n                        label_column=self.config.label_column(),\n                    )\n                else:\n                    examples = []\n            else:\n                if self.example_selector:\n                    examples = self.example_selector.select_examples(\n                        safe_serialize_to_string(chunk),\n                    )\n\n            # Construct Prompt to pass to LLM\n            final_prompt = self.task.construct_prompt(\n                chunk,\n                examples,\n                selected_labels=selected_labels,\n                max_input_tokens=self.llm.DEFAULT_CONTEXT_LENGTH,\n                get_num_tokens=self.llm.get_num_tokens,\n            )\n\n            response = self.llm.label([final_prompt])\n            for i, generations, error, latency in zip(\n                range(len(response.generations)),\n                response.generations,\n                response.errors,\n                response.latencies,\n            ):\n                input_tokens = self.llm.get_num_tokens(final_prompt)\n                if error is not None:\n                    annotation = LLMAnnotation(\n                        successfully_labeled=False,\n                        label=self.task.NULL_LABEL_TOKEN,\n                        raw_response=\"\",\n                        curr_sample=pickle.dumps(chunk),\n                        prompt=final_prompt,\n                        confidence_score=0,\n                        error=error,\n                        input_tokens=input_tokens,\n                        cost=0,\n                        latency=0,\n                    )\n                else:\n                    annotations = []\n                    for generation in generations:\n                        annotation = self.task.parse_llm_response(\n                            generation, chunk, final_prompt\n                        )\n                        annotation.confidence_prompt = (\n                            self.task.construct_confidence_prompt(chunk, examples)\n                        )\n                        annotation.input_tokens = input_tokens\n                        annotation.output_tokens = self.llm.get_num_tokens(\n                            annotation.raw_response\n                        )\n                        annotation.cost = sum(response.costs)\n                        annotation.latency = latency\n\n                        if self.config.confidence():\n                            try:\n                                annotation.confidence_score = self.get_confidence_score(\n                                    annotation, chunk, examples\n                                )\n                            except Exception as e:\n                                logger.error(f\"Error calculating confidence score: {e}\")\n                                if (\n                                    self.config.task_type()\n                                    == TaskType.ATTRIBUTE_EXTRACTION\n                                ):\n                                    annotation.confidence_score = {}\n                                else:\n                                    annotation.confidence_score = 0\n\n                        annotations.append(annotation)\n                    annotation = self.majority_annotation(annotations)\n\n                # Save the annotation in the database\n                self.save_annotation(annotation, current_index, i)\n\n            cost += sum(response.costs)\n            postfix_dict[self.COST_KEY] = f\"{cost:.2f}\"\n\n            # Evaluate the task every eval_every examples\n            if not skip_eval and (current_index + 1) % 100 == 0:\n                llm_labels = self.get_all_annotations()\n                if dataset.gt_labels:\n                    eval_result = self.task.eval(\n                        llm_labels,\n                        (\n                            dataset.gt_labels[: len(llm_labels)]\n                            if isinstance(dataset.gt_labels, list)\n                            else {\n                                k: v[: len(llm_labels)]\n                                for k, v in dataset.gt_labels.items()\n                            }\n                        ),\n                        additional_metrics=additional_metrics,\n                    )\n\n                    for m in eval_result:\n                        # This is a row wise metric\n                        if isinstance(m.value, list):\n                            continue\n                        elif m.show_running:\n                            postfix_dict[m.name] = (\n                                f\"{m.value:.4f}\"\n                                if isinstance(m.value, float)\n                                else m.value\n                            )\n\n            if self.create_task:\n                # Update task run state\n                self.task_run = self.save_task_run_state(\n                    current_index=current_index + len(chunk)\n                )\n\n        llm_labels = self.get_all_annotations()\n        eval_result = None\n        table = {}\n\n        # if true labels are provided, evaluate accuracy of predictions\n        if not skip_eval and dataset.gt_labels:\n            eval_result = self.task.eval(\n                llm_labels,\n                (\n                    dataset.gt_labels[: len(llm_labels)]\n                    if isinstance(dataset.gt_labels, list)\n                    else {k: v[: len(llm_labels)] for k, v in dataset.gt_labels.items()}\n                ),\n                additional_metrics=additional_metrics,\n            )\n            # TODO: serialize and write to file\n            for m in eval_result:\n                if isinstance(m.value, list):\n                    continue\n                elif m.show_running:\n                    table[m.name] = m.value\n                else:\n                    self.console.print(f\"{m.name}:\\n{m.value}\")\n\n        # print cost\n        self.console.print(f\"Actual Cost: {maybe_round(cost)}\")\n        print_table(table, console=self.console, default_style=METRIC_TABLE_STYLE)\n\n        dataset.process_labels(llm_labels, eval_result)\n        # Only save to csv if output_name is provided or dataset is a string\n        if not output_name and isinstance(dataset, str):\n            output_name = (\n                dataset.rsplit(\".\", 1)[0] + \"_labeled.\" + dataset.rsplit(\".\", 1)[1]\n            )\n\n        if output_name:\n            dataset.save(output_file_name=output_name)\n        return dataset\n\n    def plan(\n        self,\n        dataset: AutolabelDataset,\n        max_items: Optional[int] = None,\n        start_index: int = 0,\n    ) -&gt; None:\n        \"\"\"Calculates and prints the cost of calling autolabel.run() on a given dataset\n\n        Args:\n            dataset: path to a CSV dataset\n        \"\"\"\n        dataset = dataset.get_slice(max_items=max_items, start_index=start_index)\n\n        if (\n            self.config.confidence()\n            and \"REFUEL_API_KEY\" not in os.environ\n            and not self.llm.returns_token_probs()\n        ):\n            raise ValueError(\n                \"REFUEL_API_KEY environment variable must be set to compute confidence scores. You can request an API key at https://refuel-ai.typeform.com/llm-access.\"\n            )\n\n        prompt_list = []\n        total_cost = 0\n\n        # Get the seed examples from the dataset config\n        seed_examples = self.config.few_shot_example_set()\n\n        # If this dataset config is a string, read the corrresponding csv file\n        if isinstance(seed_examples, str):\n            seed_loader = AutolabelDataset(seed_examples, self.config)\n            seed_examples = seed_loader.inputs\n\n        # Check explanations are present in data if explanation_column is passed in\n        if (\n            self.config.explanation_column()\n            and len(seed_examples) &gt; 0\n            and self.config.explanation_column() not in list(seed_examples[0].keys())\n        ):\n            raise ValueError(\n                f\"Explanation column {self.config.explanation_column()} not found in dataset.\\nMake sure that explanations were generated using labeler.generate_explanations(seed_file).\"\n            )\n\n        self.example_selector = ExampleSelectorFactory.initialize_selector(\n            self.config,\n            [safe_serialize_to_string(example) for example in seed_examples],\n            dataset.df.keys().tolist(),\n            cache=self.generation_cache is not None,\n        )\n\n        if self.config.label_selection():\n            if self.config.task_type() != TaskType.CLASSIFICATION:\n                self.console.print(\n                    \"Warning: label_selection only supported for classification tasks!\"\n                )\n            else:\n                self.label_selector = LabelSelector(\n                    config=self.config,\n                    embedding_func=PROVIDER_TO_MODEL.get(\n                        self.config.embedding_provider(), DEFAULT_EMBEDDING_PROVIDER\n                    )(model=self.config.embedding_model_name()),\n                )\n\n        input_limit = min(len(dataset.inputs), 100) if max_items is None else max_items  # type: ignore\n        for input_i in track(\n            dataset.inputs[:input_limit],\n            description=\"Generating Prompts...\",\n            console=self.console,\n        ):\n            # TODO: Check if this needs to use the example selector\n            if self.example_selector:\n                examples = self.example_selector.select_examples(\n                    safe_serialize_to_string(input_i)\n                )\n            else:\n                examples = []\n            if (\n                self.config.label_selection()\n                and self.config.task_type() == TaskType.CLASSIFICATION\n            ):\n                selected_labels = self.label_selector.select_labels(input_i[\"example\"])\n                final_prompt = self.task.construct_prompt(\n                    input_i,\n                    examples,\n                    selected_labels=selected_labels,\n                    max_input_tokens=self.llm.DEFAULT_CONTEXT_LENGTH,\n                    get_num_tokens=self.llm.get_num_tokens,\n                )\n            else:\n                final_prompt = self.task.construct_prompt(\n                    input_i,\n                    examples,\n                    max_input_tokens=self.llm.DEFAULT_CONTEXT_LENGTH,\n                    get_num_tokens=self.llm.get_num_tokens,\n                )\n            prompt_list.append(final_prompt)\n\n            # Calculate the number of tokens\n            curr_cost = self.llm.get_cost(prompt=final_prompt, label=\"\")\n            total_cost += curr_cost\n\n        total_cost = total_cost * (len(dataset.inputs) / input_limit)\n        table = {\n            \"Total Estimated Cost\": f\"${maybe_round(total_cost)}\",\n            \"Number of Examples\": len(dataset.inputs),\n            \"Average cost per example\": f\"${maybe_round(total_cost / len(dataset.inputs))}\",\n        }\n        table = {\"parameter\": list(table.keys()), \"value\": list(table.values())}\n\n        print_table(\n            table, show_header=False, console=self.console, styles=COST_TABLE_STYLES\n        )\n        self.console.rule(\"Prompt Example\")\n        self.console.print(f\"{prompt_list[0]}\", markup=False)\n        self.console.rule()\n\n    async def async_run_transform(\n        self, transform: BaseTransform, dataset: AutolabelDataset\n    ):\n        transform_outputs = [\n            transform.apply(input_dict) for input_dict in dataset.inputs\n        ]\n\n        outputs = await gather_async_tasks_with_progress(\n            transform_outputs,\n            description=f\"Running transform {transform.name()}...\",\n            console=self.console,\n        )\n        output_df = pd.DataFrame.from_records(outputs)\n        final_df = pd.concat([dataset.df, output_df], axis=1)\n        dataset = AutolabelDataset(final_df, self.config)\n        return dataset\n\n    def transform(self, dataset: AutolabelDataset):\n        transforms = []\n        for transform_dict in self.config.transforms():\n            transforms.append(\n                TransformFactory.from_dict(transform_dict, cache=self.transform_cache)\n            )\n        for transform in transforms:\n            dataset = asyncio.run(self.async_run_transform(transform, dataset))\n\n        return dataset\n\n    def handle_existing_task_run(\n        self,\n        task_run: TaskRun,\n        csv_file_name: str,\n        gt_labels: List[str] = None,\n        additional_metrics: List[BaseMetric] = [],\n    ) -&gt; TaskRun:\n        \"\"\"\n        Allows for continuing an existing labeling task. The user will be asked whether they wish to continue from where the run previously left off, or restart from the beginning.\n        Args:\n            task_run: TaskRun to retry\n            csv_file_name: path to the dataset we wish to label (only used if user chooses to restart the task)\n            gt_labels: If ground truth labels are provided, performance metrics will be displayed, such as label accuracy\n        \"\"\"\n        self.console.print(\n            f\"There is an existing task with following details: {task_run}\"\n        )\n        llm_labels = self.get_all_annotations()\n        if gt_labels and len(llm_labels) &gt; 0:\n            pprint(\"Evaluating the existing task...\")\n            gt_labels = (\n                gt_labels[: len(llm_labels)]\n                if isinstance(gt_labels, list)\n                else {k: v[: len(llm_labels)] for k, v in gt_labels.items()}\n            )\n            eval_result = self.task.eval(\n                llm_labels, gt_labels, additional_metrics=additional_metrics\n            )\n            table = {}\n            for m in eval_result:\n                if isinstance(m.value, list):\n                    continue\n                elif m.show_running:\n                    table[m.name] = m.value\n                else:\n                    self.console.print(f\"{m.name}:\\n{m.value}\")\n\n            print_table(table, console=self.console, default_style=METRIC_TABLE_STYLE)\n        self.console.print(f\"{task_run.current_index} examples labeled so far.\")\n        if not Confirm.ask(\"Do you want to resume the task?\"):\n            TaskRunModel.delete_by_id(self.db.session, task_run.id)\n            self.console.print(\"Deleted the existing task and starting a new one...\")\n            task_run = self.db.create_task_run(\n                csv_file_name, self.task_object.id, self.dataset_obj.id\n            )\n        return task_run\n\n    def get_confidence_score(\n        self, annotation: LLMAnnotation, chunk: Dict, examples: List[Dict]\n    ) -&gt; Union[float, dict]:\n        full_confidence_input = annotation.confidence_prompt + annotation.raw_response\n        if (\n            self.llm.returns_token_probs()\n            or not self.config.confidence_chunk_column()\n            or self.get_num_tokens(full_confidence_input)\n            &lt; self.CONFIDENCE_MAX_CONTEXT_LENGTH\n        ):\n            return self.confidence.calculate(model_generation=annotation)\n        key_to_chunk = self.config.confidence_chunk_column()\n        if not key_to_chunk:\n            raise ValueError(\n                \"confidence_chunk_column must be set in the config to use confidence_chunk_size\"\n            )\n        if key_to_chunk == AUTO_CONFIDENCE_CHUNKING_COLUMN:\n            # If the confidence_chunk_column is set to auto,\n            # we choose the column with the most tokens as the chunking column.\n            max_tokens = -1\n            example_template_keys = get_format_variables(self.config.example_template())\n            for key in example_template_keys:\n                num_tokens = self.get_num_tokens(chunk[key])\n                if num_tokens &gt; max_tokens:\n                    max_tokens = num_tokens\n                    key_to_chunk = key\n\n        empty_chunk = chunk.copy()\n        empty_chunk[key_to_chunk] = \"\"\n        empty_prompt = self.task.construct_confidence_prompt(empty_chunk, examples)\n        num_tokens_empty_prompt = self.get_num_tokens(empty_prompt)\n        num_tokens_per_chunk = (\n            self.config.confidence_chunk_size() - num_tokens_empty_prompt\n        )\n        confidence_chunks = self.chunk_string(chunk[key_to_chunk], num_tokens_per_chunk)\n\n        confidence_scores = []\n        for confidence_chunk in confidence_chunks:\n            new_chunk = chunk.copy()\n            new_chunk[key_to_chunk] = confidence_chunk\n            new_prompt = self.task.construct_confidence_prompt(new_chunk, examples)\n            annotation_dict = annotation.dict()\n            annotation_dict[\"confidence_prompt\"] = new_prompt\n            confidence_scores.append(\n                self.confidence.calculate(\n                    model_generation=LLMAnnotation(**annotation_dict),\n                )\n            )\n\n        merge_function = MERGE_FUNCTION[self.config.confidence_merge_function()]\n        if isinstance(confidence_scores[0], dict):\n            merged_confidence = {}\n            for key in confidence_scores[0].keys():\n                merged_confidence[key] = merge_function(\n                    [conf[key] for conf in confidence_scores]\n                )\n            return merged_confidence\n        else:\n            merged_confidence = merge_function(confidence_scores)\n            return merged_confidence\n\n    def save_task_run_state(\n        self, current_index: int = None, status: TaskStatus = \"\", error: str = \"\"\n    ) -&gt; TaskRun:\n        \"\"\"Saves the current state of the Task being performed\"\"\"\n        # Save the current state of the task\n        if error:\n            self.task_run.error = error\n        if status:\n            self.task_run.status = status\n        if current_index:\n            self.task_run.current_index = current_index\n        return TaskRunModel.update(self.db.session, self.task_run)\n\n    def majority_annotation(\n        self, annotation_list: List[LLMAnnotation]\n    ) -&gt; LLMAnnotation:\n        labels = [a.label for a in annotation_list]\n        counts = {}\n        for ind, label in enumerate(labels):\n            # Needed for named entity recognition which outputs lists instead of strings\n            label = str(label)\n\n            if label not in counts:\n                counts[label] = (1, ind)\n            else:\n                counts[label] = (counts[label][0] + 1, counts[label][1])\n        max_label = max(counts, key=lambda x: counts[x][0])\n        return annotation_list[counts[max_label][1]]\n\n    def generate_explanations(\n        self,\n        seed_examples: Union[str, List[Dict]],\n        include_label: bool = True,\n    ) -&gt; List[Dict]:\n        \"\"\"Use LLM to generate explanations for why examples are labeled the way that they are.\"\"\"\n        out_file = None\n        if isinstance(seed_examples, str):\n            out_file = seed_examples\n            seed_loader = AutolabelDataset(seed_examples, self.config)\n            seed_examples = seed_loader.inputs\n\n        explanation_column = self.config.explanation_column()\n        if not explanation_column:\n            raise ValueError(\n                \"The explanation column needs to be specified in the dataset config.\"\n            )\n\n        for seed_example in track(\n            seed_examples,\n            description=\"Generating explanations\",\n            console=self.console,\n        ):\n            explanation_prompt = self.task.get_explanation_prompt(\n                seed_example, include_label=include_label\n            )\n            if self.task.image_col is not None:\n                explanation_prompt = json.dumps(\n                    {\n                        \"text\": explanation_prompt,\n                        \"image_url\": seed_example[self.task.image_col],\n                    }\n                )\n            explanation = self.llm.label([explanation_prompt])\n            explanation = explanation.generations[0][0].text\n            seed_example[explanation_column] = str(explanation) if explanation else \"\"\n\n        if out_file:\n            df = pd.DataFrame.from_records(seed_examples)\n            df.to_csv(out_file, index=False)\n\n        return seed_examples\n\n    def generate_synthetic_dataset(self) -&gt; AutolabelDataset:\n        columns = get_format_variables(self.config.example_template())\n        df = pd.DataFrame(columns=columns)\n        for label in track(\n            self.config.labels_list(),\n            description=\"Generating dataset\",\n            console=self.console,\n        ):\n            prompt = self.task.get_generate_dataset_prompt(label)\n\n            result = self.llm.label([prompt])\n            if result.errors[0] is not None:\n                self.console.print(\n                    f\"Error generating rows for label {label}: {result.errors[0]}\"\n                )\n            else:\n                response = result.generations[0][0].text.strip()\n\n                response = io.StringIO(response)\n                label_df = pd.read_csv(response, sep=self.config.delimiter())\n                label_df[self.config.label_column()] = label\n                df = pd.concat([df, label_df], axis=0, ignore_index=True)\n        return AutolabelDataset(df, self.config)\n\n    def clear_cache(self, use_ttl: bool = True):\n        \"\"\"\n        Clears the generation and transformation cache from autolabel.\n        Args:\n            use_ttl: If true, only clears the cache if the ttl has expired.\n        \"\"\"\n        self.generation_cache.clear(use_ttl=use_ttl)\n        self.transform_cache.clear(use_ttl=use_ttl)\n\n    def save_annotation(self, annotation: LLMAnnotation, current_index: int, i: int):\n        if self.create_task:\n            # Store the annotation in the database\n            AnnotationModel.create_from_llm_annotation(\n                self.db.session,\n                annotation,\n                current_index + i,\n                self.task_run.id,\n            )\n        else:\n            self.all_annotations.append(annotation)\n\n    def get_all_annotations(self):\n        if self.create_task:\n            db_result = AnnotationModel.get_annotations_by_task_run_id(\n                self.db.session, self.task_run.id\n            )\n            return [pickle.loads(a.llm_annotation) for a in db_result]\n        else:\n            return self.all_annotations\n\n    def get_num_tokens(self, inp: str) -&gt; int:\n        \"\"\"Returns the number of tokens in the prompt\"\"\"\n        return len(self.confidence_tokenizer.encode(str(inp)))\n\n    def chunk_string(self, inp: str, chunk_size: int) -&gt; List[str]:\n        \"\"\"Chunks the input string into chunks of size chunk_size\"\"\"\n        tokens = self.confidence_tokenizer.encode(inp)\n        chunks = [tokens[i : i + chunk_size] for i in range(0, len(tokens), chunk_size)]\n        return [self.confidence_tokenizer.decode(chunk) for chunk in chunks]\n</code></pre>"},{"location":"reference/labeler/#src.autolabel.labeler.LabelingAgent.chunk_string","title":"<code>chunk_string(inp, chunk_size)</code>","text":"<p>Chunks the input string into chunks of size chunk_size</p> Source code in <code>src/autolabel/labeler.py</code> <pre><code>def chunk_string(self, inp: str, chunk_size: int) -&gt; List[str]:\n    \"\"\"Chunks the input string into chunks of size chunk_size\"\"\"\n    tokens = self.confidence_tokenizer.encode(inp)\n    chunks = [tokens[i : i + chunk_size] for i in range(0, len(tokens), chunk_size)]\n    return [self.confidence_tokenizer.decode(chunk) for chunk in chunks]\n</code></pre>"},{"location":"reference/labeler/#src.autolabel.labeler.LabelingAgent.clear_cache","title":"<code>clear_cache(use_ttl=True)</code>","text":"<p>Clears the generation and transformation cache from autolabel. Args:     use_ttl: If true, only clears the cache if the ttl has expired.</p> Source code in <code>src/autolabel/labeler.py</code> <pre><code>def clear_cache(self, use_ttl: bool = True):\n    \"\"\"\n    Clears the generation and transformation cache from autolabel.\n    Args:\n        use_ttl: If true, only clears the cache if the ttl has expired.\n    \"\"\"\n    self.generation_cache.clear(use_ttl=use_ttl)\n    self.transform_cache.clear(use_ttl=use_ttl)\n</code></pre>"},{"location":"reference/labeler/#src.autolabel.labeler.LabelingAgent.generate_explanations","title":"<code>generate_explanations(seed_examples, include_label=True)</code>","text":"<p>Use LLM to generate explanations for why examples are labeled the way that they are.</p> Source code in <code>src/autolabel/labeler.py</code> <pre><code>def generate_explanations(\n    self,\n    seed_examples: Union[str, List[Dict]],\n    include_label: bool = True,\n) -&gt; List[Dict]:\n    \"\"\"Use LLM to generate explanations for why examples are labeled the way that they are.\"\"\"\n    out_file = None\n    if isinstance(seed_examples, str):\n        out_file = seed_examples\n        seed_loader = AutolabelDataset(seed_examples, self.config)\n        seed_examples = seed_loader.inputs\n\n    explanation_column = self.config.explanation_column()\n    if not explanation_column:\n        raise ValueError(\n            \"The explanation column needs to be specified in the dataset config.\"\n        )\n\n    for seed_example in track(\n        seed_examples,\n        description=\"Generating explanations\",\n        console=self.console,\n    ):\n        explanation_prompt = self.task.get_explanation_prompt(\n            seed_example, include_label=include_label\n        )\n        if self.task.image_col is not None:\n            explanation_prompt = json.dumps(\n                {\n                    \"text\": explanation_prompt,\n                    \"image_url\": seed_example[self.task.image_col],\n                }\n            )\n        explanation = self.llm.label([explanation_prompt])\n        explanation = explanation.generations[0][0].text\n        seed_example[explanation_column] = str(explanation) if explanation else \"\"\n\n    if out_file:\n        df = pd.DataFrame.from_records(seed_examples)\n        df.to_csv(out_file, index=False)\n\n    return seed_examples\n</code></pre>"},{"location":"reference/labeler/#src.autolabel.labeler.LabelingAgent.get_num_tokens","title":"<code>get_num_tokens(inp)</code>","text":"<p>Returns the number of tokens in the prompt</p> Source code in <code>src/autolabel/labeler.py</code> <pre><code>def get_num_tokens(self, inp: str) -&gt; int:\n    \"\"\"Returns the number of tokens in the prompt\"\"\"\n    return len(self.confidence_tokenizer.encode(str(inp)))\n</code></pre>"},{"location":"reference/labeler/#src.autolabel.labeler.LabelingAgent.handle_existing_task_run","title":"<code>handle_existing_task_run(task_run, csv_file_name, gt_labels=None, additional_metrics=[])</code>","text":"<p>Allows for continuing an existing labeling task. The user will be asked whether they wish to continue from where the run previously left off, or restart from the beginning. Args:     task_run: TaskRun to retry     csv_file_name: path to the dataset we wish to label (only used if user chooses to restart the task)     gt_labels: If ground truth labels are provided, performance metrics will be displayed, such as label accuracy</p> Source code in <code>src/autolabel/labeler.py</code> <pre><code>def handle_existing_task_run(\n    self,\n    task_run: TaskRun,\n    csv_file_name: str,\n    gt_labels: List[str] = None,\n    additional_metrics: List[BaseMetric] = [],\n) -&gt; TaskRun:\n    \"\"\"\n    Allows for continuing an existing labeling task. The user will be asked whether they wish to continue from where the run previously left off, or restart from the beginning.\n    Args:\n        task_run: TaskRun to retry\n        csv_file_name: path to the dataset we wish to label (only used if user chooses to restart the task)\n        gt_labels: If ground truth labels are provided, performance metrics will be displayed, such as label accuracy\n    \"\"\"\n    self.console.print(\n        f\"There is an existing task with following details: {task_run}\"\n    )\n    llm_labels = self.get_all_annotations()\n    if gt_labels and len(llm_labels) &gt; 0:\n        pprint(\"Evaluating the existing task...\")\n        gt_labels = (\n            gt_labels[: len(llm_labels)]\n            if isinstance(gt_labels, list)\n            else {k: v[: len(llm_labels)] for k, v in gt_labels.items()}\n        )\n        eval_result = self.task.eval(\n            llm_labels, gt_labels, additional_metrics=additional_metrics\n        )\n        table = {}\n        for m in eval_result:\n            if isinstance(m.value, list):\n                continue\n            elif m.show_running:\n                table[m.name] = m.value\n            else:\n                self.console.print(f\"{m.name}:\\n{m.value}\")\n\n        print_table(table, console=self.console, default_style=METRIC_TABLE_STYLE)\n    self.console.print(f\"{task_run.current_index} examples labeled so far.\")\n    if not Confirm.ask(\"Do you want to resume the task?\"):\n        TaskRunModel.delete_by_id(self.db.session, task_run.id)\n        self.console.print(\"Deleted the existing task and starting a new one...\")\n        task_run = self.db.create_task_run(\n            csv_file_name, self.task_object.id, self.dataset_obj.id\n        )\n    return task_run\n</code></pre>"},{"location":"reference/labeler/#src.autolabel.labeler.LabelingAgent.plan","title":"<code>plan(dataset, max_items=None, start_index=0)</code>","text":"<p>Calculates and prints the cost of calling autolabel.run() on a given dataset</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>AutolabelDataset</code> <p>path to a CSV dataset</p> required Source code in <code>src/autolabel/labeler.py</code> <pre><code>def plan(\n    self,\n    dataset: AutolabelDataset,\n    max_items: Optional[int] = None,\n    start_index: int = 0,\n) -&gt; None:\n    \"\"\"Calculates and prints the cost of calling autolabel.run() on a given dataset\n\n    Args:\n        dataset: path to a CSV dataset\n    \"\"\"\n    dataset = dataset.get_slice(max_items=max_items, start_index=start_index)\n\n    if (\n        self.config.confidence()\n        and \"REFUEL_API_KEY\" not in os.environ\n        and not self.llm.returns_token_probs()\n    ):\n        raise ValueError(\n            \"REFUEL_API_KEY environment variable must be set to compute confidence scores. You can request an API key at https://refuel-ai.typeform.com/llm-access.\"\n        )\n\n    prompt_list = []\n    total_cost = 0\n\n    # Get the seed examples from the dataset config\n    seed_examples = self.config.few_shot_example_set()\n\n    # If this dataset config is a string, read the corrresponding csv file\n    if isinstance(seed_examples, str):\n        seed_loader = AutolabelDataset(seed_examples, self.config)\n        seed_examples = seed_loader.inputs\n\n    # Check explanations are present in data if explanation_column is passed in\n    if (\n        self.config.explanation_column()\n        and len(seed_examples) &gt; 0\n        and self.config.explanation_column() not in list(seed_examples[0].keys())\n    ):\n        raise ValueError(\n            f\"Explanation column {self.config.explanation_column()} not found in dataset.\\nMake sure that explanations were generated using labeler.generate_explanations(seed_file).\"\n        )\n\n    self.example_selector = ExampleSelectorFactory.initialize_selector(\n        self.config,\n        [safe_serialize_to_string(example) for example in seed_examples],\n        dataset.df.keys().tolist(),\n        cache=self.generation_cache is not None,\n    )\n\n    if self.config.label_selection():\n        if self.config.task_type() != TaskType.CLASSIFICATION:\n            self.console.print(\n                \"Warning: label_selection only supported for classification tasks!\"\n            )\n        else:\n            self.label_selector = LabelSelector(\n                config=self.config,\n                embedding_func=PROVIDER_TO_MODEL.get(\n                    self.config.embedding_provider(), DEFAULT_EMBEDDING_PROVIDER\n                )(model=self.config.embedding_model_name()),\n            )\n\n    input_limit = min(len(dataset.inputs), 100) if max_items is None else max_items  # type: ignore\n    for input_i in track(\n        dataset.inputs[:input_limit],\n        description=\"Generating Prompts...\",\n        console=self.console,\n    ):\n        # TODO: Check if this needs to use the example selector\n        if self.example_selector:\n            examples = self.example_selector.select_examples(\n                safe_serialize_to_string(input_i)\n            )\n        else:\n            examples = []\n        if (\n            self.config.label_selection()\n            and self.config.task_type() == TaskType.CLASSIFICATION\n        ):\n            selected_labels = self.label_selector.select_labels(input_i[\"example\"])\n            final_prompt = self.task.construct_prompt(\n                input_i,\n                examples,\n                selected_labels=selected_labels,\n                max_input_tokens=self.llm.DEFAULT_CONTEXT_LENGTH,\n                get_num_tokens=self.llm.get_num_tokens,\n            )\n        else:\n            final_prompt = self.task.construct_prompt(\n                input_i,\n                examples,\n                max_input_tokens=self.llm.DEFAULT_CONTEXT_LENGTH,\n                get_num_tokens=self.llm.get_num_tokens,\n            )\n        prompt_list.append(final_prompt)\n\n        # Calculate the number of tokens\n        curr_cost = self.llm.get_cost(prompt=final_prompt, label=\"\")\n        total_cost += curr_cost\n\n    total_cost = total_cost * (len(dataset.inputs) / input_limit)\n    table = {\n        \"Total Estimated Cost\": f\"${maybe_round(total_cost)}\",\n        \"Number of Examples\": len(dataset.inputs),\n        \"Average cost per example\": f\"${maybe_round(total_cost / len(dataset.inputs))}\",\n    }\n    table = {\"parameter\": list(table.keys()), \"value\": list(table.values())}\n\n    print_table(\n        table, show_header=False, console=self.console, styles=COST_TABLE_STYLES\n    )\n    self.console.rule(\"Prompt Example\")\n    self.console.print(f\"{prompt_list[0]}\", markup=False)\n    self.console.rule()\n</code></pre>"},{"location":"reference/labeler/#src.autolabel.labeler.LabelingAgent.run","title":"<code>run(dataset, output_name=None, max_items=None, start_index=0, additional_metrics=[], skip_eval=False)</code>","text":"<p>Labels data in a given dataset. Output written to new CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>AutolabelDataset</code> <p>path to CSV dataset to be annotated</p> required <code>max_items</code> <code>Optional[int]</code> <p>maximum items in dataset to be annotated</p> <code>None</code> <code>output_name</code> <code>Optional[str]</code> <p>custom name of output CSV file</p> <code>None</code> <code>start_index</code> <code>int</code> <p>skips annotating [0, start_index)</p> <code>0</code> Source code in <code>src/autolabel/labeler.py</code> <pre><code>def run(\n    self,\n    dataset: AutolabelDataset,\n    output_name: Optional[str] = None,\n    max_items: Optional[int] = None,\n    start_index: int = 0,\n    additional_metrics: Optional[List[BaseMetric]] = [],\n    skip_eval: Optional[bool] = False,\n) -&gt; Tuple[pd.Series, pd.DataFrame, List[MetricResult]]:\n    \"\"\"Labels data in a given dataset. Output written to new CSV file.\n\n    Args:\n        dataset: path to CSV dataset to be annotated\n        max_items: maximum items in dataset to be annotated\n        output_name: custom name of output CSV file\n        start_index: skips annotating [0, start_index)\n    \"\"\"\n\n    dataset = dataset.get_slice(max_items=max_items, start_index=start_index)\n\n    if self.create_task:\n        self.db.initialize()\n        self.dataset_obj = self.db.initialize_dataset(dataset.df, self.config)\n        self.task_object = self.db.initialize_task(self.config)\n    else:\n        self.all_annotations = []\n\n    if isinstance(dataset, str):\n        csv_file_name = (\n            output_name\n            if output_name\n            else f\"{dataset.replace('.csv','')}_labeled.csv\"\n        )\n    else:\n        csv_file_name = f\"{self.config.task_name()}_labeled.csv\"\n\n    if self.create_task:\n        # Initialize task run and check if it already exists\n        self.task_run = self.db.get_task_run(\n            self.task_object.id, self.dataset_obj.id\n        )\n        # Resume/Delete the task if it already exists or create a new task run\n        if self.task_run:\n            logger.info(\"Task run already exists.\")\n            self.task_run = self.handle_existing_task_run(\n                self.task_run,\n                csv_file_name,\n                gt_labels=dataset.gt_labels,\n                additional_metrics=additional_metrics,\n            )\n        else:\n            self.task_run = self.db.create_task_run(\n                csv_file_name, self.task_object.id, self.dataset_obj.id\n            )\n\n    # Get the seed examples from the dataset config\n    seed_examples = self.config.few_shot_example_set()\n\n    # If this dataset config is a string, read the corrresponding csv file\n    if isinstance(seed_examples, str):\n        seed_loader = AutolabelDataset(seed_examples, self.config)\n        seed_examples = seed_loader.inputs\n\n    # Check explanations are present in data if explanation_column is passed in\n    if (\n        self.config.explanation_column()\n        and len(seed_examples) &gt; 0\n        and self.config.explanation_column() not in list(seed_examples[0].keys())\n    ):\n        raise ValueError(\n            f\"Explanation column {self.config.explanation_column()} not found in dataset.\\nMake sure that explanations were generated using labeler.generate_explanations(seed_file).\"\n        )\n\n    if self.example_selector is None:\n        if (\n            self.config.label_selection()\n            and self.config.few_shot_algorithm() != \"fixed\"\n        ):\n            # TODO: Add support for other few shot algorithms specially semantic similarity\n            raise ValueError(\n                \"Error: Only 'fixed' few shot example selector is supported for label selection.\"\n            )\n\n        self.example_selector = ExampleSelectorFactory.initialize_selector(\n            self.config,\n            [safe_serialize_to_string(example) for example in seed_examples],\n            dataset.df.keys().tolist(),\n            cache=self.generation_cache is not None,\n        )\n\n    if self.config.label_selection():\n        if self.config.task_type() != TaskType.CLASSIFICATION:\n            self.console.print(\n                \"Warning: label_selection only supported for classification tasks!\"\n            )\n        else:\n            self.label_selector = LabelSelector(\n                config=self.config,\n                embedding_func=PROVIDER_TO_MODEL.get(\n                    self.config.embedding_provider(), DEFAULT_EMBEDDING_PROVIDER\n                )(model=self.config.embedding_model_name()),\n            )\n\n    current_index = self.task_run.current_index if self.create_task else 0\n    cost = 0.0\n    postfix_dict = {}\n\n    indices = range(current_index, len(dataset.inputs))\n    selected_labels = self.config.labels_list()\n    for current_index in track_with_stats(\n        indices,\n        postfix_dict,\n        total=len(dataset.inputs) - current_index,\n        console=self.console,\n    ):\n        chunk = dataset.inputs[current_index]\n        examples = []\n\n        if (\n            self.config.label_selection()\n            and self.config.task_type() == TaskType.CLASSIFICATION\n        ):\n            # get every column except the one we want to label\n            toEmbed = chunk.copy()\n            if self.config.label_column() and self.config.label_column() in toEmbed:\n                del toEmbed[self.config.label_column()]\n\n            # convert this to a string\n            toEmbed = json.dumps(toEmbed)\n\n            selected_labels = self.label_selector.select_labels(toEmbed)\n\n            if self.example_selector:\n                examples = self.example_selector.select_examples(\n                    safe_serialize_to_string(chunk),\n                    selected_labels=selected_labels,\n                    label_column=self.config.label_column(),\n                )\n            else:\n                examples = []\n        else:\n            if self.example_selector:\n                examples = self.example_selector.select_examples(\n                    safe_serialize_to_string(chunk),\n                )\n\n        # Construct Prompt to pass to LLM\n        final_prompt = self.task.construct_prompt(\n            chunk,\n            examples,\n            selected_labels=selected_labels,\n            max_input_tokens=self.llm.DEFAULT_CONTEXT_LENGTH,\n            get_num_tokens=self.llm.get_num_tokens,\n        )\n\n        response = self.llm.label([final_prompt])\n        for i, generations, error, latency in zip(\n            range(len(response.generations)),\n            response.generations,\n            response.errors,\n            response.latencies,\n        ):\n            input_tokens = self.llm.get_num_tokens(final_prompt)\n            if error is not None:\n                annotation = LLMAnnotation(\n                    successfully_labeled=False,\n                    label=self.task.NULL_LABEL_TOKEN,\n                    raw_response=\"\",\n                    curr_sample=pickle.dumps(chunk),\n                    prompt=final_prompt,\n                    confidence_score=0,\n                    error=error,\n                    input_tokens=input_tokens,\n                    cost=0,\n                    latency=0,\n                )\n            else:\n                annotations = []\n                for generation in generations:\n                    annotation = self.task.parse_llm_response(\n                        generation, chunk, final_prompt\n                    )\n                    annotation.confidence_prompt = (\n                        self.task.construct_confidence_prompt(chunk, examples)\n                    )\n                    annotation.input_tokens = input_tokens\n                    annotation.output_tokens = self.llm.get_num_tokens(\n                        annotation.raw_response\n                    )\n                    annotation.cost = sum(response.costs)\n                    annotation.latency = latency\n\n                    if self.config.confidence():\n                        try:\n                            annotation.confidence_score = self.get_confidence_score(\n                                annotation, chunk, examples\n                            )\n                        except Exception as e:\n                            logger.error(f\"Error calculating confidence score: {e}\")\n                            if (\n                                self.config.task_type()\n                                == TaskType.ATTRIBUTE_EXTRACTION\n                            ):\n                                annotation.confidence_score = {}\n                            else:\n                                annotation.confidence_score = 0\n\n                    annotations.append(annotation)\n                annotation = self.majority_annotation(annotations)\n\n            # Save the annotation in the database\n            self.save_annotation(annotation, current_index, i)\n\n        cost += sum(response.costs)\n        postfix_dict[self.COST_KEY] = f\"{cost:.2f}\"\n\n        # Evaluate the task every eval_every examples\n        if not skip_eval and (current_index + 1) % 100 == 0:\n            llm_labels = self.get_all_annotations()\n            if dataset.gt_labels:\n                eval_result = self.task.eval(\n                    llm_labels,\n                    (\n                        dataset.gt_labels[: len(llm_labels)]\n                        if isinstance(dataset.gt_labels, list)\n                        else {\n                            k: v[: len(llm_labels)]\n                            for k, v in dataset.gt_labels.items()\n                        }\n                    ),\n                    additional_metrics=additional_metrics,\n                )\n\n                for m in eval_result:\n                    # This is a row wise metric\n                    if isinstance(m.value, list):\n                        continue\n                    elif m.show_running:\n                        postfix_dict[m.name] = (\n                            f\"{m.value:.4f}\"\n                            if isinstance(m.value, float)\n                            else m.value\n                        )\n\n        if self.create_task:\n            # Update task run state\n            self.task_run = self.save_task_run_state(\n                current_index=current_index + len(chunk)\n            )\n\n    llm_labels = self.get_all_annotations()\n    eval_result = None\n    table = {}\n\n    # if true labels are provided, evaluate accuracy of predictions\n    if not skip_eval and dataset.gt_labels:\n        eval_result = self.task.eval(\n            llm_labels,\n            (\n                dataset.gt_labels[: len(llm_labels)]\n                if isinstance(dataset.gt_labels, list)\n                else {k: v[: len(llm_labels)] for k, v in dataset.gt_labels.items()}\n            ),\n            additional_metrics=additional_metrics,\n        )\n        # TODO: serialize and write to file\n        for m in eval_result:\n            if isinstance(m.value, list):\n                continue\n            elif m.show_running:\n                table[m.name] = m.value\n            else:\n                self.console.print(f\"{m.name}:\\n{m.value}\")\n\n    # print cost\n    self.console.print(f\"Actual Cost: {maybe_round(cost)}\")\n    print_table(table, console=self.console, default_style=METRIC_TABLE_STYLE)\n\n    dataset.process_labels(llm_labels, eval_result)\n    # Only save to csv if output_name is provided or dataset is a string\n    if not output_name and isinstance(dataset, str):\n        output_name = (\n            dataset.rsplit(\".\", 1)[0] + \"_labeled.\" + dataset.rsplit(\".\", 1)[1]\n        )\n\n    if output_name:\n        dataset.save(output_file_name=output_name)\n    return dataset\n</code></pre>"},{"location":"reference/labeler/#src.autolabel.labeler.LabelingAgent.save_task_run_state","title":"<code>save_task_run_state(current_index=None, status='', error='')</code>","text":"<p>Saves the current state of the Task being performed</p> Source code in <code>src/autolabel/labeler.py</code> <pre><code>def save_task_run_state(\n    self, current_index: int = None, status: TaskStatus = \"\", error: str = \"\"\n) -&gt; TaskRun:\n    \"\"\"Saves the current state of the Task being performed\"\"\"\n    # Save the current state of the task\n    if error:\n        self.task_run.error = error\n    if status:\n        self.task_run.status = status\n    if current_index:\n        self.task_run.current_index = current_index\n    return TaskRunModel.update(self.db.session, self.task_run)\n</code></pre>"},{"location":"reference/models/","title":"Models","text":"<p>             Bases: <code>ABC</code></p> Source code in <code>src/autolabel/models/base.py</code> <pre><code>class BaseModel(ABC):\n    TTL_MS = 60 * 60 * 24 * 7 * 1000  # 1 week\n    DEFAULT_CONTEXT_LENGTH = None\n\n    def __init__(self, config: AutolabelConfig, cache: BaseCache) -&gt; None:\n        self.config = config\n        self.cache = cache\n        self.model_params = config.model_params()\n        # Specific classes that implement this interface should run initialization steps here\n        # E.g. initializing the LLM model with required parameters from ModelConfig\n\n    def label(self, prompts: List[str]) -&gt; RefuelLLMResult:\n        \"\"\"Label a list of prompts.\"\"\"\n        existing_prompts = {}\n        missing_prompt_idxs = list(range(len(prompts)))\n        missing_prompts = prompts\n        costs = []\n        errors = [None for i in range(len(prompts))]\n        latencies = [0 for i in range(len(prompts))]\n        if self.cache:\n            (\n                existing_prompts,\n                missing_prompt_idxs,\n                missing_prompts,\n            ) = self.get_cached_prompts(prompts)\n        # label missing prompts\n        if len(missing_prompts) &gt; 0:\n            new_results = self._label(missing_prompts)\n            for ind, prompt in enumerate(missing_prompts):\n                costs.append(\n                    self.get_cost(prompt, label=new_results.generations[ind][0].text)\n                )\n\n            # Set the existing prompts to the new results\n            for i, result, error, latency in zip(\n                missing_prompt_idxs,\n                new_results.generations,\n                new_results.errors,\n                new_results.latencies,\n            ):\n                existing_prompts[i] = result\n                errors[i] = error\n                latencies[i] = latency\n\n            if self.cache:\n                self.update_cache(missing_prompt_idxs, new_results, prompts)\n        generations = [existing_prompts[i] for i in range(len(prompts))]\n        return RefuelLLMResult(\n            generations=generations, costs=costs, errors=errors, latencies=latencies\n        )\n\n    def _label_individually(self, prompts: List[str]) -&gt; RefuelLLMResult:\n        \"\"\"Label each prompt individually. Should be used only after trying as a batch first.\n\n        Args:\n            prompts (List[str]): List of prompts to label\n\n        Returns:\n            LLMResult: LLMResult object with generations\n            List[LabelingError]: List of errors encountered while labeling\n        \"\"\"\n        generations = []\n        errors = []\n        latencies = []\n        for prompt in prompts:\n            try:\n                start_time = time()\n                response = self.llm.generate([prompt])\n                generations.append(response.generations[0])\n                errors.append(None)\n                latencies.append(time() - start_time)\n            except Exception as e:\n                print(f\"Error generating from LLM: {e}\")\n                generations.append([Generation(text=\"\")])\n                errors.append(\n                    LabelingError(\n                        error_type=ErrorType.LLM_PROVIDER_ERROR, error_message=str(e)\n                    )\n                )\n                latencies.append(0)\n\n        return RefuelLLMResult(\n            generations=generations, errors=errors, latencies=latencies\n        )\n\n    @abstractmethod\n    def _label(self, prompts: List[str]) -&gt; RefuelLLMResult:\n        # TODO: change return type to do parsing in the Model class\n        pass\n\n    @abstractmethod\n    def get_cost(self, prompt: str, label: Optional[str] = \"\") -&gt; float:\n        pass\n\n    def get_cached_prompts(self, prompts: List[str]) -&gt; Optional[str]:\n        \"\"\"Get prompts that are already cached.\"\"\"\n        model_params_string = str(\n            sorted([(k, v) for k, v in self.model_params.items()])\n        )\n        missing_prompts = []\n        missing_prompt_idxs = []\n        existing_prompts = {}\n        for i, prompt in enumerate(prompts):\n            cache_entry = GenerationCacheEntry(\n                prompt=prompt,\n                model_name=self.model_name,\n                model_params=model_params_string,\n            )\n            cache_val = self.cache.lookup(cache_entry)\n            if cache_val:\n                existing_prompts[i] = cache_val\n            else:\n                missing_prompts.append(prompt)\n                missing_prompt_idxs.append(i)\n        return (\n            existing_prompts,\n            missing_prompt_idxs,\n            missing_prompts,\n        )\n\n    def update_cache(self, missing_prompt_idxs, new_results, prompts):\n        \"\"\"Update the cache with new results.\"\"\"\n        model_params_string = str(\n            sorted([(k, v) for k, v in self.model_params.items()])\n        )\n\n        for i, result, error in zip(\n            missing_prompt_idxs, new_results.generations, new_results.errors\n        ):\n            # If there was an error, don't cache the result\n            if error is not None:\n                continue\n\n            cache_entry = GenerationCacheEntry(\n                prompt=prompts[i],\n                model_name=self.model_name,\n                model_params=model_params_string,\n                generations=result,\n                ttl_ms=self.TTL_MS,\n            )\n            self.cache.update(cache_entry)\n\n    @abstractmethod\n    def returns_token_probs(self) -&gt; bool:\n        \"\"\"Whether the LLM supports returning logprobs of generated tokens\n\n        Returns:\n            bool: whether the LLM returns supports returning logprobs of generated tokens\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_num_tokens(self, prompt: str) -&gt; int:\n        \"\"\"\n        Get the number of tokens in the prompt\"\"\"\n        pass\n</code></pre> <p>             Bases: <code>BaseModel</code></p> Source code in <code>src/autolabel/models/anthropic.py</code> <pre><code>class AnthropicLLM(BaseModel):\n    DEFAULT_MODEL = \"claude-instant-v1\"\n    DEFAULT_PARAMS = {\n        \"max_tokens_to_sample\": 1000,\n        \"temperature\": 0.0,\n    }\n\n    # Reference: https://cdn2.assets-servd.host/anthropic-website/production/images/apr-pricing-tokens.pdf\n    COST_PER_PROMPT_TOKEN = {\n        # $11.02 per million tokens\n        \"claude-v1\": (11.02 / 1000000),\n        \"claude-instant-v1\": (1.63 / 1000000),\n    }\n    COST_PER_COMPLETION_TOKEN = {\n        # $32.68 per million tokens\n        \"claude-v1\": (32.68 / 1000000),\n        \"claude-instant-v1\": (5.51 / 1000000),\n    }\n\n    def __init__(self, config: AutolabelConfig, cache: BaseCache = None) -&gt; None:\n        super().__init__(config, cache)\n\n        try:\n            from langchain.chat_models import ChatAnthropic\n            from anthropic._tokenizers import sync_get_tokenizer\n        except ImportError:\n            raise ImportError(\n                \"anthropic is required to use the anthropic LLM. Please install it with the following command: pip install 'refuel-autolabel[anthropic]'\"\n            )\n\n        # populate model name\n        self.model_name = config.model_name() or self.DEFAULT_MODEL\n        # populate model params\n        model_params = config.model_params()\n        self.model_params = {**self.DEFAULT_PARAMS, **model_params}\n        # initialize LLM\n        self.llm = ChatAnthropic(model=self.model_name, **self.model_params)\n\n        self.tokenizer = sync_get_tokenizer()\n\n    def _label(self, prompts: List[str]) -&gt; RefuelLLMResult:\n        prompts = [[HumanMessage(content=prompt)] for prompt in prompts]\n        try:\n            start_time = time()\n            result = self.llm.generate(prompts)\n            end_time = time()\n            return RefuelLLMResult(\n                generations=result.generations,\n                errors=[None] * len(result.generations),\n                latencies=[end_time - start_time] * len(result.generations),\n            )\n        except Exception as e:\n            return self._label_individually(prompts)\n\n    def get_cost(self, prompt: str, label: Optional[str] = \"\") -&gt; float:\n        num_prompt_toks = len(self.tokenizer.encode(prompt).ids)\n        if label:\n            num_label_toks = len(self.tokenizer.encode(label).ids)\n        else:\n            # get an upper bound\n            num_label_toks = self.model_params[\"max_tokens_to_sample\"]\n\n        cost_per_prompt_token = self.COST_PER_PROMPT_TOKEN[self.model_name]\n        cost_per_completion_token = self.COST_PER_COMPLETION_TOKEN[self.model_name]\n        return (num_prompt_toks * cost_per_prompt_token) + (\n            num_label_toks * cost_per_completion_token\n        )\n\n    def returns_token_probs(self) -&gt; bool:\n        return False\n\n    def get_num_tokens(self, prompt: str) -&gt; int:\n        return len(self.tokenizer.encode(prompt).ids)\n</code></pre> <p>             Bases: <code>BaseModel</code></p> Source code in <code>src/autolabel/models/hf_pipeline.py</code> <pre><code>class HFPipelineLLM(BaseModel):\n    DEFAULT_MODEL = \"google/flan-t5-xxl\"\n    DEFAULT_PARAMS = {\"temperature\": 0.0, \"quantize\": 8}\n\n    def __init__(self, config: AutolabelConfig, cache: BaseCache = None) -&gt; None:\n        super().__init__(config, cache)\n\n        from langchain.llms import HuggingFacePipeline\n\n        try:\n            from transformers import (\n                AutoConfig,\n                AutoModelForSeq2SeqLM,\n                AutoModelForCausalLM,\n                AutoTokenizer,\n                pipeline,\n            )\n            from transformers.models.auto.modeling_auto import (\n                MODEL_FOR_CAUSAL_LM_MAPPING,\n                MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING,\n            )\n        except ImportError:\n            raise ValueError(\n                \"Could not import transformers python package. \"\n                \"Please it install it with `pip install transformers`.\"\n            )\n\n        try:\n            import torch\n        except ImportError:\n            raise ValueError(\n                \"Could not import torch package. \"\n                \"Please it install it with `pip install torch`.\"\n            )\n        # populate model name\n        self.model_name = config.model_name() or self.DEFAULT_MODEL\n\n        # populate model params\n        model_params = config.model_params()\n        self.model_params = {**self.DEFAULT_PARAMS, **model_params}\n        if config.logit_bias() != 0:\n            self.model_params = {\n                **self._generate_sequence_bias(),\n                **self.model_params,\n            }\n\n        # initialize HF pipeline\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            self.model_name, use_fast=False, add_prefix_space=True\n        )\n        quantize_bits = self.model_params[\"quantize\"]\n        model_config = AutoConfig.from_pretrained(self.model_name)\n        if isinstance(model_config, tuple(MODEL_FOR_CAUSAL_LM_MAPPING)):\n            AutoModel = AutoModelForCausalLM\n        elif isinstance(model_config, tuple(MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING)):\n            AutoModel = AutoModelForSeq2SeqLM\n        else:\n            raise ValueError(\n                \"model_name is neither a causal LM nor a seq2seq LM. Please check the model_name.\"\n            )\n\n        if not torch.cuda.is_available():\n            model = AutoModel.from_pretrained(self.model_name)\n        elif quantize_bits == 8:\n            model = AutoModel.from_pretrained(\n                self.model_name, load_in_8bit=True, device_map=\"auto\"\n            )\n        elif quantize_bits == \"16\":\n            model = AutoModel.from_pretrained(\n                self.model_name, torch_dtype=torch.float16, device_map=\"auto\"\n            )\n        else:\n            model = AutoModel.from_pretrained(self.model_name, device_map=\"auto\")\n\n        model_kwargs = dict(self.model_params)  # make a copy of the model params\n        model_kwargs.pop(\"quantize\", None)  # remove quantize from the model params\n        pipe = pipeline(\n            \"text2text-generation\",\n            model=model,\n            tokenizer=self.tokenizer,\n            **model_kwargs,\n        )\n\n        # initialize LLM\n        self.llm = HuggingFacePipeline(pipeline=pipe, model_kwargs=model_kwargs)\n\n    def _generate_sequence_bias(self) -&gt; Dict:\n        \"\"\"Generates sequence bias dict to add to the config for the labels specified\n\n        Returns:\n            Dict: sequence bias, max new tokens, and num beams\n        \"\"\"\n        if len(self.config.labels_list()) == 0:\n            logger.warning(\n                \"No labels specified in the config. Skipping logit bias generation.\"\n            )\n            return {}\n        try:\n            from transformers import AutoTokenizer\n        except ImportError:\n            raise ValueError(\n                \"Could not import transformers python package. \"\n                \"Please it install it with `pip install transformers`.\"\n            )\n        sequence_bias = {tuple([self.tokenizer.eos_token_id]): self.config.logit_bias()}\n        max_new_tokens = 0\n        for label in self.config.labels_list():\n            tokens = tuple(\n                self.tokenizer([label], add_special_tokens=False).input_ids[0]\n            )\n            for token in tokens:\n                sequence_bias[tuple([token])] = self.config.logit_bias()\n            max_new_tokens = max(max_new_tokens, len(tokens))\n\n        return {\n            \"sequence_bias\": sequence_bias,\n            \"max_new_tokens\": max_new_tokens,\n        }\n\n    def _label(self, prompts: List[str]) -&gt; RefuelLLMResult:\n        try:\n            start_time = time()\n            result = self.llm.generate(prompts)\n            end_time = time()\n            return RefuelLLMResult(\n                generations=result.generations,\n                errors=[None] * len(result.generations),\n                latencies=[end_time - start_time] * len(result.generations),\n            )\n        except Exception as e:\n            return self._label_individually(prompts)\n\n    def get_cost(self, prompt: str, label: Optional[str] = \"\") -&gt; float:\n        # Model inference for this model is being run locally\n        # Revisit this in the future when we support HF inference endpoints\n        return 0.0\n\n    def returns_token_probs(self) -&gt; bool:\n        return False\n\n    def get_num_tokens(self, prompt: str) -&gt; int:\n        return len(self.tokenizer.encode(prompt))\n</code></pre> <p>             Bases: <code>BaseModel</code></p> Source code in <code>src/autolabel/models/openai.py</code> <pre><code>class OpenAILLM(BaseModel):\n    CHAT_ENGINE_MODELS = [\n        \"gpt-3.5-turbo\",\n        \"gpt-3.5-turbo-0301\",\n        \"gpt-3.5-turbo-0613\",\n        \"gpt-3.5-turbo-16k\",\n        \"gpt-3.5-turbo-16k-0613\",\n        \"gpt-4\",\n        \"gpt-4-0314\",\n        \"gpt-4-32k-0314\",\n        \"gpt-4-0613\",\n        \"gpt-4-32k\",\n        \"gpt-4-32k-0613\",\n        \"gpt-4-1106-preview\",\n        \"gpt-4-0125-preview\",\n    ]\n    MODELS_WITH_TOKEN_PROBS = [\n        \"text-curie-001\",\n        \"text-davinci-003\",\n        \"gpt-3.5-turbo\",\n        \"gpt-3.5-turbo-0301\",\n        \"gpt-3.5-turbo-0613\",\n        \"gpt-3.5-turbo-16k\",\n        \"gpt-3.5-turbo-16k-0613\",\n        \"gpt-4\",\n        \"gpt-4-0314\",\n        \"gpt-4-32k-0314\",\n        \"gpt-4-0613\",\n        \"gpt-4-32k\",\n        \"gpt-4-32k-0613\",\n        \"gpt-4-1106-preview\",\n        \"gpt-4-0125-preview\",\n    ]\n\n    # Default parameters for OpenAILLM\n    DEFAULT_MODEL = \"gpt-3.5-turbo\"\n    DEFAULT_PARAMS_COMPLETION_ENGINE = {\n        \"max_tokens\": 1000,\n        \"temperature\": 0.0,\n        \"model_kwargs\": {\"logprobs\": 1},\n        \"request_timeout\": 30,\n    }\n    DEFAULT_PARAMS_CHAT_ENGINE = {\n        \"max_tokens\": 1000,\n        \"temperature\": 0.0,\n        \"request_timeout\": 30,\n    }\n    DEFAULT_QUERY_PARAMS_CHAT_ENGINE = {\"logprobs\": True, \"top_logprobs\": 1}\n\n    # Reference: https://openai.com/pricing\n    COST_PER_PROMPT_TOKEN = {\n        \"text-davinci-003\": 0.02 / 1000,\n        \"text-curie-001\": 0.002 / 1000,\n        \"gpt-3.5-turbo\": 0.0015 / 1000,\n        \"gpt-3.5-turbo-0301\": 0.0015 / 1000,\n        \"gpt-3.5-turbo-0613\": 0.0015 / 1000,\n        \"gpt-3.5-turbo-16k\": 0.003 / 1000,\n        \"gpt-3.5-turbo-16k-0613\": 0.003 / 1000,\n        \"gpt-4\": 0.03 / 1000,\n        \"gpt-4-0613\": 0.03 / 1000,\n        \"gpt-4-32k\": 0.06 / 1000,\n        \"gpt-4-32k-0613\": 0.06 / 1000,\n        \"gpt-4-0314\": 0.03 / 1000,\n        \"gpt-4-32k-0314\": 0.06 / 1000,\n        \"gpt-4-1106-preview\": 0.01 / 1000,\n        \"gpt-4-0125-preview\": 0.01 / 1000,\n    }\n    COST_PER_COMPLETION_TOKEN = {\n        \"text-davinci-003\": 0.02 / 1000,\n        \"text-curie-001\": 0.002 / 1000,\n        \"gpt-3.5-turbo\": 0.002 / 1000,\n        \"gpt-3.5-turbo-0301\": 0.002 / 1000,\n        \"gpt-3.5-turbo-0613\": 0.002 / 1000,\n        \"gpt-3.5-turbo-16k\": 0.004 / 1000,\n        \"gpt-3.5-turbo-16k-0613\": 0.004 / 1000,\n        \"gpt-4\": 0.06 / 1000,\n        \"gpt-4-0613\": 0.06 / 1000,\n        \"gpt-4-32k\": 0.12 / 1000,\n        \"gpt-4-32k-0613\": 0.12 / 1000,\n        \"gpt-4-0314\": 0.06 / 1000,\n        \"gpt-4-32k-0314\": 0.12 / 1000,\n        \"gpt-4-1106-preview\": 0.03 / 1000,\n        \"gpt-4-0125-preview\": 0.03 / 1000,\n    }\n\n    @cached_property\n    def _engine(self) -&gt; str:\n        if self.model_name is not None and self.model_name in self.CHAT_ENGINE_MODELS:\n            return \"chat\"\n        else:\n            return \"completion\"\n\n    def __init__(self, config: AutolabelConfig, cache: BaseCache = None) -&gt; None:\n        super().__init__(config, cache)\n        try:\n            import tiktoken\n            from langchain.chat_models import ChatOpenAI\n            from langchain.llms import OpenAI\n        except ImportError:\n            raise ImportError(\n                \"openai is required to use the OpenAILLM. Please install it with the following command: pip install 'refuel-autolabel[openai]'\"\n            )\n\n        # populate model name\n        self.model_name = config.model_name() or self.DEFAULT_MODEL\n\n        if os.getenv(\"OPENAI_API_KEY\") is None:\n            raise ValueError(\"OPENAI_API_KEY environment variable not set\")\n\n        # populate model params and initialize the LLM\n        model_params = config.model_params()\n        if config.logit_bias() != 0:\n            model_params = {\n                **self._generate_logit_bias(),\n                **model_params,\n            }\n\n        if self._engine == \"chat\":\n            self.model_params = {**self.DEFAULT_PARAMS_CHAT_ENGINE, **model_params}\n            self.query_params = self.DEFAULT_QUERY_PARAMS_CHAT_ENGINE\n            self.llm = ChatOpenAI(\n                model_name=self.model_name, verbose=False, **self.model_params\n            )\n        else:\n            self.model_params = {\n                **self.DEFAULT_PARAMS_COMPLETION_ENGINE,\n                **model_params,\n            }\n            self.llm = OpenAI(\n                model_name=self.model_name, verbose=False, **self.model_params\n            )\n\n        self.tiktoken = tiktoken\n\n    def _generate_logit_bias(self) -&gt; None:\n        \"\"\"Generates logit bias for the labels specified in the config\n\n        Returns:\n            Dict: logit bias and max tokens\n        \"\"\"\n        if len(self.config.labels_list()) == 0:\n            logger.warning(\n                \"No labels specified in the config. Skipping logit bias generation.\"\n            )\n            return {}\n        encoding = self.tiktoken.encoding_for_model(self.model_name)\n        logit_bias = {}\n        max_tokens = 0\n        for label in self.config.labels_list():\n            if label not in logit_bias:\n                tokens = encoding.encode(label)\n                for token in tokens:\n                    logit_bias[token] = self.config.logit_bias()\n                max_tokens = max(max_tokens, len(tokens))\n\n        return {\"logit_bias\": logit_bias, \"max_tokens\": max_tokens}\n\n    def _chat_backward_compatibility(\n        self, generations: List[LLMResult]\n    ) -&gt; List[LLMResult]:\n        for generation_options in generations:\n            for curr_generation in generation_options:\n                generation_info = curr_generation.generation_info\n                new_logprobs = {\"top_logprobs\": []}\n                for curr_token in generation_info[\"logprobs\"][\"content\"]:\n                    new_logprobs[\"top_logprobs\"].append(\n                        {curr_token[\"token\"]: curr_token[\"logprob\"]}\n                    )\n                curr_generation.generation_info[\"logprobs\"] = new_logprobs\n        return generations\n\n    def _label(self, prompts: List[str]) -&gt; RefuelLLMResult:\n        try:\n            start_time = time()\n            if self._engine == \"chat\":\n                prompts = [[HumanMessage(content=prompt)] for prompt in prompts]\n                result = self.llm.generate(prompts, **self.query_params)\n                generations = self._chat_backward_compatibility(result.generations)\n            else:\n                result = self.llm.generate(prompts)\n                generations = result.generations\n            end_time = time()\n            return RefuelLLMResult(\n                generations=generations,\n                errors=[None] * len(generations),\n                latencies=[end_time - start_time] * len(generations),\n            )\n        except Exception as e:\n            return self._label_individually(prompts)\n\n    def get_cost(self, prompt: str, label: Optional[str] = \"\") -&gt; float:\n        encoding = self.tiktoken.encoding_for_model(self.model_name)\n        num_prompt_toks = len(encoding.encode(prompt))\n        if label:\n            num_label_toks = len(encoding.encode(label))\n        else:\n            # get an upper bound\n            num_label_toks = self.model_params[\"max_tokens\"]\n\n        cost_per_prompt_token = self.COST_PER_PROMPT_TOKEN[self.model_name]\n        cost_per_completion_token = self.COST_PER_COMPLETION_TOKEN[self.model_name]\n        return (num_prompt_toks * cost_per_prompt_token) + (\n            num_label_toks * cost_per_completion_token\n        )\n\n    def returns_token_probs(self) -&gt; bool:\n        return (\n            self.model_name is not None\n            and self.model_name in self.MODELS_WITH_TOKEN_PROBS\n        )\n\n    def get_num_tokens(self, prompt: str) -&gt; int:\n        encoding = self.tiktoken.encoding_for_model(self.model_name)\n        return len(encoding.encode(prompt))\n</code></pre> <p>             Bases: <code>BaseModel</code></p> Source code in <code>src/autolabel/models/palm.py</code> <pre><code>class PaLMLLM(BaseModel):\n    SEP_REPLACEMENT_TOKEN = \"@@\"\n    CHAT_ENGINE_MODELS = [\"chat-bison@001\"]\n\n    DEFAULT_MODEL = \"text-bison@001\"\n    # Reference: https://developers.generativeai.google/guide/concepts#model_parameters for \"A token is approximately 4 characters\"\n    DEFAULT_PARAMS = {\"temperature\": 0, \"max_output_tokens\": 1000}\n\n    # Reference: https://cloud.google.com/vertex-ai/pricing\n    COST_PER_CHARACTER = {\n        \"text-bison@001\": 0.001 / 1000,\n        \"chat-bison@001\": 0.0005 / 1000,\n        \"textembedding-gecko@001\": 0.0001 / 1000,\n    }\n\n    @cached_property\n    def _engine(self) -&gt; str:\n        if self.model_name is not None and self.model_name in self.CHAT_ENGINE_MODELS:\n            return \"chat\"\n        else:\n            return \"completion\"\n\n    def __init__(\n        self,\n        config: AutolabelConfig,\n        cache: BaseCache = None,\n    ) -&gt; None:\n        super().__init__(config, cache)\n        try:\n            from langchain.chat_models import ChatVertexAI\n            from langchain.llms import VertexAI\n            import tiktoken\n        except ImportError:\n            raise ImportError(\n                \"palm is required to use the Palm LLM. Please install it with the following command: pip install 'refuel-autolabel[google]'\"\n            )\n\n        # populate model name\n        self.model_name = config.model_name() or self.DEFAULT_MODEL\n\n        # populate model params and initialize the LLM\n        model_params = config.model_params()\n        self.model_params = {\n            **self.DEFAULT_PARAMS,\n            **model_params,\n        }\n        if self._engine == \"chat\":\n            self.llm = ChatVertexAI(model_name=self.model_name, **self.model_params)\n        else:\n            self.llm = VertexAI(model_name=self.model_name, **self.model_params)\n        self.tiktoken = tiktoken\n\n    @retry(\n        reraise=True,\n        stop=stop_after_attempt(5),\n        wait=wait_exponential(multiplier=1, min=2, max=10),\n        before_sleep=before_sleep_log(logger, logging.WARNING),\n    )\n    def _label_with_retry(self, prompts: List[str]) -&gt; LLMResult:\n        start_time = time()\n        response = self.llm.generate(prompts)\n        return response, time() - start_time\n\n    def _label_individually(self, prompts: List[str]) -&gt; RefuelLLMResult:\n        \"\"\"Label each prompt individually. Should be used only after trying as a batch first.\n\n        Args:\n            prompts (List[str]): List of prompts to label\n\n        Returns:\n            RefuelLLMResult: RefuelLLMResult object\n        \"\"\"\n        generations = []\n        errors = []\n        latencies = []\n        for i, prompt in enumerate(prompts):\n            try:\n                response, latency = self._label_with_retry([prompt])\n                for generation in response.generations[0]:\n                    generation.text = generation.text.replace(\n                        self.SEP_REPLACEMENT_TOKEN, \"\\n\"\n                    )\n                generations.append(response.generations[0])\n                errors.append(None)\n                latencies.append(latency)\n            except Exception as e:\n                print(f\"Error generating from LLM: {e}, returning empty generation\")\n                generations.append([Generation(text=\"\")])\n                errors.append(\n                    LabelingError(\n                        error_type=ErrorType.LLM_PROVIDER_ERROR, error_message=str(e)\n                    )\n                )\n                latencies.append(0)\n\n        return RefuelLLMResult(\n            generations=generations, errors=errors, latencies=latencies\n        )\n\n    def _label(self, prompts: List[str]) -&gt; RefuelLLMResult:\n        for prompt in prompts:\n            if self.SEP_REPLACEMENT_TOKEN in prompt:\n                logger.warning(\n                    f\"\"\"Current prompt contains {self.SEP_REPLACEMENT_TOKEN} \n                                which is currently used as a separator token by refuel\n                                llm. It is highly recommended to avoid having any\n                                occurences of this substring in the prompt.\n                            \"\"\"\n                )\n        prompts = [\n            prompt.replace(\"\\n\", self.SEP_REPLACEMENT_TOKEN) for prompt in prompts\n        ]\n        if self._engine == \"chat\":\n            # Need to convert list[prompts] -&gt; list[messages]\n            # Currently the entire prompt is stuck into the \"human message\"\n            # We might consider breaking this up into human vs system message in future\n            prompts = [[HumanMessage(content=prompt)] for prompt in prompts]\n\n        try:\n            start_time = time()\n            result = self._label_with_retry(prompts)\n            end_time = time()\n            for generations in result.generations:\n                for generation in generations:\n                    generation.text = generation.text.replace(\n                        self.SEP_REPLACEMENT_TOKEN, \"\\n\"\n                    )\n            return RefuelLLMResult(\n                generations=result.generations,\n                errors=[None] * len(result.generations),\n                latencies=[end_time - start_time] * len(result.generations),\n            )\n        except Exception as e:\n            return self._label_individually(prompts)\n\n    def get_cost(self, prompt: str, label: Optional[str] = \"\") -&gt; float:\n        if self.model_name is None:\n            return 0.0\n        cost_per_char = self.COST_PER_CHARACTER.get(self.model_name, 0.0)\n        return cost_per_char * len(prompt) + cost_per_char * (\n            len(label) if label else 4 * self.model_params[\"max_output_tokens\"]\n        )\n\n    def returns_token_probs(self) -&gt; bool:\n        return False\n\n    def get_num_tokens(self, prompt: str) -&gt; int:\n        # TODO(dhruva): Replace with actual tokenizer once that is available\n        encoding = self.tiktoken.encoding_for_model(\"gpt2\")\n        return len(encoding.encode(prompt))\n</code></pre> <p>             Bases: <code>BaseModel</code></p> Source code in <code>src/autolabel/models/refuel.py</code> <pre><code>class RefuelLLM(BaseModel):\n    DEFAULT_TOKENIZATION_MODEL = \"NousResearch/Llama-2-13b-chat-hf\"\n    DEFAULT_CONTEXT_LENGTH = 3250\n    DEFAULT_PARAMS = {\n        \"max_new_tokens\": 128,\n    }\n\n    def __init__(\n        self,\n        config: AutolabelConfig,\n        cache: BaseCache = None,\n    ) -&gt; None:\n        super().__init__(config, cache)\n        try:\n            from transformers import AutoTokenizer\n        except Exception as e:\n            raise Exception(\n                \"Unable to import transformers. Please install transformers to use RefuelLLM\"\n            )\n\n        # populate model name\n        # This is unused today, but in the future could\n        # be used to decide which refuel model is queried\n        self.model_name = config.model_name()\n        model_params = config.model_params()\n        self.model_params = {**self.DEFAULT_PARAMS, **model_params}\n        self.tokenizer = AutoTokenizer.from_pretrained(self.DEFAULT_TOKENIZATION_MODEL)\n\n        # initialize runtime\n        self.BASE_API = f\"https://llm.refuel.ai/models/{self.model_name}/generate\"\n        self.REFUEL_API_ENV = \"REFUEL_API_KEY\"\n        if self.REFUEL_API_ENV in os.environ and os.environ[self.REFUEL_API_ENV]:\n            self.REFUEL_API_KEY = os.environ[self.REFUEL_API_ENV]\n        else:\n            raise ValueError(\n                f\"Did not find {self.REFUEL_API_ENV}, please add an environment variable\"\n                f\" `{self.REFUEL_API_ENV}` which contains it\"\n            )\n\n    @retry(\n        reraise=True,\n        stop=stop_after_attempt(5),\n        wait=wait_exponential(multiplier=1, min=2, max=10),\n        before_sleep=before_sleep_log(logger, logging.WARNING),\n        retry=retry_if_not_exception_type(UnretryableError),\n    )\n    def _label_with_retry(self, prompt: str) -&gt; Tuple[requests.Response, float]:\n        payload = {\n            \"input\": prompt,\n            \"params\": {**self.model_params},\n            \"confidence\": self.config.confidence(),\n        }\n        headers = {\"refuel_api_key\": self.REFUEL_API_KEY}\n        start_time = time()\n        response = requests.post(self.BASE_API, json=payload, headers=headers)\n        end_time = time()\n        # raise Exception if status != 200\n        if response.status_code != 200:\n            if response.status_code in UNRETRYABLE_ERROR_CODES:\n                # This is a bad request, and we should not retry\n                raise UnretryableError(\n                    f\"NonRetryable Error: Received status code {response.status_code} from Refuel API. Response: {response.text}\"\n                )\n\n            logger.warning(\n                f\"Received status code {response.status_code} from Refuel API. Response: {response.text}\"\n            )\n            response.raise_for_status()\n        return response, end_time - start_time\n\n    def _label(self, prompts: List[str]) -&gt; RefuelLLMResult:\n        generations = []\n        errors = []\n        latencies = []\n        for prompt in prompts:\n            try:\n                response, latency = self._label_with_retry(prompt)\n                response = json.loads(response.json())\n                generations.append(\n                    [\n                        Generation(\n                            text=response[\"generated_text\"],\n                            generation_info=(\n                                {\"logprobs\": {\"top_logprobs\": response[\"logprobs\"]}}\n                                if self.config.confidence()\n                                else None\n                            ),\n                        )\n                    ]\n                )\n                errors.append(None)\n                latencies.append(latency)\n            except Exception as e:\n                # This signifies an error in generating the response using RefuelLLm\n                logger.error(\n                    f\"Unable to generate prediction: {e}\",\n                )\n                generations.append([Generation(text=\"\")])\n                errors.append(\n                    LabelingError(\n                        error_type=ErrorType.LLM_PROVIDER_ERROR, error_message=str(e)\n                    )\n                )\n                latencies.append(0)\n        return RefuelLLMResult(\n            generations=generations, errors=errors, latencies=latencies\n        )\n\n    def get_cost(self, prompt: str, label: Optional[str] = \"\") -&gt; float:\n        return 0\n\n    def returns_token_probs(self) -&gt; bool:\n        return True\n\n    def get_num_tokens(self, prompt: str) -&gt; int:\n        return len(self.tokenizer.encode(prompt))\n</code></pre>"},{"location":"reference/models/#src.autolabel.models.base.BaseModel.get_cached_prompts","title":"<code>get_cached_prompts(prompts)</code>","text":"<p>Get prompts that are already cached.</p> Source code in <code>src/autolabel/models/base.py</code> <pre><code>def get_cached_prompts(self, prompts: List[str]) -&gt; Optional[str]:\n    \"\"\"Get prompts that are already cached.\"\"\"\n    model_params_string = str(\n        sorted([(k, v) for k, v in self.model_params.items()])\n    )\n    missing_prompts = []\n    missing_prompt_idxs = []\n    existing_prompts = {}\n    for i, prompt in enumerate(prompts):\n        cache_entry = GenerationCacheEntry(\n            prompt=prompt,\n            model_name=self.model_name,\n            model_params=model_params_string,\n        )\n        cache_val = self.cache.lookup(cache_entry)\n        if cache_val:\n            existing_prompts[i] = cache_val\n        else:\n            missing_prompts.append(prompt)\n            missing_prompt_idxs.append(i)\n    return (\n        existing_prompts,\n        missing_prompt_idxs,\n        missing_prompts,\n    )\n</code></pre>"},{"location":"reference/models/#src.autolabel.models.base.BaseModel.get_num_tokens","title":"<code>get_num_tokens(prompt)</code>  <code>abstractmethod</code>","text":"<p>Get the number of tokens in the prompt</p> Source code in <code>src/autolabel/models/base.py</code> <pre><code>@abstractmethod\ndef get_num_tokens(self, prompt: str) -&gt; int:\n    \"\"\"\n    Get the number of tokens in the prompt\"\"\"\n    pass\n</code></pre>"},{"location":"reference/models/#src.autolabel.models.base.BaseModel.label","title":"<code>label(prompts)</code>","text":"<p>Label a list of prompts.</p> Source code in <code>src/autolabel/models/base.py</code> <pre><code>def label(self, prompts: List[str]) -&gt; RefuelLLMResult:\n    \"\"\"Label a list of prompts.\"\"\"\n    existing_prompts = {}\n    missing_prompt_idxs = list(range(len(prompts)))\n    missing_prompts = prompts\n    costs = []\n    errors = [None for i in range(len(prompts))]\n    latencies = [0 for i in range(len(prompts))]\n    if self.cache:\n        (\n            existing_prompts,\n            missing_prompt_idxs,\n            missing_prompts,\n        ) = self.get_cached_prompts(prompts)\n    # label missing prompts\n    if len(missing_prompts) &gt; 0:\n        new_results = self._label(missing_prompts)\n        for ind, prompt in enumerate(missing_prompts):\n            costs.append(\n                self.get_cost(prompt, label=new_results.generations[ind][0].text)\n            )\n\n        # Set the existing prompts to the new results\n        for i, result, error, latency in zip(\n            missing_prompt_idxs,\n            new_results.generations,\n            new_results.errors,\n            new_results.latencies,\n        ):\n            existing_prompts[i] = result\n            errors[i] = error\n            latencies[i] = latency\n\n        if self.cache:\n            self.update_cache(missing_prompt_idxs, new_results, prompts)\n    generations = [existing_prompts[i] for i in range(len(prompts))]\n    return RefuelLLMResult(\n        generations=generations, costs=costs, errors=errors, latencies=latencies\n    )\n</code></pre>"},{"location":"reference/models/#src.autolabel.models.base.BaseModel.returns_token_probs","title":"<code>returns_token_probs()</code>  <code>abstractmethod</code>","text":"<p>Whether the LLM supports returning logprobs of generated tokens</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>whether the LLM returns supports returning logprobs of generated tokens</p> Source code in <code>src/autolabel/models/base.py</code> <pre><code>@abstractmethod\ndef returns_token_probs(self) -&gt; bool:\n    \"\"\"Whether the LLM supports returning logprobs of generated tokens\n\n    Returns:\n        bool: whether the LLM returns supports returning logprobs of generated tokens\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/models/#src.autolabel.models.base.BaseModel.update_cache","title":"<code>update_cache(missing_prompt_idxs, new_results, prompts)</code>","text":"<p>Update the cache with new results.</p> Source code in <code>src/autolabel/models/base.py</code> <pre><code>def update_cache(self, missing_prompt_idxs, new_results, prompts):\n    \"\"\"Update the cache with new results.\"\"\"\n    model_params_string = str(\n        sorted([(k, v) for k, v in self.model_params.items()])\n    )\n\n    for i, result, error in zip(\n        missing_prompt_idxs, new_results.generations, new_results.errors\n    ):\n        # If there was an error, don't cache the result\n        if error is not None:\n            continue\n\n        cache_entry = GenerationCacheEntry(\n            prompt=prompts[i],\n            model_name=self.model_name,\n            model_params=model_params_string,\n            generations=result,\n            ttl_ms=self.TTL_MS,\n        )\n        self.cache.update(cache_entry)\n</code></pre>"},{"location":"reference/models/#src.autolabel.models.ModelFactory","title":"<code>ModelFactory</code>","text":"<p>The ModelFactory class is used to create a BaseModel object from the given AutoLabelConfig configuration.</p> Source code in <code>src/autolabel/models/__init__.py</code> <pre><code>class ModelFactory:\n    \"\"\"The ModelFactory class is used to create a BaseModel object from the given AutoLabelConfig configuration.\"\"\"\n\n    @staticmethod\n    def from_config(config: AutolabelConfig, cache: BaseCache = None) -&gt; BaseModel:\n        \"\"\"\n        Returns a BaseModel object configured with the settings found in the provided AutolabelConfig.\n        Args:\n            config: AutolabelConfig object containing project settings\n            cache: cache allows for saving results in between labeling runs for future use\n        Returns:\n            model: a fully configured BaseModel object\n        \"\"\"\n        provider = ModelProvider(config.provider())\n        try:\n            model_cls = MODEL_REGISTRY[provider]\n            model_obj = model_cls(config=config, cache=cache)\n            # The below ensures that users should based off of the BaseModel\n            # when creating/registering custom models.\n            assert isinstance(\n                model_obj, BaseModel\n            ), f\"{model_obj} should inherit from autolabel.models.BaseModel\"\n        except KeyError as e:\n            # We should never get here as the config should have already\n            # been validated by the pydantic model.\n            logger.error(\n                f\"{config.provider()} is not in the list of supported providers: \\\n                {list(ModelProvider.__members__.keys())}\"\n            )\n            raise e\n\n        return model_obj\n</code></pre>"},{"location":"reference/models/#src.autolabel.models.ModelFactory.from_config","title":"<code>from_config(config, cache=None)</code>  <code>staticmethod</code>","text":"<p>Returns a BaseModel object configured with the settings found in the provided AutolabelConfig. Args:     config: AutolabelConfig object containing project settings     cache: cache allows for saving results in between labeling runs for future use Returns:     model: a fully configured BaseModel object</p> Source code in <code>src/autolabel/models/__init__.py</code> <pre><code>@staticmethod\ndef from_config(config: AutolabelConfig, cache: BaseCache = None) -&gt; BaseModel:\n    \"\"\"\n    Returns a BaseModel object configured with the settings found in the provided AutolabelConfig.\n    Args:\n        config: AutolabelConfig object containing project settings\n        cache: cache allows for saving results in between labeling runs for future use\n    Returns:\n        model: a fully configured BaseModel object\n    \"\"\"\n    provider = ModelProvider(config.provider())\n    try:\n        model_cls = MODEL_REGISTRY[provider]\n        model_obj = model_cls(config=config, cache=cache)\n        # The below ensures that users should based off of the BaseModel\n        # when creating/registering custom models.\n        assert isinstance(\n            model_obj, BaseModel\n        ), f\"{model_obj} should inherit from autolabel.models.BaseModel\"\n    except KeyError as e:\n        # We should never get here as the config should have already\n        # been validated by the pydantic model.\n        logger.error(\n            f\"{config.provider()} is not in the list of supported providers: \\\n            {list(ModelProvider.__members__.keys())}\"\n        )\n        raise e\n\n    return model_obj\n</code></pre>"},{"location":"reference/models/#src.autolabel.models.register_model","title":"<code>register_model(name, model_cls)</code>","text":"<p>Register Model class</p> Source code in <code>src/autolabel/models/__init__.py</code> <pre><code>def register_model(name, model_cls):\n    \"\"\"Register Model class\"\"\"\n    MODEL_REGISTRY[name] = model_cls\n</code></pre>"},{"location":"reference/schema/","title":"Schema","text":""},{"location":"reference/schema/#src.autolabel.schema.AggregationFunction","title":"<code>AggregationFunction</code>","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Enum of supported aggregation functions</p> Source code in <code>src/autolabel/schema.py</code> <pre><code>class AggregationFunction(str, Enum):\n    \"\"\"Enum of supported aggregation functions\"\"\"\n\n    MAX = \"max\"\n    MEAN = \"mean\"\n</code></pre>"},{"location":"reference/schema/#src.autolabel.schema.ConfidenceCacheEntry","title":"<code>ConfidenceCacheEntry</code>","text":"<p>             Bases: <code>BaseModel</code></p> Source code in <code>src/autolabel/schema.py</code> <pre><code>class ConfidenceCacheEntry(BaseModel):\n    prompt: Optional[str] = \"\"\n    raw_response: Optional[str] = \"\"\n    logprobs: Optional[list] = None\n    score_type: Optional[str] = \"logprob_average\"\n    creation_time_ms: Optional[int] = -1\n    ttl_ms: Optional[int] = -1\n\n    class Config:\n        orm_mode = True\n\n    def get_id(self) -&gt; str:\n        \"\"\"\n        Generates a unique ID for the given confidence cache configuration\n        \"\"\"\n        return calculate_md5([self.prompt, self.raw_response, self.score_type])\n\n    def get_serialized_output(self) -&gt; str:\n        \"\"\"\n        Returns the serialized cache entry output\n        \"\"\"\n        return json.dumps(self.logprobs)\n\n    def deserialize_output(self, output: str) -&gt; Dict[str, float]:\n        \"\"\"\n        Deserializes the cache entry output\n        \"\"\"\n        return json.loads(output)\n</code></pre>"},{"location":"reference/schema/#src.autolabel.schema.ConfidenceCacheEntry.deserialize_output","title":"<code>deserialize_output(output)</code>","text":"<p>Deserializes the cache entry output</p> Source code in <code>src/autolabel/schema.py</code> <pre><code>def deserialize_output(self, output: str) -&gt; Dict[str, float]:\n    \"\"\"\n    Deserializes the cache entry output\n    \"\"\"\n    return json.loads(output)\n</code></pre>"},{"location":"reference/schema/#src.autolabel.schema.ConfidenceCacheEntry.get_id","title":"<code>get_id()</code>","text":"<p>Generates a unique ID for the given confidence cache configuration</p> Source code in <code>src/autolabel/schema.py</code> <pre><code>def get_id(self) -&gt; str:\n    \"\"\"\n    Generates a unique ID for the given confidence cache configuration\n    \"\"\"\n    return calculate_md5([self.prompt, self.raw_response, self.score_type])\n</code></pre>"},{"location":"reference/schema/#src.autolabel.schema.ConfidenceCacheEntry.get_serialized_output","title":"<code>get_serialized_output()</code>","text":"<p>Returns the serialized cache entry output</p> Source code in <code>src/autolabel/schema.py</code> <pre><code>def get_serialized_output(self) -&gt; str:\n    \"\"\"\n    Returns the serialized cache entry output\n    \"\"\"\n    return json.dumps(self.logprobs)\n</code></pre>"},{"location":"reference/schema/#src.autolabel.schema.Dataset","title":"<code>Dataset</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Contains Dataset parameters, including input file path, indexes for state management (e.g. job batching and retries), and a unique ID</p> Source code in <code>src/autolabel/schema.py</code> <pre><code>class Dataset(BaseModel):\n    \"\"\"Contains Dataset parameters, including input file path, indexes for state management (e.g. job batching and retries), and a unique ID\"\"\"\n\n    id: str\n    input_file: str\n    start_index: int\n    end_index: int\n\n    class Config:\n        orm_mode = True\n\n    @classmethod\n    def create_id(\n        self,\n        dataset: Union[str, pd.DataFrame],\n        config: AutolabelConfig,\n        start_index: int,\n        max_items: int,\n    ) -&gt; str:\n        \"\"\"\n        Generates a unique ID for the given Dataset configuration\n        Args:\n            dataset: either 1) input file name or 2) pandas Dataframe\n            config:  AutolabelConfig object containing project settings\n            start_index: index to begin labeling job at (used for job batching, retries, state management)\n            max_items: number of data points to label, beginning at start_index\n\n        Returns:\n            filehash: a unique ID generated from an MD5 hash of the functions parameters\n        \"\"\"\n        if isinstance(dataset, str):\n            filehash = calculate_md5(\n                [open(dataset, \"rb\"), config._dataset_config, start_index, max_items]\n            )\n        else:\n            filehash = calculate_md5(\n                [dataset.to_csv(), config._dataset_config, start_index, max_items]\n            )\n        return filehash\n</code></pre>"},{"location":"reference/schema/#src.autolabel.schema.Dataset.create_id","title":"<code>create_id(dataset, config, start_index, max_items)</code>  <code>classmethod</code>","text":"<p>Generates a unique ID for the given Dataset configuration Args:     dataset: either 1) input file name or 2) pandas Dataframe     config:  AutolabelConfig object containing project settings     start_index: index to begin labeling job at (used for job batching, retries, state management)     max_items: number of data points to label, beginning at start_index</p> <p>Returns:</p> Name Type Description <code>filehash</code> <code>str</code> <p>a unique ID generated from an MD5 hash of the functions parameters</p> Source code in <code>src/autolabel/schema.py</code> <pre><code>@classmethod\ndef create_id(\n    self,\n    dataset: Union[str, pd.DataFrame],\n    config: AutolabelConfig,\n    start_index: int,\n    max_items: int,\n) -&gt; str:\n    \"\"\"\n    Generates a unique ID for the given Dataset configuration\n    Args:\n        dataset: either 1) input file name or 2) pandas Dataframe\n        config:  AutolabelConfig object containing project settings\n        start_index: index to begin labeling job at (used for job batching, retries, state management)\n        max_items: number of data points to label, beginning at start_index\n\n    Returns:\n        filehash: a unique ID generated from an MD5 hash of the functions parameters\n    \"\"\"\n    if isinstance(dataset, str):\n        filehash = calculate_md5(\n            [open(dataset, \"rb\"), config._dataset_config, start_index, max_items]\n        )\n    else:\n        filehash = calculate_md5(\n            [dataset.to_csv(), config._dataset_config, start_index, max_items]\n        )\n    return filehash\n</code></pre>"},{"location":"reference/schema/#src.autolabel.schema.ErrorType","title":"<code>ErrorType</code>","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Enum of supported error types</p> Source code in <code>src/autolabel/schema.py</code> <pre><code>class ErrorType(str, Enum):\n    \"\"\"Enum of supported error types\"\"\"\n\n    LLM_PROVIDER_ERROR = \"llm_provider_error\"\n    PARSING_ERROR = \"parsing_error\"\n    OUTPUT_GUIDELINES_NOT_FOLLOWED_ERROR = \"output_guidelines_not_followed_error\"\n    EMPTY_RESPONSE_ERROR = \"empty_response_error\"\n</code></pre>"},{"location":"reference/schema/#src.autolabel.schema.FewShotAlgorithm","title":"<code>FewShotAlgorithm</code>","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Enum of supported algorithms for choosing which examples to provide the LLM in its instruction prompt</p> Source code in <code>src/autolabel/schema.py</code> <pre><code>class FewShotAlgorithm(str, Enum):\n    \"\"\"Enum of supported algorithms for choosing which examples to provide the LLM in its instruction prompt\"\"\"\n\n    FIXED = \"fixed\"\n    SEMANTIC_SIMILARITY = \"semantic_similarity\"\n    MAX_MARGINAL_RELEVANCE = \"max_marginal_relevance\"\n    LABEL_DIVERSITY_RANDOM = \"label_diversity_random\"\n    LABEL_DIVERSITY_SIMILARITY = \"label_diversity_similarity\"\n</code></pre>"},{"location":"reference/schema/#src.autolabel.schema.GenerationCacheEntry","title":"<code>GenerationCacheEntry</code>","text":"<p>             Bases: <code>BaseModel</code></p> Source code in <code>src/autolabel/schema.py</code> <pre><code>class GenerationCacheEntry(BaseModel):\n    model_name: str\n    prompt: str\n    model_params: str\n    generations: Optional[List[Union[Generation, ChatGeneration]]] = None\n    creation_time_ms: Optional[int] = -1\n    ttl_ms: Optional[int] = -1\n\n    class Config:\n        orm_mode = True\n\n    def get_id(self) -&gt; str:\n        \"\"\"\n        Generates a unique ID for the given generation cache configuration\n        \"\"\"\n        return calculate_md5([self.model_name, self.model_params, self.prompt])\n\n    def get_serialized_output(self) -&gt; str:\n        \"\"\"\n        Returns the serialized cache entry output\n        \"\"\"\n        return json.dumps([gen.dict() for gen in self.generations])\n\n    def deserialize_output(\n        self, output: str\n    ) -&gt; List[Union[Generation, ChatGeneration]]:\n        \"\"\"\n        Deserializes the cache entry output\n        \"\"\"\n        generations = [\n            Generation(**gen) if gen[\"type\"] == \"Generation\" else ChatGeneration(**gen)\n            for gen in json.loads(output)\n        ]\n        return generations\n</code></pre>"},{"location":"reference/schema/#src.autolabel.schema.GenerationCacheEntry.deserialize_output","title":"<code>deserialize_output(output)</code>","text":"<p>Deserializes the cache entry output</p> Source code in <code>src/autolabel/schema.py</code> <pre><code>def deserialize_output(\n    self, output: str\n) -&gt; List[Union[Generation, ChatGeneration]]:\n    \"\"\"\n    Deserializes the cache entry output\n    \"\"\"\n    generations = [\n        Generation(**gen) if gen[\"type\"] == \"Generation\" else ChatGeneration(**gen)\n        for gen in json.loads(output)\n    ]\n    return generations\n</code></pre>"},{"location":"reference/schema/#src.autolabel.schema.GenerationCacheEntry.get_id","title":"<code>get_id()</code>","text":"<p>Generates a unique ID for the given generation cache configuration</p> Source code in <code>src/autolabel/schema.py</code> <pre><code>def get_id(self) -&gt; str:\n    \"\"\"\n    Generates a unique ID for the given generation cache configuration\n    \"\"\"\n    return calculate_md5([self.model_name, self.model_params, self.prompt])\n</code></pre>"},{"location":"reference/schema/#src.autolabel.schema.GenerationCacheEntry.get_serialized_output","title":"<code>get_serialized_output()</code>","text":"<p>Returns the serialized cache entry output</p> Source code in <code>src/autolabel/schema.py</code> <pre><code>def get_serialized_output(self) -&gt; str:\n    \"\"\"\n    Returns the serialized cache entry output\n    \"\"\"\n    return json.dumps([gen.dict() for gen in self.generations])\n</code></pre>"},{"location":"reference/schema/#src.autolabel.schema.LLMAnnotation","title":"<code>LLMAnnotation</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Contains label information of a given data point, including the generated label, the prompt given to the LLM, and the LLMs response. Optionally includes a confidence_score if supported by the model</p> Source code in <code>src/autolabel/schema.py</code> <pre><code>class LLMAnnotation(BaseModel):\n    \"\"\"Contains label information of a given data point, including the generated label, the prompt given to the LLM, and the LLMs response. Optionally includes a confidence_score if supported by the model\"\"\"\n\n    successfully_labeled: bool\n    label: Any\n    curr_sample: Optional[bytes] = \"\"\n    confidence_score: Optional[float] = None\n    generation_info: Optional[Dict[str, Any]] = None\n    raw_response: Optional[str] = \"\"\n    explanation: Optional[str] = \"\"\n    prompt: Optional[str] = \"\"\n    confidence_prompt: Optional[str] = \"\"\n    input_tokens: Optional[int] = None\n    output_tokens: Optional[int] = None\n    cost: Optional[float] = None\n    latency: Optional[float] = None\n    error: Optional[LabelingError] = None\n</code></pre>"},{"location":"reference/schema/#src.autolabel.schema.LabelingError","title":"<code>LabelingError</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Contains information about an error that occurred during the labeling process</p> Source code in <code>src/autolabel/schema.py</code> <pre><code>class LabelingError(BaseModel):\n    \"\"\"Contains information about an error that occurred during the labeling process\"\"\"\n\n    error_type: ErrorType\n    error_message: str\n</code></pre>"},{"location":"reference/schema/#src.autolabel.schema.MetricResult","title":"<code>MetricResult</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Contains performance metrics gathered from autolabeler runs</p> Source code in <code>src/autolabel/schema.py</code> <pre><code>class MetricResult(BaseModel):\n    \"\"\"Contains performance metrics gathered from autolabeler runs\"\"\"\n\n    name: str\n    value: Any\n    show_running: Optional[bool] = True\n</code></pre>"},{"location":"reference/schema/#src.autolabel.schema.MetricType","title":"<code>MetricType</code>","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Enum of supported performance metrics. Some metrics are always available (task agnostic), while others are only supported by certain types of tasks</p> Source code in <code>src/autolabel/schema.py</code> <pre><code>class MetricType(str, Enum):\n    \"\"\"Enum of supported performance metrics. Some metrics are always available (task agnostic), while others are only supported by certain types of tasks\"\"\"\n\n    # Task agnostic\n    SUPPORT = \"support\"\n    COMPLETION_RATE = \"completion_rate\"\n    # Classification metrics\n    ACCURACY = \"accuracy\"\n    CONFUSION_MATRIX = \"confusion_matrix\"\n    LABEL_DISTRIBUTION = \"label_distribution\"\n    F1 = \"f1\"\n    F1_MICRO = \"f1_micro\"\n    F1_MACRO = \"f1_macro\"\n    F1_WEIGHTED = \"f1_weighted\"\n    TEXT_PARTIAL_MATCH = \"text_partial_match\"\n    # Token Classification metrics\n    F1_EXACT = \"f1_exact\"\n    F1_STRICT = \"f1_strict\"\n    F1_PARTIAL = \"f1_partial\"\n    F1_ENT_TYPE = \"f1_ent_type\"\n    # Confidence metrics\n    AUROC = \"auroc\"\n    THRESHOLD = \"threshold\"\n\n    # Aggregate Metrics\n    CLASSIFICATION_REPORT = \"classification_report\"\n</code></pre>"},{"location":"reference/schema/#src.autolabel.schema.ModelProvider","title":"<code>ModelProvider</code>","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Enum containing all LLM providers currently supported by autolabeler</p> Source code in <code>src/autolabel/schema.py</code> <pre><code>class ModelProvider(str, Enum):\n    \"\"\"Enum containing all LLM providers currently supported by autolabeler\"\"\"\n\n    OPENAI = \"openai\"\n    OPENAI_VISION = \"openai_vision\"\n    ANTHROPIC = \"anthropic\"\n    HUGGINGFACE_PIPELINE = \"huggingface_pipeline\"\n    HUGGINGFACE_PIPELINE_VISION = \"huggingface_pipeline_vision\"\n    REFUEL = \"refuel\"\n    GOOGLE = \"google\"\n    COHERE = \"cohere\"\n    CUSTOM = \"custom\"\n</code></pre>"},{"location":"reference/schema/#src.autolabel.schema.RefuelLLMResult","title":"<code>RefuelLLMResult</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>List of generated outputs. This is a List[List[]] because each input could have multiple candidate generations.</p> Source code in <code>src/autolabel/schema.py</code> <pre><code>class RefuelLLMResult(BaseModel):\n    \"\"\"List of generated outputs. This is a List[List[]] because\n    each input could have multiple candidate generations.\"\"\"\n\n    generations: List[List[Union[Generation, ChatGeneration]]]\n\n    \"\"\"Errors encountered while running the labeling job\"\"\"\n    errors: List[Optional[LabelingError]]\n\n    \"\"\"Costs incurred during the labeling job\"\"\"\n    costs: Optional[List[float]] = []\n\n    \"\"\"Latencies incurred during the labeling job\"\"\"\n    latencies: Optional[List[float]] = []\n</code></pre>"},{"location":"reference/schema/#src.autolabel.schema.RefuelLLMResult.costs","title":"<code>costs: Optional[List[float]] = []</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Latencies incurred during the labeling job</p>"},{"location":"reference/schema/#src.autolabel.schema.RefuelLLMResult.errors","title":"<code>errors: List[Optional[LabelingError]]</code>  <code>instance-attribute</code>","text":"<p>Costs incurred during the labeling job</p>"},{"location":"reference/schema/#src.autolabel.schema.RefuelLLMResult.generations","title":"<code>generations: List[List[Union[Generation, ChatGeneration]]]</code>  <code>instance-attribute</code>","text":"<p>Errors encountered while running the labeling job</p>"},{"location":"reference/schema/#src.autolabel.schema.TaskType","title":"<code>TaskType</code>","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Enum containing all the types of tasks that autolabeler currently supports</p> Source code in <code>src/autolabel/schema.py</code> <pre><code>class TaskType(str, Enum):\n    \"\"\"Enum containing all the types of tasks that autolabeler currently supports\"\"\"\n\n    CLASSIFICATION = \"classification\"\n    NAMED_ENTITY_RECOGNITION = \"named_entity_recognition\"\n    QUESTION_ANSWERING = \"question_answering\"\n    ENTITY_MATCHING = \"entity_matching\"\n    MULTILABEL_CLASSIFICATION = \"multilabel_classification\"\n    ATTRIBUTE_EXTRACTION = \"attribute_extraction\"\n</code></pre>"},{"location":"reference/tasks/","title":"Tasks","text":"<p>             Bases: <code>ABC</code></p> Source code in <code>src/autolabel/tasks/base.py</code> <pre><code>class BaseTask(ABC):\n    ZERO_SHOT_TEMPLATE = \"{task_guidelines}\\n\\n{output_guidelines}\\n\\nNow I want you to label the following example:\\n{current_example}\"\n    FEW_SHOT_TEMPLATE = \"{task_guidelines}\\n\\n{output_guidelines}\\n\\nSome examples with their output answers are provided below:\\n\\n{seed_examples}\\n\\nNow I want you to label the following example:\\n{current_example}\"\n\n    ZERO_SHOT_TEMPLATE_REFUEL_LLM = \"\"\"\n    &lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\n    {task_guidelines}\\n{output_guidelines}\n    &lt;&lt;/SYS&gt;&gt;\n    {current_example}[/INST]\n    \"\"\"\n    FEW_SHOT_TEMPLATE_REFUEL_LLM = \"\"\"\n    &lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\n    {task_guidelines}\\n{output_guidelines}\\n{seed_examples}\n    &lt;&lt;/SYS&gt;&gt;\n    {current_example}[/INST]\n    \"\"\"\n\n    # Downstream classes should override these\n    NULL_LABEL_TOKEN = \"NO_LABEL\"\n    DEFAULT_TASK_GUIDELINES = \"\"\n    DEFAULT_OUTPUT_GUIDELINES = \"\"\n    DEFAULT_DATASET_GENERATION_GUIDELINES = \"\"\n\n    def __init__(self, config: AutolabelConfig) -&gt; None:\n        self.config = config\n        self.image_col = self.config.image_column()\n\n        # Update the default prompt template with the prompt template from the config\n        self.task_guidelines = (\n            self.config.task_guidelines() or self.DEFAULT_TASK_GUIDELINES\n        )\n        self.output_guidelines = (\n            self.config.output_guidelines() or self.DEFAULT_OUTPUT_GUIDELINES\n        )\n\n        self.dataset_generation_guidelines = (\n            self.config.dataset_generation_guidelines()\n            or self.DEFAULT_DATASET_GENERATION_GUIDELINES\n        )\n        self._prompt_schema_init()\n\n    def _prompt_schema_init(self) -&gt; None:\n        self.use_refuel_prompt_schema = self.config.provider() == ModelProvider.REFUEL\n        if self._is_few_shot_mode():\n            self.example_template = (\n                self.FEW_SHOT_TEMPLATE_REFUEL_LLM\n                if self.use_refuel_prompt_schema\n                else self.FEW_SHOT_TEMPLATE\n            )\n        else:\n            self.example_template = (\n                self.ZERO_SHOT_TEMPLATE_REFUEL_LLM\n                if self.use_refuel_prompt_schema\n                else self.ZERO_SHOT_TEMPLATE\n            )\n        self.prompt_template = PromptTemplate(\n            input_variables=get_format_variables(self.example_template),\n            template=self.example_template,\n        )\n\n    def _is_few_shot_mode(self) -&gt; bool:\n        return self.config.few_shot_algorithm() in [x.value for x in FewShotAlgorithm]\n\n    @abstractmethod\n    def construct_prompt(\n        self,\n        input: str,\n        examples: List,\n        prompt_template_override: PromptTemplate = None,\n        refuel_prompt_override: bool = False,\n        output_guidelines_override: str = None,\n        max_input_tokens: int = None,\n        get_num_tokens: Optional[Callable] = None,\n        **kwargs,\n    ) -&gt; str:\n        pass\n\n    def construct_confidence_prompt(self, input: str, examples: List, **kwargs) -&gt; str:\n        curr_template = (\n            self.FEW_SHOT_TEMPLATE_REFUEL_LLM\n            if self._is_few_shot_mode()\n            else self.ZERO_SHOT_TEMPLATE_REFUEL_LLM\n        )\n        prompt_template = PromptTemplate(\n            input_variables=get_format_variables(curr_template),\n            template=curr_template,\n        )\n        refuel_prompt = self.construct_prompt(\n            input=input,\n            examples=examples,\n            prompt_template_override=prompt_template,\n            refuel_prompt_override=True,\n            **kwargs,\n        )\n        return refuel_prompt\n\n    def trim_prompt(\n        self,\n        prompt_template: str,\n        task_guidelines: str,\n        output_guidelines: str,\n        current_example: str,\n        seed_examples: str = None,\n        max_input_tokens: int = None,\n        get_num_tokens: Optional[Callable] = None,\n    ) -&gt; str:\n        complete_prompt = prompt_template.format(\n            task_guidelines=task_guidelines,\n            output_guidelines=output_guidelines,\n            seed_examples=seed_examples,\n            current_example=current_example,\n        )\n        if not max_input_tokens or not get_num_tokens:\n            return complete_prompt\n\n        trimming_priority = [\n            seed_examples,\n            task_guidelines,\n            output_guidelines,\n            current_example,\n        ]\n        trimmed_elements = {key: key for key in trimming_priority if key is not None}\n        for trimming_candidate in trimming_priority:\n            current_prompt_length = get_num_tokens(complete_prompt)\n            if current_prompt_length &lt;= max_input_tokens:\n                break\n            if trimming_candidate is None:\n                continue\n            extra_tokens = current_prompt_length - max_input_tokens\n            trimming_candidate_tokens = get_num_tokens(trimming_candidate)\n            max_chars = (\n                float(len(trimming_candidate))\n                * (trimming_candidate_tokens - extra_tokens - 1)\n                / (trimming_candidate_tokens + 1)\n            )\n            final_candidate_chars = int(max(0, max_chars))\n            trimmed_elements[trimming_candidate] = trimming_candidate[\n                :final_candidate_chars\n            ]\n            complete_prompt = prompt_template.format(\n                task_guidelines=trimmed_elements[task_guidelines],\n                output_guidelines=trimmed_elements[output_guidelines],\n                seed_examples=trimmed_elements[seed_examples]\n                if seed_examples is not None\n                else None,\n                current_example=trimmed_elements[current_example],\n            )\n\n        return complete_prompt\n\n    @abstractmethod\n    def eval(\n        self,\n        llm_labels: List,\n        gt_labels: List,\n        additional_metrics: Optional[List[BaseMetric]] = [],\n    ) -&gt; List[MetricResult]:\n        pass\n\n    @abstractmethod\n    def get_explanation_prompt(self, example: Dict, include_label=True) -&gt; str:\n        raise NotImplementedError(\n            \"Explanation generation not implemented for this task\"\n        )\n\n    @abstractmethod\n    def get_generate_dataset_prompt(\n        self, label: str, num_rows: int, guidelines: str = None\n    ) -&gt; str:\n        raise NotImplementedError(\"Dataset generation not implemented for this task\")\n\n    def parse_llm_response(\n        self,\n        response: Union[Generation, ChatGeneration],\n        curr_sample: Dict,\n        prompt: str,\n    ) -&gt; LLMAnnotation:\n        # The last line of the response is the label\n        # This is done to handle the case where the model generates an explanation before generating the label\n        error = None\n        if self.config.chain_of_thought():\n            try:\n                explanation = response.text.strip().split(\"\\n\")[0].strip()\n                completion_text = extract_valid_json_substring(\n                    response.text.strip().split(\"\\n\")[-1].strip()\n                )\n                completion_text = json.loads(completion_text)[\"label\"]\n            except:\n                completion_text = None\n        else:\n            completion_text = response.text.strip().split(\"\\n\")[-1].strip()\n        if len(response.text.strip()) == 0:\n            successfully_labeled = False\n            llm_label = self.NULL_LABEL_TOKEN\n            logger.warning(\"LLM response is empty\")\n            error = LabelingError(\n                error_type=ErrorType.EMPTY_RESPONSE_ERROR,\n                error_message=\"Empty response from LLM\",\n            )\n        elif not completion_text:\n            successfully_labeled = False\n            llm_label = self.NULL_LABEL_TOKEN\n            logger.warning(f\"Error parsing LLM response: {response.text}\")\n            error = LabelingError(\n                error_type=ErrorType.PARSING_ERROR,\n                error_message=f\"Error parsing LLM response: {response.text}\",\n            )\n        else:\n            llm_label = completion_text.strip()\n            if self.config.task_type() in [\n                TaskType.CLASSIFICATION,\n                TaskType.ENTITY_MATCHING,\n            ]:\n                if llm_label in self.config.labels_list():\n                    successfully_labeled = True\n                else:\n                    logger.warning(f\"LLM response is not in the labels list\")\n                    llm_label = self.NULL_LABEL_TOKEN\n                    successfully_labeled = False\n                    error = LabelingError(\n                        error_type=ErrorType.OUTPUT_GUIDELINES_NOT_FOLLOWED_ERROR,\n                        error_message=f\"LLM response is not in the labels list: {llm_label}\",\n                    )\n            elif self.config.task_type() == TaskType.MULTILABEL_CLASSIFICATION:\n                llm_multi_labels = llm_label.split(self.config.label_separator())\n                llm_multi_labels = list(\n                    filter(\n                        lambda label: label in self.config.labels_list(),\n                        llm_multi_labels,\n                    )\n                )\n                if len(llm_multi_labels) == 0:\n                    llm_label = self.NULL_LABEL_TOKEN\n                    successfully_labeled = False\n                    error = LabelingError(\n                        error_type=ErrorType.OUTPUT_GUIDELINES_NOT_FOLLOWED_ERROR,\n                        error_message=f\"LLM response is not in the labels list: {llm_label}\",\n                    )\n                else:\n                    llm_label = self.config.label_separator().join(llm_multi_labels)\n                    successfully_labeled = True\n            else:\n                successfully_labeled = True\n\n        return LLMAnnotation(\n            successfully_labeled=successfully_labeled,\n            label=llm_label,\n            generation_info=response.generation_info,\n            raw_response=response.text,\n            prompt=prompt,\n            curr_sample=pickle.dumps(curr_sample),\n            explanation=explanation if self.config.chain_of_thought() else \"\",\n            error=error,\n        )\n</code></pre> <p>             Bases: <code>BaseTask</code></p> Source code in <code>src/autolabel/tasks/classification.py</code> <pre><code>class ClassificationTask(BaseTask):\n    DEFAULT_OUTPUT_GUIDELINES = (\n        'You will return the answer with just one element: \"the correct label\"'\n    )\n    DEFAULT_TASK_GUIDELINES = \"Your job is to correctly label the provided input example into one of the following {num_labels} categories.\\nCategories:\\n{labels}\\n\"\n\n    LABEL_FORMAT_IN_EXPLANATION = (\n        \" The last line of the explanation should be - So, the answer is &lt;label&gt;.\"\n    )\n    EXCLUDE_LABEL_IN_EXPLANATION = \" Do not repeat the output of the task - simply provide an explanation for the provided output. The provided label was generated by you in a previous step and your job now is to only provided an explanation for the output. Your job is not verify the output but instead explain why it might have been generated, even if it is incorrect. If you think the provided output is incorrect, give an explanation of why it might have been generated anyway but don't say that the output may be incorrect or incorrectly generated.'\"\n    GENERATE_EXPLANATION_PROMPT = \"Your job is to provide an explanation for why a specific output might have been generated for a task. \\n\\nBEGIN TASK DESCRIPTION\\n{task_guidelines}\\nEND TASK DESCRIPTION\\nYou will be given an input example and the corresponding output. Your job is to provide an explanation for why the output is correct for the task above.\\nThink step by step and generate an explanation with at most 2 sentences.{label_format}\\n{labeled_example}\\nExplanation: \"\n\n    GENERATE_DATASET_TEMPLATE = \"{guidelines}\\n\\nThe inputs must be diverse, covering a wide range of scenarios. You will not generate duplicate inputs. These inputs should be organized in rows in csv format with the columns {columns}.\\n\\n{label_descriptions}\\n\\n{format_guidelines}\\n\\n{output_guidelines}\\n\\n```csv\"\n    DEFAULT_DATASET_GENERATION_GUIDELINES = \"You are an expert at generating plausible inputs for a given task.\\n\\nBEGIN TASK DESCRIPTION\\n{task_guidelines}\\nEND TASK DESCRIPTION\"\n    LABEL_DESCRIPTIONS_PROMPT = \"Each input should fall into one of these {num_labels} categories. These are the only categories that the inputs can belong to.\"\n    GENERATE_DATASET_FORMAT_GUIDELINES = \"Your response should be in csv format with the following columns: {columns}.\\n\\nHere is a template you can follow for your output:\\n```csv\\n{columns}\\n{example_rows}\\n```\\n\\nMake sure to replace the placeholder variables with your own values.\"\n    GENERATE_DATASET_OUTPUT_GUIDELINES = 'Now I want you to generate {num_rows} excerpts that follow the guidelines and all belong to the \"{label}\" category. They should not belong to any of the other categories.'\n\n    def __init__(self, config: AutolabelConfig) -&gt; None:\n        super().__init__(config)\n        self.metrics = [\n            AccuracyMetric(),\n            SupportMetric(),\n            CompletionRateMetric(),\n            ClassificationReportMetric(),\n        ]\n\n        if self.config.confidence():\n            self.metrics.append(AUROCMetric())\n\n        for label in self.config.labels_list():\n            if \"\\n\" in label:\n                logger.warning(\n                    \"Label contains newline character. This can have output guideline issues.\"\n                )\n\n    def construct_prompt(\n        self,\n        input: Dict,\n        examples: List,\n        selected_labels: List[str] = None,\n        prompt_template_override: PromptTemplate = None,\n        refuel_prompt_override: bool = False,\n        output_guidelines_override: str = None,\n        max_input_tokens: int = None,\n        get_num_tokens: Optional[Callable] = None,\n        **kwargs,\n    ) -&gt; str:\n        # Copy over the input so that we can modify it\n        input = input.copy()\n\n        # prepare task guideline\n        labels_list = (\n            self.config.labels_list() if not selected_labels else selected_labels\n        )\n        num_labels = len(labels_list)\n\n        fmt_task_guidelines = self.task_guidelines.format_map(\n            defaultdict(str, labels=\"\\n\".join(labels_list), num_labels=num_labels)\n        )\n\n        # prepare seed examples\n        example_template = self.config.example_template()\n        label_column = self.config.label_column()\n        fmt_examples = []\n        for eg in examples:\n            eg_copy = eg.copy()\n            # If chain of thought is enabled\n            if label_column and self.config.chain_of_thought():\n                eg_copy[label_column] = json.dumps({\"label\": eg[label_column]})\n            fmt_examples.append(example_template.format_map(defaultdict(str, eg_copy)))\n\n        # populate the current example in the prompt\n        if label_column:\n            input[label_column] = \"\"\n\n        # populate the explanation column with empty string for current example\n        explanation_column = self.config.explanation_column()\n        if explanation_column:\n            input[explanation_column] = \"\"\n\n        # check if all mapped keys in input are in the example template\n        try:\n            current_example = example_template.format(**input)\n        except KeyError as e:\n            current_example = example_template.format_map(defaultdict(str, input))\n            logger.warn(\n                f'\\n\\nKey {e} in the \"example_template\" in the given config'\n                f\"\\n\\n{example_template}\\n\\nis not present in the datsaset columns - {input.keys()}.\\n\\n\"\n                f\"Input - {input}\\n\\n\"\n                \"Continuing with the prompt as {current_example}\"\n            )\n\n        # populate the current example in the prompt\n        prompt_template = (\n            self.prompt_template\n            if prompt_template_override is None\n            else prompt_template_override\n        )\n        output_guidelines = (\n            self.output_guidelines\n            if output_guidelines_override is None\n            else output_guidelines_override\n        )\n        if self._is_few_shot_mode():\n            curr_text_prompt = self.trim_prompt(\n                prompt_template,\n                task_guidelines=fmt_task_guidelines,\n                output_guidelines=output_guidelines,\n                seed_examples=\"\\n\\n\".join(fmt_examples),\n                current_example=current_example,\n                max_input_tokens=max_input_tokens,\n                get_num_tokens=get_num_tokens,\n            )\n        else:\n            curr_text_prompt = self.trim_prompt(\n                prompt_template,\n                task_guidelines=fmt_task_guidelines,\n                output_guidelines=output_guidelines,\n                current_example=current_example,\n                max_input_tokens=max_input_tokens,\n                get_num_tokens=get_num_tokens,\n            )\n        if self.image_col is not None:\n            return json.dumps(\n                {\"text\": curr_text_prompt, \"image_url\": input[self.image_col]}\n            )\n        else:\n            return curr_text_prompt\n\n    def get_explanation_prompt(self, example: Dict, include_label=True) -&gt; str:\n        pt = PromptTemplate(\n            input_variables=get_format_variables(self.GENERATE_EXPLANATION_PROMPT),\n            template=self.GENERATE_EXPLANATION_PROMPT,\n        )\n\n        # prepare task guideline\n        labels_list = self.config.labels_list()\n        num_labels = len(labels_list)\n        fmt_task_guidelines = self.task_guidelines.format(\n            num_labels=num_labels, labels=\"\\n\".join(labels_list)\n        )\n\n        # prepare labeled example\n        example_template = self.config.example_template()\n        fmt_example = example_template.format_map(defaultdict(str, example))\n\n        return pt.format(\n            task_guidelines=fmt_task_guidelines,\n            label_format=self.LABEL_FORMAT_IN_EXPLANATION\n            if include_label\n            else self.EXCLUDE_LABEL_IN_EXPLANATION,\n            labeled_example=fmt_example,\n        )\n\n    def get_generate_dataset_prompt(self, label: str) -&gt; str:\n        pt = PromptTemplate(\n            input_variables=get_format_variables(self.GENERATE_DATASET_TEMPLATE),\n            template=self.GENERATE_DATASET_TEMPLATE,\n        )\n\n        # prepare task guideline\n        labels_list = self.config.labels_list()\n        num_labels = len(labels_list)\n        fmt_task_guidelines = self.task_guidelines.format(\n            num_labels=num_labels, labels=\"\\n\".join(labels_list)\n        )\n        fmt_guidelines = self.dataset_generation_guidelines.format(\n            task_guidelines=fmt_task_guidelines\n        )\n\n        # prepare columns\n        columns = get_format_variables(self.config.example_template())\n        columns.remove(self.config.label_column())\n\n        # prepare label descriptions\n        fmt_label_descriptions = self.LABEL_DESCRIPTIONS_PROMPT.format(\n            num_labels=num_labels\n        )\n        for i, l in enumerate(labels_list):\n            fmt_label_descriptions += f\"\\n{i+1}. {l}{': ' + self.config.label_descriptions()[l] if self.config.label_descriptions() is not None and l in self.config.label_descriptions() else ''}\"\n\n        # prepare format\n        example_rows = \"\\n\".join(\n            [\",\".join([f'\"{column}_{i+1}\"' for column in columns]) for i in range(3)]\n        )\n        fmt_format_guidelines = self.GENERATE_DATASET_FORMAT_GUIDELINES.format(\n            columns=\",\".join(columns), example_rows=example_rows\n        )\n\n        # prepare output guidelines\n        fmt_output_guidelines = self.GENERATE_DATASET_OUTPUT_GUIDELINES.format(\n            num_rows=self.config.dataset_generation_num_rows(), label=label\n        )\n\n        return pt.format(\n            guidelines=fmt_guidelines,\n            columns=columns,\n            label_descriptions=fmt_label_descriptions,\n            format_guidelines=fmt_format_guidelines,\n            output_guidelines=fmt_output_guidelines,\n        )\n\n    def eval(\n        self,\n        llm_labels: List[LLMAnnotation],\n        gt_labels: List[str],\n        additional_metrics: List[BaseMetric] = [],\n    ) -&gt; List[MetricResult]:\n        \"\"\"Evaluate the LLM generated labels by comparing them against ground truth\n\n        Args:\n            llm_labels (List[LLMAnnotation]): _description_\n            gt_labels (List[str]): _description_\n            additional_metrics (List[BaseMetric], optional): The additional metrics to run. Defaults to [].\n\n        Returns:\n            List[MetricResult]: list of metrics and corresponding values\n        \"\"\"\n\n        eval_metrics = []\n\n        for metric in self.metrics + additional_metrics:\n            eval_metrics.extend(metric.compute(llm_labels, gt_labels))\n\n        return eval_metrics\n</code></pre> <p>             Bases: <code>BaseTask</code></p> Source code in <code>src/autolabel/tasks/entity_matching.py</code> <pre><code>class EntityMatchingTask(BaseTask):\n    DEFAULT_OUTPUT_GUIDELINES = (\n        'You will return the answer with one element: \"the correct option\"\\n'\n    )\n    DEFAULT_TASK_GUIDELINES = \"Your job is to tell if the two given entities are duplicates or not. You will return the answer from one of the choices. Choices:\\n{labels}\\n\"\n    LABEL_FORMAT_IN_EXPLANATION = (\n        \" The last line of the explanation should be - So, the answer is &lt;label&gt;.\"\n    )\n    EXCLUDE_LABEL_IN_EXPLANATION = \" Do not repeat the output of the task - simply provide an explanation for the provided output. The provided label was generated by you in a previous step and your job now is to only provided an explanation for the output. Your job is not verify the output but instead explain why it might have been generated, even if it is incorrect. If you think the provided output is incorrect, give an explanation of why it might have been generated anyway but don't say that the output may be incorrect or incorrectly generated.'\"\n    GENERATE_EXPLANATION_PROMPT = \"You are an expert at providing a well reasoned explanation for the output of a given task. \\n\\nBEGIN TASK DESCRIPTION\\n{task_guidelines}\\nEND TASK DESCRIPTION\\nYou will be given an input example and the corresponding output. Your job is to provide an explanation for why the output is correct for the task above.\\nThink step by step and generate an explanation.{label_format}\\n{labeled_example}\\nExplanation: \"\n\n    GENERATE_DATASET_TEMPLATE = \"{guidelines}\\n\\nThe inputs must be diverse, covering a wide range of scenarios. You will not generate duplicate inputs. These inputs should be organized in rows in csv format with the columns {columns}.\\n\\n{label_descriptions}\\n\\n{format_guidelines}\\n\\n{output_guidelines}\\n\\n```csv\"\n    DEFAULT_DATASET_GENERATION_GUIDELINES = \"You are an expert at generating plausible inputs for a given task.\\n\\nBEGIN TASK DESCRIPTION\\n{task_guidelines}\\nEND TASK DESCRIPTION\"\n    LABEL_DESCRIPTIONS_PROMPT = \"Each input should fall into one of these {num_labels} categories. These are the only categories that the inputs can belong to.\"\n    GENERATE_DATASET_FORMAT_GUIDELINES = \"Your response should be in csv format with the following columns: {columns}.\\n\\nHere is a template you can follow for your output:\\n```csv\\n{columns}\\n{example_rows}\\n```\\n\\nMake sure to replace the placeholder variables with your own values.\"\n    GENERATE_DATASET_OUTPUT_GUIDELINES = 'Now I want you to generate {num_rows} excerpts that follow the guidelines and all belong to the \"{label}\" category. They should not belong to any of the other categories.'\n\n    def __init__(self, config: AutolabelConfig) -&gt; None:\n        super().__init__(config)\n        self.metrics = [\n            AccuracyMetric(),\n            SupportMetric(),\n            CompletionRateMetric(),\n            ClassificationReportMetric(),\n        ]\n\n        if self.config.confidence():\n            self.metrics.append(AUROCMetric())\n\n        for label in self.config.labels_list():\n            if \"\\n\" in label:\n                logger.warning(\n                    \"Label contains newline character. This can have output guideline issues.\"\n                )\n\n    def construct_prompt(\n        self,\n        input: Dict,\n        examples: List[Dict],\n        prompt_template_override: PromptTemplate = None,\n        refuel_prompt_override: bool = False,\n        output_guidelines_override: str = None,\n        max_input_tokens: int = None,\n        get_num_tokens: Optional[Callable] = None,\n        **kwargs,\n    ) -&gt; str:\n        # Copy over the input so that we can modify it\n        input = input.copy()\n\n        # prepare task guideline\n        labels_list = self.config.labels_list()\n        num_labels = len(labels_list)\n        fmt_task_guidelines = self.task_guidelines.format_map(\n            defaultdict(str, labels=\"\\n\".join(labels_list), num_labels=num_labels)\n        )\n\n        # prepare seed examples\n        example_template = self.config.example_template()\n        label_column = self.config.label_column()\n        fmt_examples = []\n        for eg in examples:\n            eg_copy = eg.copy()\n            # If chain of thought is enabled\n            if label_column and self.config.chain_of_thought():\n                eg_copy[label_column] = json.dumps({\"label\": eg[label_column]})\n            fmt_examples.append(example_template.format_map(defaultdict(str, eg_copy)))\n\n        # populate the current example in the prompt\n        if label_column:\n            input[label_column] = \"\"\n\n        # populate the explanation column with empty string for current example\n        explanation_column = self.config.explanation_column()\n        if explanation_column:\n            input[explanation_column] = \"\"\n\n        # check if all mapped keys in input are in the example template\n        try:\n            current_example = example_template.format(**input)\n        except KeyError as e:\n            current_example = example_template.format_map(defaultdict(str, input))\n            logger.warn(\n                f'\\n\\nKey {e} in the \"example_template\" in the given config'\n                f\"\\n\\n{example_template}\\n\\nis not present in the datsaset columns - {input.keys()}.\\n\\n\"\n                f\"Input - {input}\\n\\n\"\n                \"Continuing with the prompt as {current_example}\"\n            )\n\n        # populate the current example in the prompt\n        prompt_template = (\n            self.prompt_template\n            if prompt_template_override is None\n            else prompt_template_override\n        )\n        output_guidelines = (\n            self.output_guidelines\n            if output_guidelines_override is None\n            else output_guidelines_override\n        )\n        if self._is_few_shot_mode():\n            curr_text_prompt = self.trim_prompt(\n                prompt_template,\n                task_guidelines=fmt_task_guidelines,\n                output_guidelines=output_guidelines,\n                seed_examples=\"\\n\\n\".join(fmt_examples),\n                current_example=current_example,\n                max_input_tokens=max_input_tokens,\n                get_num_tokens=get_num_tokens,\n            )\n        else:\n            curr_text_prompt = self.trim_prompt(\n                prompt_template,\n                task_guidelines=fmt_task_guidelines,\n                output_guidelines=output_guidelines,\n                current_example=current_example,\n                max_input_tokens=max_input_tokens,\n                get_num_tokens=get_num_tokens,\n            )\n\n        if self.image_col is not None:\n            return json.dumps(\n                {\"text\": curr_text_prompt, \"image_url\": input[self.image_col]}\n            )\n        else:\n            return curr_text_prompt\n\n    def get_explanation_prompt(self, example: Dict, include_label=True) -&gt; str:\n        pt = PromptTemplate(\n            input_variables=get_format_variables(self.GENERATE_EXPLANATION_PROMPT),\n            template=self.GENERATE_EXPLANATION_PROMPT,\n        )\n\n        # prepare task guideline\n        labels_list = self.config.labels_list()\n        num_labels = len(labels_list)\n        fmt_task_guidelines = self.task_guidelines.format(\n            num_labels=num_labels, labels=\"\\n\".join(labels_list)\n        )\n\n        # prepare labeled example\n        example_template = self.config.example_template()\n        fmt_example = example_template.format_map(defaultdict(str, example))\n\n        return pt.format(\n            task_guidelines=fmt_task_guidelines,\n            label_format=self.LABEL_FORMAT_IN_EXPLANATION\n            if include_label\n            else self.EXCLUDE_LABEL_IN_EXPLANATION,\n            labeled_example=fmt_example,\n        )\n\n    def get_generate_dataset_prompt(self, label: str) -&gt; str:\n        pt = PromptTemplate(\n            input_variables=get_format_variables(self.GENERATE_DATASET_TEMPLATE),\n            template=self.GENERATE_DATASET_TEMPLATE,\n        )\n\n        # prepare task guideline\n        labels_list = self.config.labels_list()\n        num_labels = len(labels_list)\n        fmt_task_guidelines = self.task_guidelines.format(\n            num_labels=num_labels, labels=\"\\n\".join(labels_list)\n        )\n        fmt_guidelines = self.dataset_generation_guidelines.format(\n            task_guidelines=fmt_task_guidelines\n        )\n\n        # prepare columns\n        columns = get_format_variables(self.config.example_template())\n        columns.remove(self.config.label_column())\n\n        # prepare label descriptions\n        fmt_label_descriptions = self.LABEL_DESCRIPTIONS_PROMPT.format(\n            num_labels=num_labels\n        )\n\n        for i, l in enumerate(labels_list):\n            fmt_label_descriptions += f\"\\n{i+1}. {l}{': ' + self.config.label_descriptions()[l] if self.config.label_descriptions() is not None and l in self.config.label_descriptions() else ''}\"\n\n        # prepare format\n        example_rows = \"\\n\".join(\n            [\",\".join([f'\"{column}_{i+1}\"' for column in columns]) for i in range(3)]\n        )\n        fmt_format_guidelines = self.GENERATE_DATASET_FORMAT_GUIDELINES.format(\n            columns=\",\".join(columns), example_rows=example_rows\n        )\n\n        # prepare output guidelines\n        fmt_output_guidelines = self.GENERATE_DATASET_OUTPUT_GUIDELINES.format(\n            num_rows=self.config.dataset_generation_num_rows(), label=label\n        )\n\n        return pt.format(\n            guidelines=fmt_guidelines,\n            columns=columns,\n            label_descriptions=fmt_label_descriptions,\n            format_guidelines=fmt_format_guidelines,\n            output_guidelines=fmt_output_guidelines,\n        )\n\n    def eval(\n        self,\n        llm_labels: List[LLMAnnotation],\n        gt_labels: List[str],\n        additional_metrics: List[BaseMetric] = [],\n    ) -&gt; List[MetricResult]:\n        \"\"\"Evaluate the LLM generated labels by comparing them against ground truth\n\n        Args:\n            llm_labels (List[LLMAnnotation]): _description_\n            gt_labels (List[str]): _description_\n            additional_metrics (List[BaseMetric], optional): List of additional metrics to run. Defaults to [].\n\n        Returns:\n            List[MetricResult]: list of metrics and corresponding values\n        \"\"\"\n\n        eval_metrics = []\n\n        for metric in self.metrics + additional_metrics:\n            eval_metrics.extend(metric.compute(llm_labels, gt_labels))\n\n        return eval_metrics\n</code></pre> <p>             Bases: <code>BaseTask</code></p> Source code in <code>src/autolabel/tasks/question_answering.py</code> <pre><code>class QuestionAnsweringTask(BaseTask):\n    DEFAULT_OUTPUT_GUIDELINES = (\n        'You will return the answer one element: \"the correct label\"\\n'\n    )\n    REFUEL_LLM_DEFAULT_OUTPUT_GUIDELINES = \"\"\n    DEFAULT_TASK_GUIDELINES = \"Your job is to answer the following questions using the options provided for each question. Choose the best answer for the question.\\n\"\n    NULL_LABEL_TOKEN = \"NO_LABEL\"\n\n    LABEL_FORMAT_IN_EXPLANATION = (\n        \" The last line of the explanation should be - So, the answer is &lt;label&gt;.\"\n    )\n    EXCLUDE_LABEL_IN_EXPLANATION = \" Do not repeat the output of the task - simply provide an explanation for the provided output. The provided label was generated by you in a previous step and your job now is to only provided an explanation for the output. Your job is not verify the output but instead explain why it might have been generated, even if it is incorrect. If you think the provided output is incorrect, give an explanation of why it might have been generated anyway but don't say that the output may be incorrect or incorrectly generated.'\"\n    GENERATE_EXPLANATION_PROMPT = \"You are an expert at providing a well reasoned explanation for the output of a given task. \\n\\nBEGIN TASK DESCRIPTION\\n{task_guidelines}\\nEND TASK DESCRIPTION\\nYou will be given an input example and the corresponding output. You will be given a question and an answer. Your job is to provide an explanation for why the answer is correct for the task above.\\nThink step by step and generate an explanation.{label_format}\\n{labeled_example}\\nExplanation: \"\n\n    def __init__(self, config: AutolabelConfig) -&gt; None:\n        if config.provider() == ModelProvider.REFUEL:\n            self.DEFAULT_OUTPUT_GUIDELINES = self.REFUEL_LLM_DEFAULT_OUTPUT_GUIDELINES\n\n        super().__init__(config)\n        self.metrics = [\n            AccuracyMetric(),\n            SupportMetric(),\n            CompletionRateMetric(),\n            F1Metric(\n                type=F1Type.TEXT,\n            ),\n        ]\n\n        if self.config.confidence():\n            self.metrics.append(AUROCMetric())\n\n    def construct_prompt(\n        self,\n        input: Dict,\n        examples: List[Dict],\n        prompt_template_override: PromptTemplate = None,\n        refuel_prompt_override: bool = False,\n        output_guidelines_override: str = None,\n        max_input_tokens: int = None,\n        get_num_tokens: Optional[Callable] = None,\n        **kwargs,\n    ) -&gt; str:\n        # Copy over the input so that we can modify it\n        input = input.copy()\n\n        # prepare seed examples\n        example_template = self.config.example_template()\n        label_column = self.config.label_column()\n        fmt_examples = []\n        for eg in examples:\n            eg_copy = eg.copy()\n            # If chain of thought is enabled\n            if label_column and self.config.chain_of_thought():\n                eg_copy[label_column] = json.dumps({\"label\": eg[label_column]})\n            fmt_examples.append(example_template.format_map(defaultdict(str, eg_copy)))\n\n        # populate the current example in the prompt\n        if label_column:\n            input[label_column] = \"\"\n\n        # populate the explanation column with empty string for current example\n        explanation_column = self.config.explanation_column()\n        if explanation_column:\n            input[explanation_column] = \"\"\n\n            # check if all mapped keys in input are in the example template\n        try:\n            current_example = example_template.format(**input)\n        except KeyError as e:\n            current_example = example_template.format_map(defaultdict(str, input))\n            logger.warn(\n                f'\\n\\nKey {e} in the \"example_template\" in the given config'\n                f\"\\n\\n{example_template}\\n\\nis not present in the datsaset columns - {input.keys()}.\\n\\n\"\n                f\"Input - {input}\\n\\n\"\n                \"Continuing with the prompt as {current_example}\"\n            )\n\n        # populate the current example in the prompt\n        prompt_template = (\n            self.prompt_template\n            if prompt_template_override is None\n            else prompt_template_override\n        )\n        output_guidelines = (\n            self.output_guidelines\n            if output_guidelines_override is None\n            else output_guidelines_override\n        )\n        if self._is_few_shot_mode():\n            curr_text_prompt = prompt_template.format(\n                task_guidelines=self.task_guidelines,\n                output_guidelines=output_guidelines,\n                seed_examples=\"\\n\\n\".join(fmt_examples),\n                current_example=current_example,\n            )\n        else:\n            curr_text_prompt = prompt_template.format(\n                task_guidelines=self.task_guidelines,\n                output_guidelines=output_guidelines,\n                current_example=current_example,\n            )\n\n        if self.image_col is not None:\n            return json.dumps(\n                {\"text\": curr_text_prompt, \"image_url\": input[self.image_col]}\n            )\n        else:\n            return curr_text_prompt\n\n    def construct_confidence_prompt(self, input: str, examples: List, **kwargs) -&gt; str:\n        output_guidelines_override = (\n            self.config.output_guidelines() or self.REFUEL_LLM_DEFAULT_OUTPUT_GUIDELINES\n        )\n        refuel_prompt = super().construct_confidence_prompt(\n            input,\n            examples,\n            output_guidelines_override=output_guidelines_override,\n            **kwargs,\n        )\n        return refuel_prompt\n\n    def get_explanation_prompt(self, example: Dict, include_label=True) -&gt; str:\n        pt = PromptTemplate(\n            input_variables=get_format_variables(self.GENERATE_EXPLANATION_PROMPT),\n            template=self.GENERATE_EXPLANATION_PROMPT,\n        )\n        example_template = self.config.example_template()\n        fmt_example = example_template.format_map(defaultdict(str, example))\n\n        return pt.format(\n            task_guidelines=self.task_guidelines,\n            label_format=self.LABEL_FORMAT_IN_EXPLANATION\n            if include_label\n            else self.EXCLUDE_LABEL_IN_EXPLANATION,\n            labeled_example=fmt_example,\n        )\n\n    def get_generate_dataset_prompt(\n        self, label: str, num_rows: int, guidelines: str = None\n    ) -&gt; str:\n        raise NotImplementedError(\"Dataset generation not implemented for this task\")\n\n    def eval(\n        self,\n        llm_labels: List[LLMAnnotation],\n        gt_labels: List[str],\n        additional_metrics: Optional[List[BaseMetric]] = [],\n    ) -&gt; List[MetricResult]:\n        \"\"\"Evaluate the LLM generated labels by comparing them against ground truth\n\n        Args:\n            llm_labels (List[LLMAnnotation]): _description_\n            gt_labels (List[str]): _description_\n            additional_metrics (Optional[List[BaseMetric]], optional): _description_. Defaults to [].\n\n        Returns:\n            List[MetricResult]: list of metrics and corresponding values\n        \"\"\"\n        eval_metrics = []\n\n        for metric in self.metrics + additional_metrics:\n            eval_metrics.extend(metric.compute(llm_labels, gt_labels))\n\n        return eval_metrics\n</code></pre> <p>             Bases: <code>BaseTask</code></p> Source code in <code>src/autolabel/tasks/named_entity_recognition.py</code> <pre><code>class NamedEntityRecognitionTask(BaseTask):\n    DEFAULT_OUTPUT_GUIDELINES = \"You will return the answer in CSV format, with two columns seperated by the % character. First column is the extracted entity and second column is the category. Rows in the CSV are separated by new line character.\"\n    DEFAULT_TASK_GUIDELINES = \"Your job is to extract named entities mentioned in text, and classify them into one of the following {num_labels} categories.\\nCategories:\\n{labels}\\n \"\n    NULL_LABEL = {}\n\n    def __init__(self, config: AutolabelConfig) -&gt; None:\n        super().__init__(config)\n\n    def _json_to_llm_format(self, input_label: str) -&gt; str:\n        # `label` format: {\"entity type\": [list of entities of this type]}\n        try:\n            labels = json.loads(input_label)\n            rows = []\n            for entity_type, detected_entites in labels.items():\n                for e in detected_entites:\n                    row = \"%\".join([e, entity_type])\n                    rows.append(row)\n            llm_formatted_label = \"\\n\".join(rows)\n            return llm_formatted_label\n        except json.JSONDecodeError as e:\n            logger.error(\n                f\"Could not parse label: {input_label}. Few-shot examples might be formatted incorrectly\"\n            )\n            return input_label\n\n    def _llm_to_json_format(self, response: str):\n        split_response = response.split(\"\\n\")\n        json_output = {i: [] for i in self.config.labels_list()}\n\n        for row in split_response:\n            parts = row.split(\"%\")\n            if len(parts) != 2 or parts[1] not in json_output.keys():\n                logger.debug(f\"Malformed LLM response: {row}\")\n                continue\n            named_entity = parts[0]\n            category = parts[1]\n            json_output[category].append(named_entity)\n        return json_output\n\n    def construct_prompt(\n        self,\n        input: Dict,\n        examples: List,\n        prompt_template_override: PromptTemplate = None,\n        refuel_prompt_override: bool = False,\n        output_guidelines_override: str = None,\n        max_input_tokens: int = None,\n        get_num_tokens: Optional[Callable] = None,\n        **kwargs,\n    ) -&gt; str:\n        # prepare task guideline\n        labels_list = self.config.labels_list()\n        num_labels = len(labels_list)\n        fmt_task_guidelines = self.task_guidelines.format_map(\n            defaultdict(str, labels=\"\\n\".join(labels_list), num_labels=num_labels)\n        )\n\n        # prepare seed examples\n        label_column = self.config.label_column()\n        example_template = self.config.example_template()\n        fmt_examples = []\n        for eg in examples:\n            eg_copy = deepcopy(eg)\n            if label_column:\n                eg_copy[label_column] = self._json_to_llm_format(eg_copy[label_column])\n            fmt_examples.append(example_template.format_map(defaultdict(str, eg_copy)))\n\n        # populate the current example in the prompt\n        if label_column:\n            input[label_column] = \"\"\n\n        # populate the explanation column with empty string for current example\n        explanation_column = self.config.explanation_column()\n        if explanation_column:\n            input[explanation_column] = \"\"\n\n        # check if all mapped keys in input are in the example template\n        try:\n            current_example = example_template.format(**input)\n        except KeyError as e:\n            current_example = example_template.format_map(defaultdict(str, input))\n            logger.warn(\n                f'\\n\\nKey {e} in the \"example_template\" in the given config'\n                f\"\\n\\n{example_template}\\n\\nis not present in the datsaset columns - {input.keys()}.\\n\\n\"\n                f\"Input - {input}\\n\\n\"\n                \"Continuing with the prompt as {current_example}\"\n            )\n\n        # populate the current example in the prompt\n        prompt_template = (\n            self.prompt_template\n            if prompt_template_override is None\n            else prompt_template_override\n        )\n        output_guidelines = (\n            self.output_guidelines\n            if output_guidelines_override is None\n            else output_guidelines_override\n        )\n        if self._is_few_shot_mode():\n            curr_text_prompt = self.trim_prompt(\n                prompt_template,\n                task_guidelines=fmt_task_guidelines,\n                output_guidelines=output_guidelines,\n                seed_examples=\"\\n\\n\".join(fmt_examples),\n                current_example=current_example,\n                max_input_tokens=max_input_tokens,\n                get_num_tokens=get_num_tokens,\n            )\n        else:\n            curr_text_prompt = self.trim_prompt(\n                prompt_template,\n                task_guidelines=fmt_task_guidelines,\n                output_guidelines=output_guidelines,\n                current_example=current_example,\n                max_input_tokens=max_input_tokens,\n                get_num_tokens=get_num_tokens,\n            )\n\n        if self.image_col is not None:\n            return json.dumps(\n                {\"text\": curr_text_prompt, \"image_url\": input[self.image_col]}\n            )\n        else:\n            return curr_text_prompt\n\n    def get_explanation_prompt(self, example: Dict, include_label=True) -&gt; str:\n        raise NotImplementedError(\n            \"Explanation generation not implemented for this task\"\n        )\n\n    def get_generate_dataset_prompt(\n        self, label: str, num_rows: int, guidelines: str = None\n    ) -&gt; str:\n        raise NotImplementedError(\"Dataset generation not implemented for this task\")\n\n    def add_text_spans(self, raw_output: dict, input: str) -&gt; list:\n        processed_output = []\n        for entity_type in raw_output:\n            for curr_entity in raw_output[entity_type]:\n                processed_output.append({\"type\": entity_type, \"text\": curr_entity})\n\n        # create a frequency dict of each named entity in the input to determine text spans for repeated entities\n        frequency_count = {label[\"text\"]: 0 for label in processed_output}\n\n        for label in processed_output:\n            text = label[\"text\"]\n            matches = [i.start() for i in re.finditer(text, input)]\n            count = frequency_count[text]\n            # if count of the named entity is greater than the number of matches, default to last found match\n            if count &gt;= len(matches):\n                count = -1\n\n            # if no occurence of named entity in input, default text span to start: -1, end: -1\n            if len(matches) == 0:\n                label[\"start\"] = -1\n                label[\"end\"] = -1\n            else:\n                label[\"start\"] = matches[count]\n                label[\"end\"] = matches[count] + len(text)\n            frequency_count[text] += 1\n        return processed_output\n\n    def parse_llm_response(\n        self,\n        response: Union[Generation, ChatGeneration],\n        curr_sample: Dict,\n        prompt: str,\n    ) -&gt; LLMAnnotation:\n        output = {}\n        successfully_labeled = False\n        error = None\n        text_column = self.config.text_column()\n        input_str = curr_sample[text_column]\n        try:\n            completion_text = response.text\n            output = self._llm_to_json_format(completion_text.strip())\n            llm_label = self.add_text_spans(output, input_str)\n        except Exception as e:\n            logger.error(f\"Error parsing LLM response: {response.text}, Error: {e}\")\n            llm_label = self.NULL_LABEL\n            error = LabelingError(error_type=ErrorType.PARSING_ERROR, error_msg=str(e))\n\n        successfully_labeled = False if llm_label == self.NULL_LABEL else True\n\n        # TODO: parse generation info correctly to fetch &amp; transform logprobs -&gt; score\n        return LLMAnnotation(\n            curr_sample=input_str,\n            successfully_labeled=successfully_labeled,\n            label=llm_label,\n            generation_info=response.generation_info,\n            raw_response=response.text,\n            prompt=prompt,\n            error=error,\n        )\n\n    def auroc_score_labels(\n        self, gt_labels, llm_labels_with_conf\n    ) -&gt; Tuple[List[int], List[float]]:\n        labels = []\n        confidences = []\n        for index, pred_entities in enumerate(llm_labels_with_conf):\n            gt_entities = gt_labels[index]\n            pred_conf = pred_entities[0][\"conf\"] if len(pred_entities) &gt; 0 else 0\n            for gt_entity in gt_entities:\n                match_found = False\n                pred_index = 0\n                while not match_found and pred_index &lt; len(pred_entities):\n                    curr_match = True\n                    for key in gt_entity:\n                        if gt_entity[key] != pred_entities[pred_index][key]:\n                            curr_match = False\n                    if curr_match:\n                        match_found = True\n                    pred_index += 1\n                labels.append(int(match_found))\n                confidences.append(pred_conf)\n        return labels, confidences\n\n    def get_labels_predictions_with_threshold(self, gt_labels, llm_labels, threshold):\n        answered_gt_labels, answered_llm_preds = [], []\n        for index, l in enumerate(llm_labels):\n            if l.successfully_labeled and (\n                l.confidence_score is None or l.confidence_score &gt;= threshold\n            ):\n                answered_gt_labels.append(\n                    [{**entity, \"label\": entity[\"type\"]} for entity in gt_labels[index]]\n                )\n                answered_llm_preds.append(\n                    [\n                        {\n                            **entity,\n                            \"label\": entity[\"type\"],\n                            \"conf\": l.confidence_score,\n                        }\n                        for entity in l.label\n                    ],\n                )\n\n        return answered_gt_labels, answered_llm_preds\n\n    def run_metrics(\n        self,\n        answered_gt_labels,\n        answered_llm_preds,\n        entity_types_set,\n    ) -&gt; List[MetricResult]:\n        eval_metrics = []\n        evaluator = Evaluator(\n            answered_gt_labels, answered_llm_preds, tags=entity_types_set\n        )\n\n        results, _ = evaluator.evaluate()\n        # f1 score for exact match\n        eval_metrics.append(\n            MetricResult(\n                name=MetricType.F1_EXACT,\n                value=results[\"exact\"][\"f1\"],\n            )\n        )\n        # f1 score for strict match\n        eval_metrics.append(\n            MetricResult(\n                name=MetricType.F1_STRICT,\n                value=results[\"strict\"][\"f1\"],\n            )\n        )\n        # f1 score for partial match\n        eval_metrics.append(\n            MetricResult(\n                name=MetricType.F1_PARTIAL,\n                value=results[\"partial\"][\"f1\"],\n            )\n        )\n        # f1 score for entity type match\n        eval_metrics.append(\n            MetricResult(\n                name=MetricType.F1_ENT_TYPE,\n                value=results[\"ent_type\"][\"f1\"],\n            )\n        )\n        # accuracy\n        accuracy = (\n            results.get(\"strict\").get(\"correct\")\n            / (results.get(\"strict\").get(\"possible\"))\n            if results.get(\"strict\").get(\"possible\") &gt; 0\n            else 0.0\n        )\n        eval_metrics.append(\n            MetricResult(\n                name=MetricType.ACCURACY,\n                value=accuracy,\n            )\n        )\n\n        if self.config.confidence():\n            match, confidences = self.auroc_score_labels(\n                answered_gt_labels, answered_llm_preds\n            )\n            auroc = roc_auc_score(match, confidences)\n            eval_metrics.append(\n                MetricResult(\n                    name=MetricType.AUROC,\n                    value=auroc,\n                )\n            )\n\n        return eval_metrics\n\n    def eval(\n        self,\n        llm_labels: List[LLMAnnotation],\n        gt_labels: List[str],\n        additional_metrics: Optional[List[BaseMetric]] = [],\n    ) -&gt; List[MetricResult]:\n        \"\"\"Evaluate the LLM generated labels by comparing them against ground truth\n\n        Args:\n            llm_labels (List[LLMAnnotation]): _description_\n            gt_labels (List[str]): _description_\n\n        Returns:\n            List[MetricResult]: list of metrics and corresponding values\n        \"\"\"\n        gt_labels = [\n            self.add_text_spans(\n                json.loads(gt_labels[index]), llm_labels[index].curr_sample.decode()\n            )\n            for index in range(len(gt_labels))\n        ]\n\n        (\n            curr_gt_labels,\n            curr_llm_labels,\n        ) = self.get_labels_predictions_with_threshold(\n            gt_labels, llm_labels, float(\"-inf\")\n        )\n\n        entity_types_set = list(\n            set(\n                [\n                    gt_entity.get(\"label\")\n                    for gt_label in curr_gt_labels\n                    for gt_entity in gt_label\n                ]\n            )\n        )\n\n        eval_metrics = []\n\n        eval_metrics.append(\n            MetricResult(\n                name=MetricType.SUPPORT,\n                value=len(gt_labels),\n            )\n        )\n\n        eval_metrics.append(\n            MetricResult(\n                name=MetricType.COMPLETION_RATE,\n                value=(\n                    len(curr_llm_labels) / float(len(gt_labels))\n                    if len(gt_labels) &gt; 0\n                    else 0.0\n                ),\n            )\n        )\n\n        curr_threshold_metrics = self.run_metrics(\n            curr_gt_labels,\n            curr_llm_labels,\n            entity_types_set,\n        )\n\n        eval_metrics.extend(curr_threshold_metrics)\n        return eval_metrics\n</code></pre>"},{"location":"reference/tasks/#src.autolabel.tasks.classification.ClassificationTask.eval","title":"<code>eval(llm_labels, gt_labels, additional_metrics=[])</code>","text":"<p>Evaluate the LLM generated labels by comparing them against ground truth</p> <p>Parameters:</p> Name Type Description Default <code>llm_labels</code> <code>List[LLMAnnotation]</code> <p>description</p> required <code>gt_labels</code> <code>List[str]</code> <p>description</p> required <code>additional_metrics</code> <code>List[BaseMetric]</code> <p>The additional metrics to run. Defaults to [].</p> <code>[]</code> <p>Returns:</p> Type Description <code>List[MetricResult]</code> <p>List[MetricResult]: list of metrics and corresponding values</p> Source code in <code>src/autolabel/tasks/classification.py</code> <pre><code>def eval(\n    self,\n    llm_labels: List[LLMAnnotation],\n    gt_labels: List[str],\n    additional_metrics: List[BaseMetric] = [],\n) -&gt; List[MetricResult]:\n    \"\"\"Evaluate the LLM generated labels by comparing them against ground truth\n\n    Args:\n        llm_labels (List[LLMAnnotation]): _description_\n        gt_labels (List[str]): _description_\n        additional_metrics (List[BaseMetric], optional): The additional metrics to run. Defaults to [].\n\n    Returns:\n        List[MetricResult]: list of metrics and corresponding values\n    \"\"\"\n\n    eval_metrics = []\n\n    for metric in self.metrics + additional_metrics:\n        eval_metrics.extend(metric.compute(llm_labels, gt_labels))\n\n    return eval_metrics\n</code></pre>"},{"location":"reference/tasks/#src.autolabel.tasks.entity_matching.EntityMatchingTask.eval","title":"<code>eval(llm_labels, gt_labels, additional_metrics=[])</code>","text":"<p>Evaluate the LLM generated labels by comparing them against ground truth</p> <p>Parameters:</p> Name Type Description Default <code>llm_labels</code> <code>List[LLMAnnotation]</code> <p>description</p> required <code>gt_labels</code> <code>List[str]</code> <p>description</p> required <code>additional_metrics</code> <code>List[BaseMetric]</code> <p>List of additional metrics to run. Defaults to [].</p> <code>[]</code> <p>Returns:</p> Type Description <code>List[MetricResult]</code> <p>List[MetricResult]: list of metrics and corresponding values</p> Source code in <code>src/autolabel/tasks/entity_matching.py</code> <pre><code>def eval(\n    self,\n    llm_labels: List[LLMAnnotation],\n    gt_labels: List[str],\n    additional_metrics: List[BaseMetric] = [],\n) -&gt; List[MetricResult]:\n    \"\"\"Evaluate the LLM generated labels by comparing them against ground truth\n\n    Args:\n        llm_labels (List[LLMAnnotation]): _description_\n        gt_labels (List[str]): _description_\n        additional_metrics (List[BaseMetric], optional): List of additional metrics to run. Defaults to [].\n\n    Returns:\n        List[MetricResult]: list of metrics and corresponding values\n    \"\"\"\n\n    eval_metrics = []\n\n    for metric in self.metrics + additional_metrics:\n        eval_metrics.extend(metric.compute(llm_labels, gt_labels))\n\n    return eval_metrics\n</code></pre>"},{"location":"reference/tasks/#src.autolabel.tasks.question_answering.QuestionAnsweringTask.eval","title":"<code>eval(llm_labels, gt_labels, additional_metrics=[])</code>","text":"<p>Evaluate the LLM generated labels by comparing them against ground truth</p> <p>Parameters:</p> Name Type Description Default <code>llm_labels</code> <code>List[LLMAnnotation]</code> <p>description</p> required <code>gt_labels</code> <code>List[str]</code> <p>description</p> required <code>additional_metrics</code> <code>Optional[List[BaseMetric]]</code> <p>description. Defaults to [].</p> <code>[]</code> <p>Returns:</p> Type Description <code>List[MetricResult]</code> <p>List[MetricResult]: list of metrics and corresponding values</p> Source code in <code>src/autolabel/tasks/question_answering.py</code> <pre><code>def eval(\n    self,\n    llm_labels: List[LLMAnnotation],\n    gt_labels: List[str],\n    additional_metrics: Optional[List[BaseMetric]] = [],\n) -&gt; List[MetricResult]:\n    \"\"\"Evaluate the LLM generated labels by comparing them against ground truth\n\n    Args:\n        llm_labels (List[LLMAnnotation]): _description_\n        gt_labels (List[str]): _description_\n        additional_metrics (Optional[List[BaseMetric]], optional): _description_. Defaults to [].\n\n    Returns:\n        List[MetricResult]: list of metrics and corresponding values\n    \"\"\"\n    eval_metrics = []\n\n    for metric in self.metrics + additional_metrics:\n        eval_metrics.extend(metric.compute(llm_labels, gt_labels))\n\n    return eval_metrics\n</code></pre>"},{"location":"reference/tasks/#src.autolabel.tasks.named_entity_recognition.NamedEntityRecognitionTask.eval","title":"<code>eval(llm_labels, gt_labels, additional_metrics=[])</code>","text":"<p>Evaluate the LLM generated labels by comparing them against ground truth</p> <p>Parameters:</p> Name Type Description Default <code>llm_labels</code> <code>List[LLMAnnotation]</code> <p>description</p> required <code>gt_labels</code> <code>List[str]</code> <p>description</p> required <p>Returns:</p> Type Description <code>List[MetricResult]</code> <p>List[MetricResult]: list of metrics and corresponding values</p> Source code in <code>src/autolabel/tasks/named_entity_recognition.py</code> <pre><code>def eval(\n    self,\n    llm_labels: List[LLMAnnotation],\n    gt_labels: List[str],\n    additional_metrics: Optional[List[BaseMetric]] = [],\n) -&gt; List[MetricResult]:\n    \"\"\"Evaluate the LLM generated labels by comparing them against ground truth\n\n    Args:\n        llm_labels (List[LLMAnnotation]): _description_\n        gt_labels (List[str]): _description_\n\n    Returns:\n        List[MetricResult]: list of metrics and corresponding values\n    \"\"\"\n    gt_labels = [\n        self.add_text_spans(\n            json.loads(gt_labels[index]), llm_labels[index].curr_sample.decode()\n        )\n        for index in range(len(gt_labels))\n    ]\n\n    (\n        curr_gt_labels,\n        curr_llm_labels,\n    ) = self.get_labels_predictions_with_threshold(\n        gt_labels, llm_labels, float(\"-inf\")\n    )\n\n    entity_types_set = list(\n        set(\n            [\n                gt_entity.get(\"label\")\n                for gt_label in curr_gt_labels\n                for gt_entity in gt_label\n            ]\n        )\n    )\n\n    eval_metrics = []\n\n    eval_metrics.append(\n        MetricResult(\n            name=MetricType.SUPPORT,\n            value=len(gt_labels),\n        )\n    )\n\n    eval_metrics.append(\n        MetricResult(\n            name=MetricType.COMPLETION_RATE,\n            value=(\n                len(curr_llm_labels) / float(len(gt_labels))\n                if len(gt_labels) &gt; 0\n                else 0.0\n            ),\n        )\n    )\n\n    curr_threshold_metrics = self.run_metrics(\n        curr_gt_labels,\n        curr_llm_labels,\n        entity_types_set,\n    )\n\n    eval_metrics.extend(curr_threshold_metrics)\n    return eval_metrics\n</code></pre>"},{"location":"reference/tasks/#src.autolabel.tasks.utils.filter_unlabeled_examples","title":"<code>filter_unlabeled_examples(gt_labels, llm_labels)</code>","text":"<p>Filter out unlabeled examples from the ground truth and LLM generated labels. This is done by checking the ground truth labels which have nan values. The corresponding ground truth and LLM labels are removed from the filtered labels lists.</p> <p>Parameters:</p> Name Type Description Default <code>gt_labels</code> <code>List[str]</code> <p>ground truth labels</p> required <code>llm_labels</code> <code>List[LLMAnnotation]</code> <p>llm labels</p> required <p>Returns:</p> Type Description <code>Tuple[List[str], List[LLMAnnotation]]</code> <p>filtered_gt_labels, filtered_llm_labels: filtered ground truth and LLM generated labels</p> Source code in <code>src/autolabel/tasks/utils.py</code> <pre><code>def filter_unlabeled_examples(\n    gt_labels: List[str], llm_labels: List[LLMAnnotation]\n) -&gt; Tuple[List[str], List[LLMAnnotation]]:\n    \"\"\"Filter out unlabeled examples from the ground truth and LLM generated labels.\n    This is done by checking the ground truth labels which have nan values.\n    The corresponding ground truth and LLM labels are removed from the filtered labels lists.\n\n    Args:\n        gt_labels (List[str]): ground truth labels\n        llm_labels (List[LLMAnnotation]): llm labels\n\n    Returns:\n        filtered_gt_labels, filtered_llm_labels: filtered ground truth and LLM generated labels\n    \"\"\"\n    filtered_gt_labels = []\n    filtered_llm_labels = []\n    for gt_label, llm_label in zip(gt_labels, llm_labels):\n        if gt_label != \"nan\":\n            filtered_gt_labels.append(gt_label)\n            filtered_llm_labels.append(llm_label)\n    return filtered_gt_labels, filtered_llm_labels\n</code></pre>"},{"location":"reference/tasks/#src.autolabel.tasks.utils.normalize_text","title":"<code>normalize_text(s)</code>","text":"<p>Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.</p> Source code in <code>src/autolabel/tasks/utils.py</code> <pre><code>def normalize_text(s: str) -&gt; str:\n    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n\n    def remove_articles(text):\n        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n        return re.sub(regex, \" \", text)\n\n    def white_space_fix(text):\n        return \" \".join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return \"\".join(ch for ch in text if ch not in exclude)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n</code></pre>"}]}